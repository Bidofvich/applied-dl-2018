save path : ./logs/
{'batch_size': 32, 'data_path': 'd:/db/data/IDC_regular_ps50_idx5/', 'dataset': 'IDC_regular_ps50_idx5', 'decay': 0.0005, 'epochs': 250, 'evaluate': False, 'gammas': [0.1, 0.1], 'imgDim': 3, 'img_scale': 224, 'learning_rate': 0.002, 'manualSeed': 2222, 'momentum': 0.9, 'ngpu': 1, 'num_classes': 2, 'print_freq': 200, 'resume': '', 'save_path': './logs/', 'save_path_model': './logs//IDC_regular_ps50_idx5/SimpleNet/', 'schedule': [150, 225], 'start_epoch': 0, 'tensorboard': True, 'use_cuda': True, 'validationRatio': 0.85, 'workers': 0}

==>>[2018-03-31 17:38:44] [Epoch=000/250] [Need: 00:00:00] [learning_rate=0.0020] [Best : Accuracy=0.00, Error=100.00]

==>>Epoch=[000/250]], [2018-03-31 17:38:44], LR=[0.002], Batch=[32] [Model=SimpleNet]
  Epoch: [000][000/1475]   Time 1.362 (1.362)   Data 0.125 (0.125)   Loss 0.7035 (0.7035)   Prec@1 46.875 (46.875)   Prec@5 46.875 (46.875)   [2018-03-31 17:38:45]
  Epoch: [000][200/1475]   Time 0.141 (0.149)   Data 0.094 (0.109)   Loss 0.4666 (0.5172)   Prec@1 75.000 (75.808)   Prec@5 75.000 (75.808)   [2018-03-31 17:39:14]
  Epoch: [000][400/1475]   Time 0.148 (0.144)   Data 0.101 (0.107)   Loss 0.4252 (0.4988)   Prec@1 78.125 (77.026)   Prec@5 78.125 (77.026)   [2018-03-31 17:39:41]
  Epoch: [000][600/1475]   Time 0.125 (0.142)   Data 0.094 (0.107)   Loss 0.3503 (0.4842)   Prec@1 93.750 (77.985)   Prec@5 93.750 (77.985)   [2018-03-31 17:40:09]
  Epoch: [000][800/1475]   Time 0.141 (0.141)   Data 0.109 (0.106)   Loss 0.3548 (0.4772)   Prec@1 84.375 (78.410)   Prec@5 84.375 (78.410)   [2018-03-31 17:40:37]
  Epoch: [000][1000/1475]   Time 0.147 (0.140)   Data 0.109 (0.106)   Loss 0.5378 (0.4680)   Prec@1 81.250 (78.915)   Prec@5 81.250 (78.915)   [2018-03-31 17:41:04]
  Epoch: [000][1200/1475]   Time 0.141 (0.140)   Data 0.109 (0.106)   Loss 0.4075 (0.4644)   Prec@1 81.250 (79.088)   Prec@5 81.250 (79.088)   [2018-03-31 17:41:32]
  Epoch: [000][1400/1475]   Time 0.141 (0.140)   Data 0.094 (0.106)   Loss 0.4181 (0.4609)   Prec@1 78.125 (79.309)   Prec@5 78.125 (79.309)   [2018-03-31 17:41:59]
  **Train** Prec@1 79.334 Prec@5 79.334 Error@1 20.666
  **VAL** Prec@1 76.123 Prec@5 76.123 Error@1 23.877

==>>[2018-03-31 17:42:28] [Epoch=001/250] [Need: 15:25:21] [learning_rate=0.0020] [Best : Accuracy=76.12, Error=23.88]

==>>Epoch=[001/250]], [2018-03-31 17:42:28], LR=[0.002], Batch=[32] [Model=SimpleNet]
  Epoch: [001][000/1475]   Time 0.191 (0.191)   Data 0.113 (0.113)   Loss 0.4177 (0.4177)   Prec@1 81.250 (81.250)   Prec@5 81.250 (81.250)   [2018-03-31 17:42:28]
  Epoch: [001][200/1475]   Time 0.141 (0.139)   Data 0.094 (0.106)   Loss 0.3445 (0.4376)   Prec@1 84.375 (80.348)   Prec@5 84.375 (80.348)   [2018-03-31 17:42:56]
  Epoch: [001][400/1475]   Time 0.141 (0.139)   Data 0.109 (0.105)   Loss 0.4100 (0.4374)   Prec@1 90.625 (80.775)   Prec@5 90.625 (80.775)   [2018-03-31 17:43:24]
  Epoch: [001][600/1475]   Time 0.141 (0.139)   Data 0.109 (0.105)   Loss 0.4501 (0.4349)   Prec@1 75.000 (80.798)   Prec@5 75.000 (80.798)   [2018-03-31 17:43:51]
  Epoch: [001][800/1475]   Time 0.141 (0.139)   Data 0.109 (0.105)   Loss 0.3811 (0.4349)   Prec@1 84.375 (80.875)   Prec@5 84.375 (80.875)   [2018-03-31 17:44:19]
  Epoch: [001][1000/1475]   Time 0.147 (0.139)   Data 0.116 (0.106)   Loss 0.5461 (0.4323)   Prec@1 71.875 (80.991)   Prec@5 71.875 (80.991)   [2018-03-31 17:44:47]
  Epoch: [001][1200/1475]   Time 0.140 (0.139)   Data 0.107 (0.106)   Loss 0.3671 (0.4325)   Prec@1 87.500 (80.987)   Prec@5 87.500 (80.987)   [2018-03-31 17:45:15]
  Epoch: [001][1400/1475]   Time 0.141 (0.139)   Data 0.109 (0.106)   Loss 0.3943 (0.4320)   Prec@1 87.500 (80.967)   Prec@5 87.500 (80.967)   [2018-03-31 17:45:43]
  **Train** Prec@1 80.896 Prec@5 80.896 Error@1 19.104
  **VAL** Prec@1 82.356 Prec@5 82.356 Error@1 17.644

==>>[2018-03-31 17:46:12] [Epoch=002/250] [Need: 15:23:09] [learning_rate=0.0020] [Best : Accuracy=82.36, Error=17.64]

==>>Epoch=[002/250]], [2018-03-31 17:46:12], LR=[0.002], Batch=[32] [Model=SimpleNet]
  Epoch: [002][000/1475]   Time 0.141 (0.141)   Data 0.109 (0.109)   Loss 0.3478 (0.3478)   Prec@1 78.125 (78.125)   Prec@5 78.125 (78.125)   [2018-03-31 17:46:12]
  Epoch: [002][200/1475]   Time 0.141 (0.139)   Data 0.109 (0.105)   Loss 0.4230 (0.4303)   Prec@1 84.375 (81.141)   Prec@5 84.375 (81.141)   [2018-03-31 17:46:40]
  Epoch: [002][400/1475]   Time 0.141 (0.139)   Data 0.109 (0.105)   Loss 0.2658 (0.4269)   Prec@1 90.625 (81.227)   Prec@5 90.625 (81.227)   [2018-03-31 17:47:07]
  Epoch: [002][600/1475]   Time 0.141 (0.139)   Data 0.094 (0.105)   Loss 0.4203 (0.4246)   Prec@1 81.250 (81.416)   Prec@5 81.250 (81.416)   [2018-03-31 17:47:35]
  Epoch: [002][800/1475]   Time 0.129 (0.139)   Data 0.098 (0.105)   Loss 0.6971 (0.4269)   Prec@1 71.875 (81.312)   Prec@5 71.875 (81.312)   [2018-03-31 17:48:03]
  Epoch: [002][1000/1475]   Time 0.141 (0.139)   Data 0.109 (0.105)   Loss 0.4614 (0.4256)   Prec@1 71.875 (81.390)   Prec@5 71.875 (81.390)   [2018-03-31 17:48:31]
  Epoch: [002][1200/1475]   Time 0.141 (0.139)   Data 0.109 (0.105)   Loss 0.4553 (0.4218)   Prec@1 78.125 (81.588)   Prec@5 78.125 (81.588)   [2018-03-31 17:48:58]
  Epoch: [002][1400/1475]   Time 0.125 (0.139)   Data 0.094 (0.105)   Loss 0.4077 (0.4227)   Prec@1 71.875 (81.529)   Prec@5 71.875 (81.529)   [2018-03-31 17:49:26]
  **Train** Prec@1 81.566 Prec@5 81.566 Error@1 18.434
  **VAL** Prec@1 82.597 Prec@5 82.597 Error@1 17.403

==>>[2018-03-31 17:49:54] [Epoch=003/250] [Need: 15:18:38] [learning_rate=0.0020] [Best : Accuracy=82.60, Error=17.40]

==>>Epoch=[003/250]], [2018-03-31 17:49:54], LR=[0.002], Batch=[32] [Model=SimpleNet]
  Epoch: [003][000/1475]   Time 0.146 (0.146)   Data 0.115 (0.115)   Loss 0.2575 (0.2575)   Prec@1 93.750 (93.750)   Prec@5 93.750 (93.750)   [2018-03-31 17:49:54]
  Epoch: [003][200/1475]   Time 0.135 (0.139)   Data 0.104 (0.105)   Loss 0.2516 (0.4130)   Prec@1 93.750 (81.919)   Prec@5 93.750 (81.919)   [2018-03-31 17:50:22]
  Epoch: [003][400/1475]   Time 0.134 (0.139)   Data 0.103 (0.105)   Loss 0.3811 (0.4188)   Prec@1 84.375 (81.546)   Prec@5 84.375 (81.546)   [2018-03-31 17:50:50]
  Epoch: [003][600/1475]   Time 0.141 (0.139)   Data 0.109 (0.105)   Loss 0.3859 (0.4220)   Prec@1 81.250 (81.489)   Prec@5 81.250 (81.489)   [2018-03-31 17:51:18]
  Epoch: [003][800/1475]   Time 0.141 (0.139)   Data 0.109 (0.105)   Loss 0.4548 (0.4224)   Prec@1 81.250 (81.348)   Prec@5 81.250 (81.348)   [2018-03-31 17:51:45]
  Epoch: [003][1000/1475]   Time 0.125 (0.139)   Data 0.094 (0.105)   Loss 0.2635 (0.4193)   Prec@1 93.750 (81.543)   Prec@5 93.750 (81.543)   [2018-03-31 17:52:13]
  Epoch: [003][1200/1475]   Time 0.141 (0.139)   Data 0.109 (0.105)   Loss 0.3902 (0.4176)   Prec@1 84.375 (81.619)   Prec@5 84.375 (81.619)   [2018-03-31 17:52:41]
  Epoch: [003][1400/1475]   Time 0.141 (0.139)   Data 0.109 (0.105)   Loss 0.4982 (0.4173)   Prec@1 81.250 (81.611)   Prec@5 81.250 (81.611)   [2018-03-31 17:53:09]
  **Train** Prec@1 81.663 Prec@5 81.663 Error@1 18.337
  **VAL** Prec@1 80.867 Prec@5 80.867 Error@1 19.133

==>>[2018-03-31 17:53:37] [Epoch=004/250] [Need: 15:14:11] [learning_rate=0.0020] [Best : Accuracy=82.60, Error=17.40]

==>>Epoch=[004/250]], [2018-03-31 17:53:37], LR=[0.002], Batch=[32] [Model=SimpleNet]
  Epoch: [004][000/1475]   Time 0.128 (0.128)   Data 0.097 (0.097)   Loss 0.3219 (0.3219)   Prec@1 90.625 (90.625)   Prec@5 90.625 (90.625)   [2018-03-31 17:53:37]
  Epoch: [004][200/1475]   Time 0.141 (0.139)   Data 0.109 (0.106)   Loss 0.3606 (0.4143)   Prec@1 84.375 (81.281)   Prec@5 84.375 (81.281)   [2018-03-31 17:54:05]
  Epoch: [004][400/1475]   Time 0.141 (0.140)   Data 0.109 (0.106)   Loss 0.4381 (0.4125)   Prec@1 81.250 (81.655)   Prec@5 81.250 (81.655)   [2018-03-31 17:54:33]
  Epoch: [004][600/1475]   Time 0.141 (0.140)   Data 0.109 (0.107)   Loss 0.2945 (0.4129)   Prec@1 81.250 (81.713)   Prec@5 81.250 (81.713)   [2018-03-31 17:55:01]
  Epoch: [004][800/1475]   Time 0.125 (0.140)   Data 0.094 (0.106)   Loss 0.3775 (0.4128)   Prec@1 84.375 (81.730)   Prec@5 84.375 (81.730)   [2018-03-31 17:55:29]
  Epoch: [004][1000/1475]   Time 0.141 (0.139)   Data 0.109 (0.106)   Loss 0.4903 (0.4104)   Prec@1 78.125 (81.793)   Prec@5 78.125 (81.793)   [2018-03-31 17:55:56]
  Epoch: [004][1200/1475]   Time 0.141 (0.139)   Data 0.109 (0.106)   Loss 0.3528 (0.4093)   Prec@1 87.500 (81.838)   Prec@5 87.500 (81.838)   [2018-03-31 17:56:24]
  Epoch: [004][1400/1475]   Time 0.141 (0.139)   Data 0.109 (0.106)   Loss 0.4637 (0.4076)   Prec@1 81.250 (81.917)   Prec@5 81.250 (81.917)   [2018-03-31 17:56:52]
  **Train** Prec@1 81.926 Prec@5 81.926 Error@1 18.074
  **VAL** Prec@1 80.483 Prec@5 80.483 Error@1 19.517

==>>[2018-03-31 17:57:20] [Epoch=005/250] [Need: 15:10:37] [learning_rate=0.0020] [Best : Accuracy=82.60, Error=17.40]

==>>Epoch=[005/250]], [2018-03-31 17:57:20], LR=[0.002], Batch=[32] [Model=SimpleNet]
  Epoch: [005][000/1475]   Time 0.141 (0.141)   Data 0.109 (0.109)   Loss 0.2413 (0.2413)   Prec@1 87.500 (87.500)   Prec@5 87.500 (87.500)   [2018-03-31 17:57:20]
  Epoch: [005][200/1475]   Time 0.141 (0.139)   Data 0.094 (0.106)   Loss 0.3241 (0.4135)   Prec@1 84.375 (81.763)   Prec@5 84.375 (81.763)   [2018-03-31 17:57:48]
  Epoch: [005][400/1475]   Time 0.139 (0.139)   Data 0.108 (0.105)   Loss 0.5364 (0.4128)   Prec@1 75.000 (81.866)   Prec@5 75.000 (81.866)   [2018-03-31 17:58:16]
  Epoch: [005][600/1475]   Time 0.141 (0.139)   Data 0.109 (0.105)   Loss 0.3907 (0.4098)   Prec@1 81.250 (82.014)   Prec@5 81.250 (82.014)   [2018-03-31 17:58:43]
  Epoch: [005][800/1475]   Time 0.142 (0.139)   Data 0.109 (0.105)   Loss 0.3515 (0.4072)   Prec@1 87.500 (82.155)   Prec@5 87.500 (82.155)   [2018-03-31 17:59:11]
  Epoch: [005][1000/1475]   Time 0.141 (0.139)   Data 0.109 (0.105)   Loss 0.5075 (0.4060)   Prec@1 78.125 (82.237)   Prec@5 78.125 (82.237)   [2018-03-31 17:59:39]
  Epoch: [005][1200/1475]   Time 0.141 (0.139)   Data 0.109 (0.105)   Loss 0.3219 (0.4056)   Prec@1 93.750 (82.332)   Prec@5 93.750 (82.332)   [2018-03-31 18:00:07]
  Epoch: [005][1400/1475]   Time 0.141 (0.139)   Data 0.109 (0.105)   Loss 0.3852 (0.4024)   Prec@1 84.375 (82.463)   Prec@5 84.375 (82.463)   [2018-03-31 18:00:34]
  **Train** Prec@1 82.460 Prec@5 82.460 Error@1 17.540
  **VAL** Prec@1 84.242 Prec@5 84.242 Error@1 15.758

==>>[2018-03-31 18:01:03] [Epoch=006/250] [Need: 15:06:40] [learning_rate=0.0020] [Best : Accuracy=84.24, Error=15.76]

==>>Epoch=[006/250]], [2018-03-31 18:01:03], LR=[0.002], Batch=[32] [Model=SimpleNet]
  Epoch: [006][000/1475]   Time 0.141 (0.141)   Data 0.109 (0.109)   Loss 0.3861 (0.3861)   Prec@1 81.250 (81.250)   Prec@5 81.250 (81.250)   [2018-03-31 18:01:03]
  Epoch: [006][200/1475]   Time 0.141 (0.139)   Data 0.109 (0.105)   Loss 0.3255 (0.4149)   Prec@1 87.500 (81.981)   Prec@5 87.500 (81.981)   [2018-03-31 18:01:30]
  Epoch: [006][400/1475]   Time 0.130 (0.139)   Data 0.106 (0.105)   Loss 0.3068 (0.4040)   Prec@1 87.500 (82.474)   Prec@5 87.500 (82.474)   [2018-03-31 18:01:58]
  Epoch: [006][600/1475]   Time 0.141 (0.139)   Data 0.094 (0.105)   Loss 0.4021 (0.4026)   Prec@1 78.125 (82.456)   Prec@5 78.125 (82.456)   [2018-03-31 18:02:26]
  Epoch: [006][800/1475]   Time 0.141 (0.139)   Data 0.109 (0.105)   Loss 0.2809 (0.4027)   Prec@1 90.625 (82.374)   Prec@5 90.625 (82.374)   [2018-03-31 18:02:54]
  Epoch: [006][1000/1475]   Time 0.141 (0.139)   Data 0.109 (0.105)   Loss 0.3386 (0.4016)   Prec@1 81.250 (82.386)   Prec@5 81.250 (82.386)   [2018-03-31 18:03:21]
  Epoch: [006][1200/1475]   Time 0.137 (0.139)   Data 0.106 (0.105)   Loss 0.3371 (0.4010)   Prec@1 87.500 (82.413)   Prec@5 87.500 (82.413)   [2018-03-31 18:03:49]
  Epoch: [006][1400/1475]   Time 0.141 (0.139)   Data 0.109 (0.105)   Loss 0.3910 (0.3985)   Prec@1 78.125 (82.588)   Prec@5 78.125 (82.588)   [2018-03-31 18:04:17]
  **Train** Prec@1 82.549 Prec@5 82.549 Error@1 17.451
  **VAL** Prec@1 84.891 Prec@5 84.891 Error@1 15.109

==>>[2018-03-31 18:04:45] [Epoch=007/250] [Need: 15:02:39] [learning_rate=0.0020] [Best : Accuracy=84.89, Error=15.11]

==>>Epoch=[007/250]], [2018-03-31 18:04:45], LR=[0.002], Batch=[32] [Model=SimpleNet]
  Epoch: [007][000/1475]   Time 0.145 (0.145)   Data 0.114 (0.114)   Loss 0.3689 (0.3689)   Prec@1 87.500 (87.500)   Prec@5 87.500 (87.500)   [2018-03-31 18:04:45]
  Epoch: [007][200/1475]   Time 0.134 (0.139)   Data 0.103 (0.106)   Loss 0.4324 (0.4049)   Prec@1 78.125 (82.447)   Prec@5 78.125 (82.447)   [2018-03-31 18:05:13]
  Epoch: [007][400/1475]   Time 0.141 (0.139)   Data 0.109 (0.106)   Loss 0.4664 (0.3943)   Prec@1 81.250 (83.003)   Prec@5 81.250 (83.003)   [2018-03-31 18:05:41]
  Epoch: [007][600/1475]   Time 0.125 (0.139)   Data 0.094 (0.106)   Loss 0.3388 (0.3945)   Prec@1 90.625 (82.903)   Prec@5 90.625 (82.903)   [2018-03-31 18:06:08]
  Epoch: [007][800/1475]   Time 0.145 (0.139)   Data 0.109 (0.106)   Loss 0.3637 (0.3960)   Prec@1 81.250 (82.760)   Prec@5 81.250 (82.760)   [2018-03-31 18:06:36]
  Epoch: [007][1000/1475]   Time 0.156 (0.139)   Data 0.125 (0.106)   Loss 0.4196 (0.3955)   Prec@1 81.250 (82.745)   Prec@5 81.250 (82.745)   [2018-03-31 18:07:04]
  Epoch: [007][1200/1475]   Time 0.140 (0.139)   Data 0.094 (0.106)   Loss 0.5195 (0.3950)   Prec@1 75.000 (82.884)   Prec@5 75.000 (82.884)   [2018-03-31 18:07:32]
  Epoch: [007][1400/1475]   Time 0.141 (0.139)   Data 0.109 (0.106)   Loss 0.6335 (0.3953)   Prec@1 78.125 (82.852)   Prec@5 78.125 (82.852)   [2018-03-31 18:07:59]
  **Train** Prec@1 82.821 Prec@5 82.821 Error@1 17.179
  **VAL** Prec@1 83.341 Prec@5 83.341 Error@1 16.659

==>>[2018-03-31 18:08:28] [Epoch=008/250] [Need: 14:58:46] [learning_rate=0.0020] [Best : Accuracy=84.89, Error=15.11]

==>>Epoch=[008/250]], [2018-03-31 18:08:28], LR=[0.002], Batch=[32] [Model=SimpleNet]
  Epoch: [008][000/1475]   Time 0.141 (0.141)   Data 0.109 (0.109)   Loss 0.5194 (0.5194)   Prec@1 71.875 (71.875)   Prec@5 71.875 (71.875)   [2018-03-31 18:08:28]
  Epoch: [008][200/1475]   Time 0.143 (0.139)   Data 0.112 (0.105)   Loss 0.3863 (0.3916)   Prec@1 84.375 (83.100)   Prec@5 84.375 (83.100)   [2018-03-31 18:08:55]
  Epoch: [008][400/1475]   Time 0.125 (0.139)   Data 0.094 (0.105)   Loss 0.3736 (0.3973)   Prec@1 78.125 (82.481)   Prec@5 78.125 (82.481)   [2018-03-31 18:09:23]
  Epoch: [008][600/1475]   Time 0.126 (0.139)   Data 0.095 (0.105)   Loss 0.3198 (0.3967)   Prec@1 87.500 (82.529)   Prec@5 87.500 (82.529)   [2018-03-31 18:09:51]
  Epoch: [008][800/1475]   Time 0.141 (0.139)   Data 0.109 (0.105)   Loss 0.3511 (0.3960)   Prec@1 84.375 (82.588)   Prec@5 84.375 (82.588)   [2018-03-31 18:10:19]
  Epoch: [008][1000/1475]   Time 0.145 (0.139)   Data 0.113 (0.105)   Loss 0.3541 (0.3950)   Prec@1 90.625 (82.711)   Prec@5 90.625 (82.711)   [2018-03-31 18:10:46]
  Epoch: [008][1200/1475]   Time 0.144 (0.139)   Data 0.117 (0.105)   Loss 0.3723 (0.3952)   Prec@1 84.375 (82.780)   Prec@5 84.375 (82.780)   [2018-03-31 18:11:14]
  Epoch: [008][1400/1475]   Time 0.141 (0.139)   Data 0.109 (0.105)   Loss 0.4068 (0.3954)   Prec@1 87.500 (82.733)   Prec@5 87.500 (82.733)   [2018-03-31 18:11:42]
  **Train** Prec@1 82.776 Prec@5 82.776 Error@1 17.224
  **VAL** Prec@1 85.071 Prec@5 85.071 Error@1 14.929

==>>[2018-03-31 18:12:10] [Epoch=009/250] [Need: 14:54:48] [learning_rate=0.0020] [Best : Accuracy=85.07, Error=14.93]

==>>Epoch=[009/250]], [2018-03-31 18:12:10], LR=[0.002], Batch=[32] [Model=SimpleNet]
  Epoch: [009][000/1475]   Time 0.141 (0.141)   Data 0.109 (0.109)   Loss 0.2604 (0.2604)   Prec@1 93.750 (93.750)   Prec@5 93.750 (93.750)   [2018-03-31 18:12:10]
  Epoch: [009][200/1475]   Time 0.141 (0.139)   Data 0.109 (0.105)   Loss 0.3674 (0.3902)   Prec@1 84.375 (83.287)   Prec@5 84.375 (83.287)   [2018-03-31 18:12:38]
  Epoch: [009][400/1475]   Time 0.141 (0.139)   Data 0.109 (0.105)   Loss 0.3705 (0.3894)   Prec@1 87.500 (83.222)   Prec@5 87.500 (83.222)   [2018-03-31 18:13:06]
  Epoch: [009][600/1475]   Time 0.141 (0.139)   Data 0.113 (0.105)   Loss 0.4069 (0.3930)   Prec@1 90.625 (83.033)   Prec@5 90.625 (83.033)   [2018-03-31 18:13:33]
  Epoch: [009][800/1475]   Time 0.141 (0.139)   Data 0.109 (0.105)   Loss 0.3480 (0.3937)   Prec@1 81.250 (82.873)   Prec@5 81.250 (82.873)   [2018-03-31 18:14:01]
  Epoch: [009][1000/1475]   Time 0.141 (0.139)   Data 0.109 (0.105)   Loss 0.3591 (0.3916)   Prec@1 90.625 (82.933)   Prec@5 90.625 (82.933)   [2018-03-31 18:14:29]
  Epoch: [009][1200/1475]   Time 0.144 (0.139)   Data 0.111 (0.105)   Loss 0.5137 (0.3936)   Prec@1 84.375 (82.793)   Prec@5 84.375 (82.793)   [2018-03-31 18:14:57]
  Epoch: [009][1400/1475]   Time 0.125 (0.139)   Data 0.094 (0.105)   Loss 0.4669 (0.3927)   Prec@1 81.250 (82.905)   Prec@5 81.250 (82.905)   [2018-03-31 18:15:24]
  **Train** Prec@1 82.865 Prec@5 82.865 Error@1 17.135
  **VAL** Prec@1 85.179 Prec@5 85.179 Error@1 14.821

==>>[2018-03-31 18:15:52] [Epoch=010/250] [Need: 14:51:02] [learning_rate=0.0020] [Best : Accuracy=85.18, Error=14.82]

==>>Epoch=[010/250]], [2018-03-31 18:15:52], LR=[0.002], Batch=[32] [Model=SimpleNet]
  Epoch: [010][000/1475]   Time 0.141 (0.141)   Data 0.109 (0.109)   Loss 0.4250 (0.4250)   Prec@1 78.125 (78.125)   Prec@5 78.125 (78.125)   [2018-03-31 18:15:53]
  Epoch: [010][200/1475]   Time 0.141 (0.139)   Data 0.109 (0.106)   Loss 0.2805 (0.4000)   Prec@1 90.625 (82.400)   Prec@5 90.625 (82.400)   [2018-03-31 18:16:20]
  Epoch: [010][400/1475]   Time 0.125 (0.139)   Data 0.094 (0.106)   Loss 0.4615 (0.3989)   Prec@1 78.125 (82.388)   Prec@5 78.125 (82.388)   [2018-03-31 18:16:48]
  Epoch: [010][600/1475]   Time 0.141 (0.139)   Data 0.109 (0.106)   Loss 0.4058 (0.3959)   Prec@1 84.375 (82.493)   Prec@5 84.375 (82.493)   [2018-03-31 18:17:16]
  Epoch: [010][800/1475]   Time 0.141 (0.139)   Data 0.109 (0.105)   Loss 0.2414 (0.3923)   Prec@1 87.500 (82.674)   Prec@5 87.500 (82.674)   [2018-03-31 18:17:44]
  Epoch: [010][1000/1475]   Time 0.125 (0.139)   Data 0.094 (0.105)   Loss 0.5842 (0.3899)   Prec@1 75.000 (82.848)   Prec@5 75.000 (82.848)   [2018-03-31 18:18:11]
  Epoch: [010][1200/1475]   Time 0.125 (0.139)   Data 0.094 (0.105)   Loss 0.3558 (0.3918)   Prec@1 84.375 (82.814)   Prec@5 84.375 (82.814)   [2018-03-31 18:18:39]
  Epoch: [010][1400/1475]   Time 0.141 (0.139)   Data 0.109 (0.105)   Loss 0.3760 (0.3921)   Prec@1 87.500 (82.856)   Prec@5 87.500 (82.856)   [2018-03-31 18:19:07]
  **Train** Prec@1 82.865 Prec@5 82.865 Error@1 17.135
  **VAL** Prec@1 85.299 Prec@5 85.299 Error@1 14.701

==>>[2018-03-31 18:19:35] [Epoch=011/250] [Need: 14:47:21] [learning_rate=0.0020] [Best : Accuracy=85.30, Error=14.70]

==>>Epoch=[011/250]], [2018-03-31 18:19:35], LR=[0.002], Batch=[32] [Model=SimpleNet]
  Epoch: [011][000/1475]   Time 0.141 (0.141)   Data 0.094 (0.094)   Loss 0.2901 (0.2901)   Prec@1 87.500 (87.500)   Prec@5 87.500 (87.500)   [2018-03-31 18:19:35]
  Epoch: [011][200/1475]   Time 0.142 (0.140)   Data 0.110 (0.106)   Loss 0.2613 (0.3879)   Prec@1 87.500 (82.836)   Prec@5 87.500 (82.836)   [2018-03-31 18:20:03]
  Epoch: [011][400/1475]   Time 0.141 (0.140)   Data 0.094 (0.106)   Loss 0.4833 (0.3855)   Prec@1 78.125 (83.229)   Prec@5 78.125 (83.229)   [2018-03-31 18:20:31]
  Epoch: [011][600/1475]   Time 0.141 (0.140)   Data 0.109 (0.106)   Loss 0.2987 (0.3870)   Prec@1 84.375 (83.075)   Prec@5 84.375 (83.075)   [2018-03-31 18:20:59]
  Epoch: [011][800/1475]   Time 0.141 (0.139)   Data 0.094 (0.106)   Loss 0.3790 (0.3892)   Prec@1 81.250 (83.006)   Prec@5 81.250 (83.006)   [2018-03-31 18:21:27]
  Epoch: [011][1000/1475]   Time 0.141 (0.139)   Data 0.109 (0.106)   Loss 0.3997 (0.3891)   Prec@1 84.375 (83.001)   Prec@5 84.375 (83.001)   [2018-03-31 18:21:55]
  Epoch: [011][1200/1475]   Time 0.141 (0.139)   Data 0.109 (0.106)   Loss 0.5225 (0.3911)   Prec@1 71.875 (82.957)   Prec@5 71.875 (82.957)   [2018-03-31 18:22:23]
  Epoch: [011][1400/1475]   Time 0.141 (0.139)   Data 0.109 (0.106)   Loss 0.4060 (0.3892)   Prec@1 84.375 (83.012)   Prec@5 84.375 (83.012)   [2018-03-31 18:22:51]
  **Train** Prec@1 83.028 Prec@5 83.028 Error@1 16.972
  **VAL** Prec@1 85.491 Prec@5 85.491 Error@1 14.509

==>>[2018-03-31 18:23:19] [Epoch=012/250] [Need: 14:43:58] [learning_rate=0.0020] [Best : Accuracy=85.49, Error=14.51]

==>>Epoch=[012/250]], [2018-03-31 18:23:19], LR=[0.002], Batch=[32] [Model=SimpleNet]
  Epoch: [012][000/1475]   Time 0.149 (0.149)   Data 0.102 (0.102)   Loss 0.2558 (0.2558)   Prec@1 90.625 (90.625)   Prec@5 90.625 (90.625)   [2018-03-31 18:23:19]
  Epoch: [012][200/1475]   Time 0.126 (0.139)   Data 0.094 (0.105)   Loss 0.2542 (0.3892)   Prec@1 84.375 (82.572)   Prec@5 84.375 (82.572)   [2018-03-31 18:23:47]
  Epoch: [012][400/1475]   Time 0.125 (0.139)   Data 0.094 (0.106)   Loss 0.5709 (0.3920)   Prec@1 71.875 (82.746)   Prec@5 71.875 (82.746)   [2018-03-31 18:24:15]
  Epoch: [012][600/1475]   Time 0.141 (0.139)   Data 0.109 (0.105)   Loss 0.3480 (0.3908)   Prec@1 78.125 (82.867)   Prec@5 78.125 (82.867)   [2018-03-31 18:24:43]
  Epoch: [012][800/1475]   Time 0.125 (0.140)   Data 0.094 (0.106)   Loss 0.3423 (0.3911)   Prec@1 90.625 (82.857)   Prec@5 90.625 (82.857)   [2018-03-31 18:25:11]
  Epoch: [012][1000/1475]   Time 0.128 (0.139)   Data 0.097 (0.106)   Loss 0.5491 (0.3909)   Prec@1 65.625 (82.883)   Prec@5 65.625 (82.883)   [2018-03-31 18:25:39]
  Epoch: [012][1200/1475]   Time 0.140 (0.139)   Data 0.107 (0.106)   Loss 0.3644 (0.3888)   Prec@1 84.375 (83.053)   Prec@5 84.375 (83.053)   [2018-03-31 18:26:07]
  Epoch: [012][1400/1475]   Time 0.141 (0.140)   Data 0.109 (0.106)   Loss 0.4163 (0.3901)   Prec@1 84.375 (82.992)   Prec@5 84.375 (82.992)   [2018-03-31 18:26:35]
  **Train** Prec@1 82.971 Prec@5 82.971 Error@1 17.029
  **VAL** Prec@1 85.707 Prec@5 85.707 Error@1 14.293

==>>[2018-03-31 18:27:03] [Epoch=013/250] [Need: 14:40:32] [learning_rate=0.0020] [Best : Accuracy=85.71, Error=14.29]

==>>Epoch=[013/250]], [2018-03-31 18:27:03], LR=[0.002], Batch=[32] [Model=SimpleNet]
  Epoch: [013][000/1475]   Time 0.141 (0.141)   Data 0.109 (0.109)   Loss 0.2650 (0.2650)   Prec@1 90.625 (90.625)   Prec@5 90.625 (90.625)   [2018-03-31 18:27:03]
  Epoch: [013][200/1475]   Time 0.141 (0.140)   Data 0.109 (0.106)   Loss 0.2744 (0.3845)   Prec@1 87.500 (83.178)   Prec@5 87.500 (83.178)   [2018-03-31 18:27:31]
  Epoch: [013][400/1475]   Time 0.141 (0.140)   Data 0.109 (0.106)   Loss 0.3186 (0.3848)   Prec@1 81.250 (83.128)   Prec@5 81.250 (83.128)   [2018-03-31 18:27:59]
  Epoch: [013][600/1475]   Time 0.135 (0.140)   Data 0.114 (0.106)   Loss 0.4942 (0.3864)   Prec@1 71.875 (83.137)   Prec@5 71.875 (83.137)   [2018-03-31 18:28:27]
  Epoch: [013][800/1475]   Time 0.141 (0.140)   Data 0.109 (0.106)   Loss 0.3246 (0.3887)   Prec@1 84.375 (83.166)   Prec@5 84.375 (83.166)   [2018-03-31 18:28:55]
  Epoch: [013][1000/1475]   Time 0.141 (0.140)   Data 0.094 (0.106)   Loss 0.3202 (0.3880)   Prec@1 87.500 (83.201)   Prec@5 87.500 (83.201)   [2018-03-31 18:29:23]
  Epoch: [013][1200/1475]   Time 0.141 (0.140)   Data 0.094 (0.106)   Loss 0.2596 (0.3855)   Prec@1 84.375 (83.264)   Prec@5 84.375 (83.264)   [2018-03-31 18:29:51]
  Epoch: [013][1400/1475]   Time 0.144 (0.140)   Data 0.113 (0.106)   Loss 0.2810 (0.3866)   Prec@1 90.625 (83.235)   Prec@5 90.625 (83.235)   [2018-03-31 18:30:18]
  **Train** Prec@1 83.194 Prec@5 83.194 Error@1 16.806
  **VAL** Prec@1 85.852 Prec@5 85.852 Error@1 14.148

==>>[2018-03-31 18:30:47] [Epoch=014/250] [Need: 14:37:05] [learning_rate=0.0020] [Best : Accuracy=85.85, Error=14.15]

==>>Epoch=[014/250]], [2018-03-31 18:30:47], LR=[0.002], Batch=[32] [Model=SimpleNet]
  Epoch: [014][000/1475]   Time 0.141 (0.141)   Data 0.094 (0.094)   Loss 0.1925 (0.1925)   Prec@1 93.750 (93.750)   Prec@5 93.750 (93.750)   [2018-03-31 18:30:47]
  Epoch: [014][200/1475]   Time 0.141 (0.140)   Data 0.109 (0.106)   Loss 0.2857 (0.3895)   Prec@1 93.750 (83.240)   Prec@5 93.750 (83.240)   [2018-03-31 18:31:15]
  Epoch: [014][400/1475]   Time 0.144 (0.140)   Data 0.113 (0.106)   Loss 0.4124 (0.3819)   Prec@1 78.125 (83.479)   Prec@5 78.125 (83.479)   [2018-03-31 18:31:43]
  Epoch: [014][600/1475]   Time 0.141 (0.140)   Data 0.109 (0.106)   Loss 0.2291 (0.3873)   Prec@1 93.750 (83.085)   Prec@5 93.750 (83.085)   [2018-03-31 18:32:11]
  Epoch: [014][800/1475]   Time 0.141 (0.140)   Data 0.109 (0.106)   Loss 0.5454 (0.3900)   Prec@1 81.250 (83.091)   Prec@5 81.250 (83.091)   [2018-03-31 18:32:39]
  Epoch: [014][1000/1475]   Time 0.141 (0.140)   Data 0.109 (0.106)   Loss 0.2111 (0.3892)   Prec@1 87.500 (83.161)   Prec@5 87.500 (83.161)   [2018-03-31 18:33:07]
  Epoch: [014][1200/1475]   Time 0.141 (0.140)   Data 0.109 (0.106)   Loss 0.2219 (0.3902)   Prec@1 93.750 (83.142)   Prec@5 93.750 (83.142)   [2018-03-31 18:33:34]
  Epoch: [014][1400/1475]   Time 0.141 (0.140)   Data 0.109 (0.106)   Loss 0.2131 (0.3877)   Prec@1 90.625 (83.235)   Prec@5 90.625 (83.235)   [2018-03-31 18:34:02]
  **Train** Prec@1 83.255 Prec@5 83.255 Error@1 16.745
  **VAL** Prec@1 83.762 Prec@5 83.762 Error@1 16.238

==>>[2018-03-31 18:34:31] [Epoch=015/250] [Need: 14:33:37] [learning_rate=0.0020] [Best : Accuracy=85.85, Error=14.15]

==>>Epoch=[015/250]], [2018-03-31 18:34:31], LR=[0.002], Batch=[32] [Model=SimpleNet]
  Epoch: [015][000/1475]   Time 0.141 (0.141)   Data 0.109 (0.109)   Loss 0.2756 (0.2756)   Prec@1 90.625 (90.625)   Prec@5 90.625 (90.625)   [2018-03-31 18:34:31]
  Epoch: [015][200/1475]   Time 0.141 (0.140)   Data 0.109 (0.106)   Loss 0.4091 (0.3789)   Prec@1 81.250 (83.784)   Prec@5 81.250 (83.784)   [2018-03-31 18:34:59]
  Epoch: [015][400/1475]   Time 0.141 (0.140)   Data 0.109 (0.106)   Loss 0.4977 (0.3863)   Prec@1 71.875 (83.315)   Prec@5 71.875 (83.315)   [2018-03-31 18:35:27]
  Epoch: [015][600/1475]   Time 0.141 (0.140)   Data 0.109 (0.105)   Loss 0.3930 (0.3905)   Prec@1 78.125 (83.132)   Prec@5 78.125 (83.132)   [2018-03-31 18:35:55]
  Epoch: [015][800/1475]   Time 0.147 (0.140)   Data 0.116 (0.105)   Loss 0.4084 (0.3914)   Prec@1 84.375 (83.049)   Prec@5 84.375 (83.049)   [2018-03-31 18:36:23]
  Epoch: [015][1000/1475]   Time 0.141 (0.140)   Data 0.109 (0.106)   Loss 0.4237 (0.3886)   Prec@1 81.250 (83.136)   Prec@5 81.250 (83.136)   [2018-03-31 18:36:50]
  Epoch: [015][1200/1475]   Time 0.138 (0.140)   Data 0.118 (0.106)   Loss 0.2731 (0.3889)   Prec@1 90.625 (83.108)   Prec@5 90.625 (83.108)   [2018-03-31 18:37:18]
  Epoch: [015][1400/1475]   Time 0.143 (0.140)   Data 0.112 (0.106)   Loss 0.5375 (0.3881)   Prec@1 68.750 (83.171)   Prec@5 68.750 (83.171)   [2018-03-31 18:37:46]
  **Train** Prec@1 83.183 Prec@5 83.183 Error@1 16.817
  **VAL** Prec@1 84.987 Prec@5 84.987 Error@1 15.013

==>>[2018-03-31 18:38:15] [Epoch=016/250] [Need: 14:30:08] [learning_rate=0.0020] [Best : Accuracy=85.85, Error=14.15]

==>>Epoch=[016/250]], [2018-03-31 18:38:15], LR=[0.002], Batch=[32] [Model=SimpleNet]
  Epoch: [016][000/1475]   Time 0.145 (0.145)   Data 0.098 (0.098)   Loss 0.4204 (0.4204)   Prec@1 87.500 (87.500)   Prec@5 87.500 (87.500)   [2018-03-31 18:38:15]
  Epoch: [016][200/1475]   Time 0.141 (0.140)   Data 0.109 (0.106)   Loss 0.4342 (0.3922)   Prec@1 75.000 (82.991)   Prec@5 75.000 (82.991)   [2018-03-31 18:38:43]
  Epoch: [016][400/1475]   Time 0.141 (0.140)   Data 0.109 (0.106)   Loss 0.4053 (0.3855)   Prec@1 81.250 (83.276)   Prec@5 81.250 (83.276)   [2018-03-31 18:39:11]
  Epoch: [016][600/1475]   Time 0.141 (0.140)   Data 0.109 (0.106)   Loss 0.4057 (0.3837)   Prec@1 84.375 (83.439)   Prec@5 84.375 (83.439)   [2018-03-31 18:39:39]
  Epoch: [016][800/1475]   Time 0.141 (0.140)   Data 0.109 (0.106)   Loss 0.3507 (0.3818)   Prec@1 87.500 (83.482)   Prec@5 87.500 (83.482)   [2018-03-31 18:40:06]
  Epoch: [016][1000/1475]   Time 0.141 (0.139)   Data 0.109 (0.106)   Loss 0.3716 (0.3852)   Prec@1 84.375 (83.195)   Prec@5 84.375 (83.195)   [2018-03-31 18:40:34]
  Epoch: [016][1200/1475]   Time 0.135 (0.139)   Data 0.104 (0.106)   Loss 0.3565 (0.3850)   Prec@1 84.375 (83.220)   Prec@5 84.375 (83.220)   [2018-03-31 18:41:02]
  Epoch: [016][1400/1475]   Time 0.137 (0.140)   Data 0.106 (0.106)   Loss 0.4617 (0.3833)   Prec@1 78.125 (83.362)   Prec@5 78.125 (83.362)   [2018-03-31 18:41:30]
  **Train** Prec@1 83.363 Prec@5 83.363 Error@1 16.637
  **VAL** Prec@1 85.635 Prec@5 85.635 Error@1 14.365

==>>[2018-03-31 18:41:59] [Epoch=017/250] [Need: 14:26:35] [learning_rate=0.0020] [Best : Accuracy=85.85, Error=14.15]

==>>Epoch=[017/250]], [2018-03-31 18:41:59], LR=[0.002], Batch=[32] [Model=SimpleNet]
  Epoch: [017][000/1475]   Time 0.141 (0.141)   Data 0.109 (0.109)   Loss 0.3172 (0.3172)   Prec@1 87.500 (87.500)   Prec@5 87.500 (87.500)   [2018-03-31 18:41:59]
  Epoch: [017][200/1475]   Time 0.137 (0.140)   Data 0.090 (0.106)   Loss 0.2826 (0.3839)   Prec@1 84.375 (83.567)   Prec@5 84.375 (83.567)   [2018-03-31 18:42:27]
  Epoch: [017][400/1475]   Time 0.141 (0.140)   Data 0.094 (0.106)   Loss 0.3774 (0.3859)   Prec@1 81.250 (83.339)   Prec@5 81.250 (83.339)   [2018-03-31 18:42:55]
  Epoch: [017][600/1475]   Time 0.141 (0.140)   Data 0.109 (0.106)   Loss 0.1443 (0.3871)   Prec@1 100.000 (83.377)   Prec@5 100.000 (83.377)   [2018-03-31 18:43:22]
  Epoch: [017][800/1475]   Time 0.141 (0.140)   Data 0.109 (0.106)   Loss 0.2905 (0.3880)   Prec@1 81.250 (83.263)   Prec@5 81.250 (83.263)   [2018-03-31 18:43:50]
  Epoch: [017][1000/1475]   Time 0.129 (0.140)   Data 0.098 (0.106)   Loss 0.4317 (0.3866)   Prec@1 81.250 (83.201)   Prec@5 81.250 (83.201)   [2018-03-31 18:44:18]
  Epoch: [017][1200/1475]   Time 0.131 (0.140)   Data 0.100 (0.106)   Loss 0.3143 (0.3859)   Prec@1 84.375 (83.215)   Prec@5 84.375 (83.215)   [2018-03-31 18:44:46]
  Epoch: [017][1400/1475]   Time 0.146 (0.140)   Data 0.109 (0.106)   Loss 0.5002 (0.3859)   Prec@1 75.000 (83.211)   Prec@5 75.000 (83.211)   [2018-03-31 18:45:14]
  **Train** Prec@1 83.190 Prec@5 83.190 Error@1 16.810
  **VAL** Prec@1 86.044 Prec@5 86.044 Error@1 13.956

==>>[2018-03-31 18:45:42] [Epoch=018/250] [Need: 14:23:01] [learning_rate=0.0020] [Best : Accuracy=86.04, Error=13.96]

==>>Epoch=[018/250]], [2018-03-31 18:45:42], LR=[0.002], Batch=[32] [Model=SimpleNet]
  Epoch: [018][000/1475]   Time 0.141 (0.141)   Data 0.109 (0.109)   Loss 0.2975 (0.2975)   Prec@1 87.500 (87.500)   Prec@5 87.500 (87.500)   [2018-03-31 18:45:43]
  Epoch: [018][200/1475]   Time 0.125 (0.141)   Data 0.094 (0.107)   Loss 0.3777 (0.3720)   Prec@1 84.375 (84.095)   Prec@5 84.375 (84.095)   [2018-03-31 18:46:11]
  Epoch: [018][400/1475]   Time 0.125 (0.140)   Data 0.094 (0.106)   Loss 0.2381 (0.3849)   Prec@1 90.625 (83.346)   Prec@5 90.625 (83.346)   [2018-03-31 18:46:39]
  Epoch: [018][600/1475]   Time 0.141 (0.140)   Data 0.094 (0.106)   Loss 0.3617 (0.3824)   Prec@1 81.250 (83.387)   Prec@5 81.250 (83.387)   [2018-03-31 18:47:07]
  Epoch: [018][800/1475]   Time 0.142 (0.140)   Data 0.111 (0.106)   Loss 0.2615 (0.3812)   Prec@1 90.625 (83.466)   Prec@5 90.625 (83.466)   [2018-03-31 18:47:35]
  Epoch: [018][1000/1475]   Time 0.141 (0.140)   Data 0.109 (0.106)   Loss 0.4343 (0.3795)   Prec@1 84.375 (83.613)   Prec@5 84.375 (83.613)   [2018-03-31 18:48:02]
  Epoch: [018][1200/1475]   Time 0.141 (0.140)   Data 0.109 (0.106)   Loss 0.2951 (0.3806)   Prec@1 87.500 (83.509)   Prec@5 87.500 (83.509)   [2018-03-31 18:48:30]
  Epoch: [018][1400/1475]   Time 0.141 (0.140)   Data 0.109 (0.106)   Loss 0.2102 (0.3801)   Prec@1 93.750 (83.570)   Prec@5 93.750 (83.570)   [2018-03-31 18:48:58]
  **Train** Prec@1 83.556 Prec@5 83.556 Error@1 16.444
  **VAL** Prec@1 85.779 Prec@5 85.779 Error@1 14.221

==>>[2018-03-31 18:49:27] [Epoch=019/250] [Need: 14:19:31] [learning_rate=0.0020] [Best : Accuracy=86.04, Error=13.96]

==>>Epoch=[019/250]], [2018-03-31 18:49:27], LR=[0.002], Batch=[32] [Model=SimpleNet]
  Epoch: [019][000/1475]   Time 0.141 (0.141)   Data 0.109 (0.109)   Loss 0.3376 (0.3376)   Prec@1 87.500 (87.500)   Prec@5 87.500 (87.500)   [2018-03-31 18:49:27]
  Epoch: [019][200/1475]   Time 0.141 (0.140)   Data 0.109 (0.106)   Loss 0.3522 (0.3982)   Prec@1 90.625 (82.478)   Prec@5 90.625 (82.478)   [2018-03-31 18:49:55]
  Epoch: [019][400/1475]   Time 0.141 (0.140)   Data 0.109 (0.106)   Loss 0.2606 (0.3829)   Prec@1 90.625 (83.354)   Prec@5 90.625 (83.354)   [2018-03-31 18:50:23]
  Epoch: [019][600/1475]   Time 0.141 (0.140)   Data 0.109 (0.106)   Loss 0.3126 (0.3812)   Prec@1 90.625 (83.512)   Prec@5 90.625 (83.512)   [2018-03-31 18:50:51]
  Epoch: [019][800/1475]   Time 0.141 (0.140)   Data 0.109 (0.106)   Loss 0.3645 (0.3829)   Prec@1 81.250 (83.446)   Prec@5 81.250 (83.446)   [2018-03-31 18:51:18]
  Epoch: [019][1000/1475]   Time 0.125 (0.139)   Data 0.094 (0.106)   Loss 0.2724 (0.3828)   Prec@1 87.500 (83.404)   Prec@5 87.500 (83.404)   [2018-03-31 18:51:46]
  Epoch: [019][1200/1475]   Time 0.148 (0.139)   Data 0.116 (0.106)   Loss 0.3694 (0.3825)   Prec@1 81.250 (83.378)   Prec@5 81.250 (83.378)   [2018-03-31 18:52:14]
  Epoch: [019][1400/1475]   Time 0.141 (0.139)   Data 0.109 (0.106)   Loss 0.5273 (0.3817)   Prec@1 78.125 (83.376)   Prec@5 78.125 (83.376)   [2018-03-31 18:52:42]
  **Train** Prec@1 83.353 Prec@5 83.353 Error@1 16.647
  **VAL** Prec@1 86.188 Prec@5 86.188 Error@1 13.812

==>>[2018-03-31 18:53:10] [Epoch=020/250] [Need: 14:15:53] [learning_rate=0.0020] [Best : Accuracy=86.19, Error=13.81]

==>>Epoch=[020/250]], [2018-03-31 18:53:10], LR=[0.002], Batch=[32] [Model=SimpleNet]
  Epoch: [020][000/1475]   Time 0.135 (0.135)   Data 0.105 (0.105)   Loss 0.4039 (0.4039)   Prec@1 84.375 (84.375)   Prec@5 84.375 (84.375)   [2018-03-31 18:53:11]
  Epoch: [020][200/1475]   Time 0.141 (0.139)   Data 0.109 (0.106)   Loss 0.3641 (0.3902)   Prec@1 81.250 (83.038)   Prec@5 81.250 (83.038)   [2018-03-31 18:53:38]
  Epoch: [020][400/1475]   Time 0.141 (0.139)   Data 0.109 (0.105)   Loss 0.2756 (0.3862)   Prec@1 90.625 (83.268)   Prec@5 90.625 (83.268)   [2018-03-31 18:54:06]
  Epoch: [020][600/1475]   Time 0.147 (0.139)   Data 0.100 (0.106)   Loss 0.5149 (0.3858)   Prec@1 68.750 (83.226)   Prec@5 68.750 (83.226)   [2018-03-31 18:54:34]
  Epoch: [020][800/1475]   Time 0.146 (0.139)   Data 0.099 (0.106)   Loss 0.2387 (0.3846)   Prec@1 90.625 (83.329)   Prec@5 90.625 (83.329)   [2018-03-31 18:55:02]
  Epoch: [020][1000/1475]   Time 0.144 (0.139)   Data 0.113 (0.106)   Loss 0.2976 (0.3844)   Prec@1 87.500 (83.360)   Prec@5 87.500 (83.360)   [2018-03-31 18:55:30]
  Epoch: [020][1200/1475]   Time 0.141 (0.139)   Data 0.094 (0.106)   Loss 0.4626 (0.3829)   Prec@1 81.250 (83.462)   Prec@5 81.250 (83.462)   [2018-03-31 18:55:58]
  Epoch: [020][1400/1475]   Time 0.141 (0.139)   Data 0.094 (0.106)   Loss 0.3905 (0.3823)   Prec@1 84.375 (83.489)   Prec@5 84.375 (83.489)   [2018-03-31 18:56:26]
  **Train** Prec@1 83.486 Prec@5 83.486 Error@1 16.514
  **VAL** Prec@1 85.816 Prec@5 85.816 Error@1 14.184

==>>[2018-03-31 18:56:54] [Epoch=021/250] [Need: 14:12:16] [learning_rate=0.0020] [Best : Accuracy=86.19, Error=13.81]

==>>Epoch=[021/250]], [2018-03-31 18:56:54], LR=[0.002], Batch=[32] [Model=SimpleNet]
  Epoch: [021][000/1475]   Time 0.141 (0.141)   Data 0.109 (0.109)   Loss 0.4184 (0.4184)   Prec@1 78.125 (78.125)   Prec@5 78.125 (78.125)   [2018-03-31 18:56:54]
  Epoch: [021][200/1475]   Time 0.145 (0.140)   Data 0.113 (0.106)   Loss 0.2776 (0.3733)   Prec@1 87.500 (83.706)   Prec@5 87.500 (83.706)   [2018-03-31 18:57:22]
  Epoch: [021][400/1475]   Time 0.156 (0.140)   Data 0.109 (0.106)   Loss 0.5064 (0.3769)   Prec@1 75.000 (83.650)   Prec@5 75.000 (83.650)   [2018-03-31 18:57:50]
  Epoch: [021][600/1475]   Time 0.145 (0.140)   Data 0.109 (0.106)   Loss 0.3880 (0.3781)   Prec@1 81.250 (83.673)   Prec@5 81.250 (83.673)   [2018-03-31 18:58:18]
  Epoch: [021][800/1475]   Time 0.141 (0.140)   Data 0.109 (0.106)   Loss 0.4697 (0.3810)   Prec@1 78.125 (83.450)   Prec@5 78.125 (83.450)   [2018-03-31 18:58:46]
  Epoch: [021][1000/1475]   Time 0.141 (0.140)   Data 0.109 (0.106)   Loss 0.5128 (0.3803)   Prec@1 78.125 (83.563)   Prec@5 78.125 (83.563)   [2018-03-31 18:59:14]
  Epoch: [021][1200/1475]   Time 0.136 (0.140)   Data 0.105 (0.106)   Loss 0.3655 (0.3802)   Prec@1 84.375 (83.576)   Prec@5 84.375 (83.576)   [2018-03-31 18:59:42]
  Epoch: [021][1400/1475]   Time 0.141 (0.140)   Data 0.109 (0.106)   Loss 0.1823 (0.3785)   Prec@1 93.750 (83.643)   Prec@5 93.750 (83.643)   [2018-03-31 19:00:10]
  **Train** Prec@1 83.624 Prec@5 83.624 Error@1 16.376
  **VAL** Prec@1 85.371 Prec@5 85.371 Error@1 14.629

==>>[2018-03-31 19:00:38] [Epoch=022/250] [Need: 14:08:38] [learning_rate=0.0020] [Best : Accuracy=86.19, Error=13.81]

==>>Epoch=[022/250]], [2018-03-31 19:00:38], LR=[0.002], Batch=[32] [Model=SimpleNet]
  Epoch: [022][000/1475]   Time 0.145 (0.145)   Data 0.114 (0.114)   Loss 0.4830 (0.4830)   Prec@1 75.000 (75.000)   Prec@5 75.000 (75.000)   [2018-03-31 19:00:38]
  Epoch: [022][200/1475]   Time 0.141 (0.140)   Data 0.109 (0.107)   Loss 0.5086 (0.3755)   Prec@1 81.250 (84.173)   Prec@5 81.250 (84.173)   [2018-03-31 19:01:06]
  Epoch: [022][400/1475]   Time 0.128 (0.140)   Data 0.109 (0.107)   Loss 0.3223 (0.3804)   Prec@1 84.375 (83.853)   Prec@5 84.375 (83.853)   [2018-03-31 19:01:34]
  Epoch: [022][600/1475]   Time 0.141 (0.140)   Data 0.094 (0.107)   Loss 0.3505 (0.3836)   Prec@1 87.500 (83.652)   Prec@5 87.500 (83.652)   [2018-03-31 19:02:02]
  Epoch: [022][800/1475]   Time 0.141 (0.140)   Data 0.094 (0.106)   Loss 0.4555 (0.3828)   Prec@1 84.375 (83.595)   Prec@5 84.375 (83.595)   [2018-03-31 19:02:30]
  Epoch: [022][1000/1475]   Time 0.141 (0.140)   Data 0.109 (0.106)   Loss 0.3029 (0.3828)   Prec@1 90.625 (83.560)   Prec@5 90.625 (83.560)   [2018-03-31 19:02:58]
  Epoch: [022][1200/1475]   Time 0.141 (0.140)   Data 0.109 (0.106)   Loss 0.3719 (0.3814)   Prec@1 90.625 (83.615)   Prec@5 90.625 (83.615)   [2018-03-31 19:03:26]
  Epoch: [022][1400/1475]   Time 0.129 (0.140)   Data 0.109 (0.106)   Loss 0.2359 (0.3808)   Prec@1 96.875 (83.650)   Prec@5 96.875 (83.650)   [2018-03-31 19:03:54]
  **Train** Prec@1 83.639 Prec@5 83.639 Error@1 16.361
  **VAL** Prec@1 85.659 Prec@5 85.659 Error@1 14.341

==>>[2018-03-31 19:04:22] [Epoch=023/250] [Need: 14:05:02] [learning_rate=0.0020] [Best : Accuracy=86.19, Error=13.81]

==>>Epoch=[023/250]], [2018-03-31 19:04:22], LR=[0.002], Batch=[32] [Model=SimpleNet]
  Epoch: [023][000/1475]   Time 0.127 (0.127)   Data 0.096 (0.096)   Loss 0.3338 (0.3338)   Prec@1 87.500 (87.500)   Prec@5 87.500 (87.500)   [2018-03-31 19:04:22]
  Epoch: [023][200/1475]   Time 0.141 (0.140)   Data 0.094 (0.105)   Loss 0.3022 (0.3765)   Prec@1 84.375 (83.520)   Prec@5 84.375 (83.520)   [2018-03-31 19:04:50]
  Epoch: [023][400/1475]   Time 0.141 (0.140)   Data 0.109 (0.105)   Loss 0.3271 (0.3786)   Prec@1 84.375 (83.510)   Prec@5 84.375 (83.510)   [2018-03-31 19:05:18]
  Epoch: [023][600/1475]   Time 0.141 (0.139)   Data 0.110 (0.105)   Loss 0.3251 (0.3803)   Prec@1 87.500 (83.429)   Prec@5 87.500 (83.429)   [2018-03-31 19:05:46]
  Epoch: [023][800/1475]   Time 0.141 (0.140)   Data 0.109 (0.106)   Loss 0.2221 (0.3819)   Prec@1 93.750 (83.485)   Prec@5 93.750 (83.485)   [2018-03-31 19:06:14]
  Epoch: [023][1000/1475]   Time 0.141 (0.140)   Data 0.109 (0.106)   Loss 0.3520 (0.3818)   Prec@1 84.375 (83.432)   Prec@5 84.375 (83.432)   [2018-03-31 19:06:42]
  Epoch: [023][1200/1475]   Time 0.125 (0.140)   Data 0.094 (0.106)   Loss 0.3934 (0.3802)   Prec@1 78.125 (83.620)   Prec@5 78.125 (83.620)   [2018-03-31 19:07:10]
  Epoch: [023][1400/1475]   Time 0.141 (0.140)   Data 0.094 (0.106)   Loss 0.2811 (0.3820)   Prec@1 81.250 (83.505)   Prec@5 81.250 (83.505)   [2018-03-31 19:07:38]
  **Train** Prec@1 83.529 Prec@5 83.529 Error@1 16.471
  **VAL** Prec@1 86.140 Prec@5 86.140 Error@1 13.860

==>>[2018-03-31 19:08:06] [Epoch=024/250] [Need: 14:01:24] [learning_rate=0.0020] [Best : Accuracy=86.19, Error=13.81]

==>>Epoch=[024/250]], [2018-03-31 19:08:06], LR=[0.002], Batch=[32] [Model=SimpleNet]
  Epoch: [024][000/1475]   Time 0.141 (0.141)   Data 0.094 (0.094)   Loss 0.2904 (0.2904)   Prec@1 90.625 (90.625)   Prec@5 90.625 (90.625)   [2018-03-31 19:08:06]
  Epoch: [024][200/1475]   Time 0.134 (0.140)   Data 0.103 (0.106)   Loss 0.4645 (0.3776)   Prec@1 71.875 (83.691)   Prec@5 71.875 (83.691)   [2018-03-31 19:08:34]
  Epoch: [024][400/1475]   Time 0.141 (0.140)   Data 0.109 (0.106)   Loss 0.3645 (0.3822)   Prec@1 81.250 (83.416)   Prec@5 81.250 (83.416)   [2018-03-31 19:09:02]
  Epoch: [024][600/1475]   Time 0.141 (0.140)   Data 0.094 (0.106)   Loss 0.4495 (0.3837)   Prec@1 84.375 (83.449)   Prec@5 84.375 (83.449)   [2018-03-31 19:09:30]
  Epoch: [024][800/1475]   Time 0.141 (0.140)   Data 0.109 (0.106)   Loss 0.1548 (0.3815)   Prec@1 96.875 (83.501)   Prec@5 96.875 (83.501)   [2018-03-31 19:09:58]
  Epoch: [024][1000/1475]   Time 0.141 (0.140)   Data 0.109 (0.106)   Loss 0.2670 (0.3789)   Prec@1 90.625 (83.679)   Prec@5 90.625 (83.679)   [2018-03-31 19:10:26]
  Epoch: [024][1200/1475]   Time 0.141 (0.140)   Data 0.094 (0.106)   Loss 0.3560 (0.3797)   Prec@1 87.500 (83.597)   Prec@5 87.500 (83.597)   [2018-03-31 19:10:54]
  Epoch: [024][1400/1475]   Time 0.141 (0.140)   Data 0.109 (0.106)   Loss 0.2827 (0.3800)   Prec@1 84.375 (83.530)   Prec@5 84.375 (83.530)   [2018-03-31 19:11:22]
  **Train** Prec@1 83.541 Prec@5 83.541 Error@1 16.459
  **VAL** Prec@1 85.924 Prec@5 85.924 Error@1 14.076

==>>[2018-03-31 19:11:50] [Epoch=025/250] [Need: 13:57:47] [learning_rate=0.0020] [Best : Accuracy=86.19, Error=13.81]

==>>Epoch=[025/250]], [2018-03-31 19:11:50], LR=[0.002], Batch=[32] [Model=SimpleNet]
  Epoch: [025][000/1475]   Time 0.141 (0.141)   Data 0.110 (0.110)   Loss 0.5015 (0.5015)   Prec@1 68.750 (68.750)   Prec@5 68.750 (68.750)   [2018-03-31 19:11:50]
  Epoch: [025][200/1475]   Time 0.156 (0.140)   Data 0.125 (0.106)   Loss 0.4325 (0.3829)   Prec@1 81.250 (83.364)   Prec@5 81.250 (83.364)   [2018-03-31 19:12:18]
  Epoch: [025][400/1475]   Time 0.141 (0.140)   Data 0.094 (0.106)   Loss 0.2618 (0.3772)   Prec@1 96.875 (83.650)   Prec@5 96.875 (83.650)   [2018-03-31 19:12:46]
  Epoch: [025][600/1475]   Time 0.141 (0.139)   Data 0.109 (0.106)   Loss 0.4019 (0.3785)   Prec@1 81.250 (83.683)   Prec@5 81.250 (83.683)   [2018-03-31 19:13:14]
  Epoch: [025][800/1475]   Time 0.141 (0.139)   Data 0.109 (0.106)   Loss 0.5024 (0.3807)   Prec@1 71.875 (83.634)   Prec@5 71.875 (83.634)   [2018-03-31 19:13:42]
  Epoch: [025][1000/1475]   Time 0.141 (0.139)   Data 0.094 (0.106)   Loss 0.3029 (0.3827)   Prec@1 87.500 (83.520)   Prec@5 87.500 (83.520)   [2018-03-31 19:14:10]
  Epoch: [025][1200/1475]   Time 0.141 (0.139)   Data 0.094 (0.106)   Loss 0.4232 (0.3822)   Prec@1 78.125 (83.542)   Prec@5 78.125 (83.542)   [2018-03-31 19:14:38]
  Epoch: [025][1400/1475]   Time 0.141 (0.139)   Data 0.109 (0.106)   Loss 0.3052 (0.3806)   Prec@1 90.625 (83.628)   Prec@5 90.625 (83.628)   [2018-03-31 19:15:05]
  **Train** Prec@1 83.594 Prec@5 83.594 Error@1 16.406
  **VAL** Prec@1 86.404 Prec@5 86.404 Error@1 13.596

==>>[2018-03-31 19:15:34] [Epoch=026/250] [Need: 13:54:05] [learning_rate=0.0020] [Best : Accuracy=86.40, Error=13.60]

==>>Epoch=[026/250]], [2018-03-31 19:15:34], LR=[0.002], Batch=[32] [Model=SimpleNet]
  Epoch: [026][000/1475]   Time 0.126 (0.126)   Data 0.095 (0.095)   Loss 0.5014 (0.5014)   Prec@1 75.000 (75.000)   Prec@5 75.000 (75.000)   [2018-03-31 19:15:34]
  Epoch: [026][200/1475]   Time 0.141 (0.140)   Data 0.109 (0.106)   Loss 0.5809 (0.3784)   Prec@1 71.875 (83.862)   Prec@5 71.875 (83.862)   [2018-03-31 19:16:02]
  Epoch: [026][400/1475]   Time 0.141 (0.139)   Data 0.109 (0.106)   Loss 0.4639 (0.3735)   Prec@1 84.375 (83.837)   Prec@5 84.375 (83.837)   [2018-03-31 19:16:30]
  Epoch: [026][600/1475]   Time 0.141 (0.139)   Data 0.109 (0.106)   Loss 0.2871 (0.3779)   Prec@1 87.500 (83.689)   Prec@5 87.500 (83.689)   [2018-03-31 19:16:58]
  Epoch: [026][800/1475]   Time 0.141 (0.140)   Data 0.094 (0.106)   Loss 0.3780 (0.3806)   Prec@1 87.500 (83.521)   Prec@5 87.500 (83.521)   [2018-03-31 19:17:26]
  Epoch: [026][1000/1475]   Time 0.141 (0.140)   Data 0.109 (0.106)   Loss 0.2620 (0.3787)   Prec@1 90.625 (83.629)   Prec@5 90.625 (83.629)   [2018-03-31 19:17:54]
  Epoch: [026][1200/1475]   Time 0.143 (0.140)   Data 0.112 (0.106)   Loss 0.3799 (0.3779)   Prec@1 84.375 (83.659)   Prec@5 84.375 (83.659)   [2018-03-31 19:18:21]
  Epoch: [026][1400/1475]   Time 0.141 (0.140)   Data 0.109 (0.106)   Loss 0.3205 (0.3807)   Prec@1 90.625 (83.534)   Prec@5 90.625 (83.534)   [2018-03-31 19:18:49]
  **Train** Prec@1 83.546 Prec@5 83.546 Error@1 16.454
  **VAL** Prec@1 85.023 Prec@5 85.023 Error@1 14.977

==>>[2018-03-31 19:19:18] [Epoch=027/250] [Need: 13:50:26] [learning_rate=0.0020] [Best : Accuracy=86.40, Error=13.60]

==>>Epoch=[027/250]], [2018-03-31 19:19:18], LR=[0.002], Batch=[32] [Model=SimpleNet]
  Epoch: [027][000/1475]   Time 0.138 (0.138)   Data 0.107 (0.107)   Loss 0.3873 (0.3873)   Prec@1 81.250 (81.250)   Prec@5 81.250 (81.250)   [2018-03-31 19:19:18]
  Epoch: [027][200/1475]   Time 0.138 (0.140)   Data 0.091 (0.106)   Loss 0.5900 (0.3788)   Prec@1 68.750 (83.349)   Prec@5 68.750 (83.349)   [2018-03-31 19:19:46]
  Epoch: [027][400/1475]   Time 0.151 (0.139)   Data 0.109 (0.106)   Loss 0.4272 (0.3840)   Prec@1 84.375 (83.261)   Prec@5 84.375 (83.261)   [2018-03-31 19:20:14]
  Epoch: [027][600/1475]   Time 0.141 (0.140)   Data 0.109 (0.106)   Loss 0.5835 (0.3858)   Prec@1 65.625 (83.366)   Prec@5 65.625 (83.366)   [2018-03-31 19:20:42]
  Epoch: [027][800/1475]   Time 0.141 (0.139)   Data 0.109 (0.106)   Loss 0.2805 (0.3860)   Prec@1 90.625 (83.240)   Prec@5 90.625 (83.240)   [2018-03-31 19:21:09]
  Epoch: [027][1000/1475]   Time 0.141 (0.139)   Data 0.109 (0.106)   Loss 0.3638 (0.3845)   Prec@1 87.500 (83.398)   Prec@5 87.500 (83.398)   [2018-03-31 19:21:37]
  Epoch: [027][1200/1475]   Time 0.148 (0.139)   Data 0.116 (0.106)   Loss 0.1659 (0.3822)   Prec@1 96.875 (83.519)   Prec@5 96.875 (83.519)   [2018-03-31 19:22:05]
  Epoch: [027][1400/1475]   Time 0.142 (0.139)   Data 0.110 (0.106)   Loss 0.2304 (0.3807)   Prec@1 87.500 (83.597)   Prec@5 87.500 (83.597)   [2018-03-31 19:22:33]
  **Train** Prec@1 83.569 Prec@5 83.569 Error@1 16.431
  **VAL** Prec@1 86.524 Prec@5 86.524 Error@1 13.476

==>>[2018-03-31 19:23:01] [Epoch=028/250] [Need: 13:46:45] [learning_rate=0.0020] [Best : Accuracy=86.52, Error=13.48]

==>>Epoch=[028/250]], [2018-03-31 19:23:01], LR=[0.002], Batch=[32] [Model=SimpleNet]
  Epoch: [028][000/1475]   Time 0.141 (0.141)   Data 0.094 (0.094)   Loss 0.3629 (0.3629)   Prec@1 84.375 (84.375)   Prec@5 84.375 (84.375)   [2018-03-31 19:23:02]
  Epoch: [028][200/1475]   Time 0.141 (0.140)   Data 0.094 (0.106)   Loss 0.4026 (0.3757)   Prec@1 87.500 (83.769)   Prec@5 87.500 (83.769)   [2018-03-31 19:23:30]
  Epoch: [028][400/1475]   Time 0.145 (0.140)   Data 0.098 (0.106)   Loss 0.5270 (0.3786)   Prec@1 78.125 (83.697)   Prec@5 78.125 (83.697)   [2018-03-31 19:23:57]
  Epoch: [028][600/1475]   Time 0.141 (0.139)   Data 0.109 (0.106)   Loss 0.2764 (0.3790)   Prec@1 90.625 (83.574)   Prec@5 90.625 (83.574)   [2018-03-31 19:24:25]
  Epoch: [028][800/1475]   Time 0.139 (0.139)   Data 0.108 (0.106)   Loss 0.2890 (0.3812)   Prec@1 87.500 (83.427)   Prec@5 87.500 (83.427)   [2018-03-31 19:24:53]
  Epoch: [028][1000/1475]   Time 0.141 (0.139)   Data 0.109 (0.106)   Loss 0.3294 (0.3819)   Prec@1 84.375 (83.392)   Prec@5 84.375 (83.392)   [2018-03-31 19:25:21]
  Epoch: [028][1200/1475]   Time 0.141 (0.139)   Data 0.109 (0.106)   Loss 0.4310 (0.3825)   Prec@1 78.125 (83.368)   Prec@5 78.125 (83.368)   [2018-03-31 19:25:49]
  Epoch: [028][1400/1475]   Time 0.145 (0.139)   Data 0.114 (0.106)   Loss 0.2479 (0.3802)   Prec@1 90.625 (83.559)   Prec@5 90.625 (83.559)   [2018-03-31 19:26:17]
  **Train** Prec@1 83.565 Prec@5 83.565 Error@1 16.435
  **VAL** Prec@1 85.599 Prec@5 85.599 Error@1 14.401

==>>[2018-03-31 19:26:45] [Epoch=029/250] [Need: 13:43:04] [learning_rate=0.0020] [Best : Accuracy=86.52, Error=13.48]

==>>Epoch=[029/250]], [2018-03-31 19:26:45], LR=[0.002], Batch=[32] [Model=SimpleNet]
  Epoch: [029][000/1475]   Time 0.141 (0.141)   Data 0.094 (0.094)   Loss 0.5576 (0.5576)   Prec@1 68.750 (68.750)   Prec@5 68.750 (68.750)   [2018-03-31 19:26:45]
  Epoch: [029][200/1475]   Time 0.141 (0.139)   Data 0.109 (0.106)   Loss 0.6175 (0.3943)   Prec@1 68.750 (82.634)   Prec@5 68.750 (82.634)   [2018-03-31 19:27:13]
  Epoch: [029][400/1475]   Time 0.146 (0.140)   Data 0.099 (0.106)   Loss 0.3546 (0.3826)   Prec@1 87.500 (83.268)   Prec@5 87.500 (83.268)   [2018-03-31 19:27:41]
  Epoch: [029][600/1475]   Time 0.133 (0.140)   Data 0.114 (0.106)   Loss 0.3396 (0.3794)   Prec@1 84.375 (83.434)   Prec@5 84.375 (83.434)   [2018-03-31 19:28:09]
  Epoch: [029][800/1475]   Time 0.141 (0.140)   Data 0.109 (0.106)   Loss 0.4661 (0.3789)   Prec@1 81.250 (83.497)   Prec@5 81.250 (83.497)   [2018-03-31 19:28:37]
  Epoch: [029][1000/1475]   Time 0.141 (0.139)   Data 0.109 (0.106)   Loss 0.3672 (0.3775)   Prec@1 84.375 (83.532)   Prec@5 84.375 (83.532)   [2018-03-31 19:29:05]
  Epoch: [029][1200/1475]   Time 0.125 (0.139)   Data 0.094 (0.106)   Loss 0.7013 (0.3776)   Prec@1 65.625 (83.607)   Prec@5 65.625 (83.607)   [2018-03-31 19:29:33]
  Epoch: [029][1400/1475]   Time 0.143 (0.140)   Data 0.107 (0.106)   Loss 0.2455 (0.3764)   Prec@1 90.625 (83.646)   Prec@5 90.625 (83.646)   [2018-03-31 19:30:01]
  **Train** Prec@1 83.711 Prec@5 83.711 Error@1 16.289
  **VAL** Prec@1 86.668 Prec@5 86.668 Error@1 13.332

==>>[2018-03-31 19:30:30] [Epoch=030/250] [Need: 13:39:32] [learning_rate=0.0020] [Best : Accuracy=86.67, Error=13.33]

==>>Epoch=[030/250]], [2018-03-31 19:30:30], LR=[0.002], Batch=[32] [Model=SimpleNet]
  Epoch: [030][000/1475]   Time 0.129 (0.129)   Data 0.098 (0.098)   Loss 0.4943 (0.4943)   Prec@1 71.875 (71.875)   Prec@5 71.875 (71.875)   [2018-03-31 19:30:30]
  Epoch: [030][200/1475]   Time 0.145 (0.145)   Data 0.109 (0.111)   Loss 0.4091 (0.3817)   Prec@1 71.875 (83.302)   Prec@5 71.875 (83.302)   [2018-03-31 19:30:59]
  Epoch: [030][400/1475]   Time 0.140 (0.144)   Data 0.106 (0.110)   Loss 0.6893 (0.3742)   Prec@1 71.875 (83.697)   Prec@5 71.875 (83.697)   [2018-03-31 19:31:28]
  Epoch: [030][600/1475]   Time 0.143 (0.144)   Data 0.107 (0.110)   Loss 0.5635 (0.3725)   Prec@1 68.750 (83.824)   Prec@5 68.750 (83.824)   [2018-03-31 19:31:57]
  Epoch: [030][800/1475]   Time 0.146 (0.143)   Data 0.115 (0.109)   Loss 0.3323 (0.3724)   Prec@1 87.500 (83.848)   Prec@5 87.500 (83.848)   [2018-03-31 19:32:25]
  Epoch: [030][1000/1475]   Time 0.152 (0.143)   Data 0.116 (0.108)   Loss 0.4278 (0.3753)   Prec@1 81.250 (83.744)   Prec@5 81.250 (83.744)   [2018-03-31 19:32:53]
  Epoch: [030][1200/1475]   Time 0.141 (0.142)   Data 0.109 (0.108)   Loss 0.3304 (0.3738)   Prec@1 93.750 (83.862)   Prec@5 93.750 (83.862)   [2018-03-31 19:33:21]
  Epoch: [030][1400/1475]   Time 0.141 (0.142)   Data 0.109 (0.108)   Loss 0.5951 (0.3735)   Prec@1 68.750 (83.817)   Prec@5 68.750 (83.817)   [2018-03-31 19:33:49]
  **Train** Prec@1 83.758 Prec@5 83.758 Error@1 16.242
  **VAL** Prec@1 86.344 Prec@5 86.344 Error@1 13.656

==>>[2018-03-31 19:34:18] [Epoch=031/250] [Need: 13:36:19] [learning_rate=0.0020] [Best : Accuracy=86.67, Error=13.33]

==>>Epoch=[031/250]], [2018-03-31 19:34:18], LR=[0.002], Batch=[32] [Model=SimpleNet]
  Epoch: [031][000/1475]   Time 0.137 (0.137)   Data 0.105 (0.105)   Loss 0.3040 (0.3040)   Prec@1 90.625 (90.625)   Prec@5 90.625 (90.625)   [2018-03-31 19:34:18]
  Epoch: [031][200/1475]   Time 0.138 (0.140)   Data 0.105 (0.106)   Loss 0.4389 (0.3824)   Prec@1 78.125 (83.613)   Prec@5 78.125 (83.613)   [2018-03-31 19:34:46]
  Epoch: [031][400/1475]   Time 0.140 (0.140)   Data 0.106 (0.106)   Loss 0.4482 (0.3801)   Prec@1 78.125 (83.752)   Prec@5 78.125 (83.752)   [2018-03-31 19:35:14]
  Epoch: [031][600/1475]   Time 0.138 (0.140)   Data 0.105 (0.106)   Loss 0.3673 (0.3787)   Prec@1 87.500 (83.730)   Prec@5 87.500 (83.730)   [2018-03-31 19:35:42]
  Epoch: [031][800/1475]   Time 0.138 (0.140)   Data 0.105 (0.106)   Loss 0.1840 (0.3781)   Prec@1 96.875 (83.751)   Prec@5 96.875 (83.751)   [2018-03-31 19:36:10]
  Epoch: [031][1000/1475]   Time 0.139 (0.140)   Data 0.106 (0.106)   Loss 0.3144 (0.3763)   Prec@1 81.250 (83.826)   Prec@5 81.250 (83.826)   [2018-03-31 19:36:38]
  Epoch: [031][1200/1475]   Time 0.140 (0.140)   Data 0.106 (0.106)   Loss 0.5538 (0.3784)   Prec@1 68.750 (83.667)   Prec@5 68.750 (83.667)   [2018-03-31 19:37:06]
  Epoch: [031][1400/1475]   Time 0.138 (0.140)   Data 0.105 (0.106)   Loss 0.3702 (0.3768)   Prec@1 78.125 (83.802)   Prec@5 78.125 (83.802)   [2018-03-31 19:37:34]
  **Train** Prec@1 83.770 Prec@5 83.770 Error@1 16.230
  **VAL** Prec@1 86.644 Prec@5 86.644 Error@1 13.356

==>>[2018-03-31 19:38:02] [Epoch=032/250] [Need: 13:32:38] [learning_rate=0.0020] [Best : Accuracy=86.67, Error=13.33]

==>>Epoch=[032/250]], [2018-03-31 19:38:02], LR=[0.002], Batch=[32] [Model=SimpleNet]
  Epoch: [032][000/1475]   Time 0.137 (0.137)   Data 0.103 (0.103)   Loss 0.4196 (0.4196)   Prec@1 84.375 (84.375)   Prec@5 84.375 (84.375)   [2018-03-31 19:38:02]
  Epoch: [032][200/1475]   Time 0.149 (0.141)   Data 0.115 (0.107)   Loss 0.2549 (0.3676)   Prec@1 90.625 (83.909)   Prec@5 90.625 (83.909)   [2018-03-31 19:38:30]
  Epoch: [032][400/1475]   Time 0.139 (0.141)   Data 0.106 (0.107)   Loss 0.4346 (0.3763)   Prec@1 81.250 (83.549)   Prec@5 81.250 (83.549)   [2018-03-31 19:38:59]
  Epoch: [032][600/1475]   Time 0.149 (0.142)   Data 0.114 (0.108)   Loss 0.6197 (0.3756)   Prec@1 75.000 (83.694)   Prec@5 75.000 (83.694)   [2018-03-31 19:39:27]
  Epoch: [032][800/1475]   Time 0.144 (0.143)   Data 0.110 (0.108)   Loss 0.4562 (0.3757)   Prec@1 81.250 (83.817)   Prec@5 81.250 (83.817)   [2018-03-31 19:39:56]
  Epoch: [032][1000/1475]   Time 0.139 (0.143)   Data 0.106 (0.108)   Loss 0.5117 (0.3768)   Prec@1 78.125 (83.776)   Prec@5 78.125 (83.776)   [2018-03-31 19:40:25]
  Epoch: [032][1200/1475]   Time 0.144 (0.142)   Data 0.108 (0.108)   Loss 0.4370 (0.3759)   Prec@1 81.250 (83.774)   Prec@5 81.250 (83.774)   [2018-03-31 19:40:53]
  Epoch: [032][1400/1475]   Time 0.138 (0.142)   Data 0.104 (0.108)   Loss 0.4422 (0.3778)   Prec@1 81.250 (83.715)   Prec@5 81.250 (83.715)   [2018-03-31 19:41:22]
  **Train** Prec@1 83.772 Prec@5 83.772 Error@1 16.228
  **VAL** Prec@1 86.080 Prec@5 86.080 Error@1 13.920

==>>[2018-03-31 19:41:51] [Epoch=033/250] [Need: 13:29:27] [learning_rate=0.0020] [Best : Accuracy=86.67, Error=13.33]

==>>Epoch=[033/250]], [2018-03-31 19:41:51], LR=[0.002], Batch=[32] [Model=SimpleNet]
  Epoch: [033][000/1475]   Time 0.137 (0.137)   Data 0.103 (0.103)   Loss 0.6507 (0.6507)   Prec@1 78.125 (78.125)   Prec@5 78.125 (78.125)   [2018-03-31 19:41:51]
  Epoch: [033][200/1475]   Time 0.141 (0.143)   Data 0.107 (0.109)   Loss 0.2953 (0.3792)   Prec@1 90.625 (83.504)   Prec@5 90.625 (83.504)   [2018-03-31 19:42:19]
  Epoch: [033][400/1475]   Time 0.146 (0.143)   Data 0.108 (0.108)   Loss 0.3684 (0.3799)   Prec@1 87.500 (83.603)   Prec@5 87.500 (83.603)   [2018-03-31 19:42:48]
  Epoch: [033][600/1475]   Time 0.138 (0.142)   Data 0.105 (0.108)   Loss 0.3730 (0.3828)   Prec@1 81.250 (83.470)   Prec@5 81.250 (83.470)   [2018-03-31 19:43:16]
  Epoch: [033][800/1475]   Time 0.136 (0.143)   Data 0.103 (0.108)   Loss 0.2542 (0.3781)   Prec@1 84.375 (83.805)   Prec@5 84.375 (83.805)   [2018-03-31 19:43:45]
  Epoch: [033][1000/1475]   Time 0.138 (0.142)   Data 0.105 (0.108)   Loss 0.2288 (0.3778)   Prec@1 96.875 (83.882)   Prec@5 96.875 (83.882)   [2018-03-31 19:44:13]
  Epoch: [033][1200/1475]   Time 0.139 (0.142)   Data 0.105 (0.108)   Loss 0.3136 (0.3776)   Prec@1 90.625 (83.875)   Prec@5 90.625 (83.875)   [2018-03-31 19:44:41]
  Epoch: [033][1400/1475]   Time 0.138 (0.142)   Data 0.105 (0.108)   Loss 0.2762 (0.3772)   Prec@1 87.500 (83.873)   Prec@5 87.500 (83.873)   [2018-03-31 19:45:09]
  **Train** Prec@1 83.811 Prec@5 83.811 Error@1 16.189
  **VAL** Prec@1 86.404 Prec@5 86.404 Error@1 13.596

==>>[2018-03-31 19:45:38] [Epoch=034/250] [Need: 13:26:06] [learning_rate=0.0020] [Best : Accuracy=86.67, Error=13.33]

==>>Epoch=[034/250]], [2018-03-31 19:45:38], LR=[0.002], Batch=[32] [Model=SimpleNet]
  Epoch: [034][000/1475]   Time 0.138 (0.138)   Data 0.104 (0.104)   Loss 0.1874 (0.1874)   Prec@1 96.875 (96.875)   Prec@5 96.875 (96.875)   [2018-03-31 19:45:38]
  Epoch: [034][200/1475]   Time 0.147 (0.141)   Data 0.113 (0.107)   Loss 0.1501 (0.3605)   Prec@1 96.875 (84.437)   Prec@5 96.875 (84.437)   [2018-03-31 19:46:07]
  Epoch: [034][400/1475]   Time 0.140 (0.143)   Data 0.107 (0.109)   Loss 0.6680 (0.3714)   Prec@1 71.875 (84.055)   Prec@5 71.875 (84.055)   [2018-03-31 19:46:36]
  Epoch: [034][600/1475]   Time 0.140 (0.142)   Data 0.106 (0.108)   Loss 0.2345 (0.3733)   Prec@1 90.625 (84.219)   Prec@5 90.625 (84.219)   [2018-03-31 19:47:04]
  Epoch: [034][800/1475]   Time 0.140 (0.142)   Data 0.105 (0.108)   Loss 0.2644 (0.3777)   Prec@1 87.500 (83.887)   Prec@5 87.500 (83.887)   [2018-03-31 19:47:32]
  Epoch: [034][1000/1475]   Time 0.141 (0.142)   Data 0.108 (0.108)   Loss 0.3974 (0.3768)   Prec@1 84.375 (83.904)   Prec@5 84.375 (83.904)   [2018-03-31 19:48:00]
  Epoch: [034][1200/1475]   Time 0.139 (0.141)   Data 0.105 (0.108)   Loss 0.3333 (0.3776)   Prec@1 81.250 (83.865)   Prec@5 81.250 (83.865)   [2018-03-31 19:48:28]
  Epoch: [034][1400/1475]   Time 0.137 (0.141)   Data 0.104 (0.107)   Loss 0.2963 (0.3769)   Prec@1 87.500 (83.889)   Prec@5 87.500 (83.889)   [2018-03-31 19:48:56]
  **Train** Prec@1 83.904 Prec@5 83.904 Error@1 16.096
  **VAL** Prec@1 85.683 Prec@5 85.683 Error@1 14.317

==>>[2018-03-31 19:49:24] [Epoch=035/250] [Need: 13:22:36] [learning_rate=0.0020] [Best : Accuracy=86.67, Error=13.33]

==>>Epoch=[035/250]], [2018-03-31 19:49:24], LR=[0.002], Batch=[32] [Model=SimpleNet]
  Epoch: [035][000/1475]   Time 0.139 (0.139)   Data 0.104 (0.104)   Loss 0.5186 (0.5186)   Prec@1 75.000 (75.000)   Prec@5 75.000 (75.000)   [2018-03-31 19:49:25]
  Epoch: [035][200/1475]   Time 0.143 (0.143)   Data 0.109 (0.109)   Loss 0.2762 (0.3823)   Prec@1 90.625 (83.427)   Prec@5 90.625 (83.427)   [2018-03-31 19:49:53]
  Epoch: [035][400/1475]   Time 0.138 (0.141)   Data 0.105 (0.108)   Loss 0.2181 (0.3787)   Prec@1 96.875 (83.635)   Prec@5 96.875 (83.635)   [2018-03-31 19:50:21]
  Epoch: [035][600/1475]   Time 0.146 (0.141)   Data 0.112 (0.107)   Loss 0.4580 (0.3768)   Prec@1 84.375 (83.756)   Prec@5 84.375 (83.756)   [2018-03-31 19:50:49]
  Epoch: [035][800/1475]   Time 0.137 (0.140)   Data 0.104 (0.107)   Loss 0.2866 (0.3740)   Prec@1 81.250 (83.880)   Prec@5 81.250 (83.880)   [2018-03-31 19:51:17]
  Epoch: [035][1000/1475]   Time 0.139 (0.140)   Data 0.105 (0.107)   Loss 0.4526 (0.3765)   Prec@1 81.250 (83.857)   Prec@5 81.250 (83.857)   [2018-03-31 19:51:45]
  Epoch: [035][1200/1475]   Time 0.137 (0.140)   Data 0.104 (0.106)   Loss 0.4839 (0.3764)   Prec@1 71.875 (83.810)   Prec@5 71.875 (83.810)   [2018-03-31 19:52:13]
  Epoch: [035][1400/1475]   Time 0.138 (0.140)   Data 0.105 (0.106)   Loss 0.3810 (0.3755)   Prec@1 84.375 (83.779)   Prec@5 84.375 (83.779)   [2018-03-31 19:52:40]
  **Train** Prec@1 83.868 Prec@5 83.868 Error@1 16.132
  **VAL** Prec@1 85.431 Prec@5 85.431 Error@1 14.569

==>>[2018-03-31 19:53:09] [Epoch=036/250] [Need: 13:18:55] [learning_rate=0.0020] [Best : Accuracy=86.67, Error=13.33]

==>>Epoch=[036/250]], [2018-03-31 19:53:09], LR=[0.002], Batch=[32] [Model=SimpleNet]
  Epoch: [036][000/1475]   Time 0.138 (0.138)   Data 0.104 (0.104)   Loss 0.5882 (0.5882)   Prec@1 71.875 (71.875)   Prec@5 71.875 (71.875)   [2018-03-31 19:53:09]
  Epoch: [036][200/1475]   Time 0.140 (0.139)   Data 0.106 (0.106)   Loss 0.4700 (0.3785)   Prec@1 78.125 (84.049)   Prec@5 78.125 (84.049)   [2018-03-31 19:53:37]
  Epoch: [036][400/1475]   Time 0.138 (0.139)   Data 0.105 (0.106)   Loss 0.4331 (0.3821)   Prec@1 75.000 (83.759)   Prec@5 75.000 (83.759)   [2018-03-31 19:54:05]
  Epoch: [036][600/1475]   Time 0.140 (0.139)   Data 0.107 (0.106)   Loss 0.2653 (0.3789)   Prec@1 87.500 (83.829)   Prec@5 87.500 (83.829)   [2018-03-31 19:54:32]
  Epoch: [036][800/1475]   Time 0.137 (0.139)   Data 0.104 (0.106)   Loss 0.3079 (0.3762)   Prec@1 87.500 (84.004)   Prec@5 87.500 (84.004)   [2018-03-31 19:55:00]
  Epoch: [036][1000/1475]   Time 0.142 (0.139)   Data 0.106 (0.106)   Loss 0.3545 (0.3751)   Prec@1 84.375 (84.010)   Prec@5 84.375 (84.010)   [2018-03-31 19:55:28]
  Epoch: [036][1200/1475]   Time 0.140 (0.139)   Data 0.107 (0.106)   Loss 0.3617 (0.3753)   Prec@1 84.375 (83.959)   Prec@5 84.375 (83.959)   [2018-03-31 19:55:56]
  Epoch: [036][1400/1475]   Time 0.138 (0.139)   Data 0.105 (0.106)   Loss 0.4887 (0.3759)   Prec@1 75.000 (83.878)   Prec@5 75.000 (83.878)   [2018-03-31 19:56:24]
  **Train** Prec@1 83.796 Prec@5 83.796 Error@1 16.204
  **VAL** Prec@1 85.287 Prec@5 85.287 Error@1 14.713

==>>[2018-03-31 19:56:52] [Epoch=037/250] [Need: 13:15:09] [learning_rate=0.0020] [Best : Accuracy=86.67, Error=13.33]

==>>Epoch=[037/250]], [2018-03-31 19:56:52], LR=[0.002], Batch=[32] [Model=SimpleNet]
  Epoch: [037][000/1475]   Time 0.137 (0.137)   Data 0.103 (0.103)   Loss 0.4126 (0.4126)   Prec@1 81.250 (81.250)   Prec@5 81.250 (81.250)   [2018-03-31 19:56:53]
  Epoch: [037][200/1475]   Time 0.140 (0.141)   Data 0.107 (0.107)   Loss 0.3986 (0.3743)   Prec@1 81.250 (83.675)   Prec@5 81.250 (83.675)   [2018-03-31 19:57:21]
  Epoch: [037][400/1475]   Time 0.145 (0.141)   Data 0.112 (0.107)   Loss 0.3112 (0.3757)   Prec@1 87.500 (83.791)   Prec@5 87.500 (83.791)   [2018-03-31 19:57:49]
  Epoch: [037][600/1475]   Time 0.146 (0.142)   Data 0.113 (0.108)   Loss 0.3640 (0.3743)   Prec@1 75.000 (83.819)   Prec@5 75.000 (83.819)   [2018-03-31 19:58:18]
  Epoch: [037][800/1475]   Time 0.138 (0.142)   Data 0.105 (0.108)   Loss 0.2812 (0.3756)   Prec@1 84.375 (83.657)   Prec@5 84.375 (83.657)   [2018-03-31 19:58:46]
  Epoch: [037][1000/1475]   Time 0.142 (0.142)   Data 0.109 (0.108)   Loss 0.4264 (0.3774)   Prec@1 78.125 (83.551)   Prec@5 78.125 (83.551)   [2018-03-31 19:59:14]
  Epoch: [037][1200/1475]   Time 0.150 (0.142)   Data 0.117 (0.108)   Loss 0.2953 (0.3763)   Prec@1 87.500 (83.670)   Prec@5 87.500 (83.670)   [2018-03-31 19:59:43]
  Epoch: [037][1400/1475]   Time 0.141 (0.142)   Data 0.108 (0.108)   Loss 0.2990 (0.3765)   Prec@1 84.375 (83.655)   Prec@5 84.375 (83.655)   [2018-03-31 20:00:11]
  **Train** Prec@1 83.673 Prec@5 83.673 Error@1 16.327
  **VAL** Prec@1 85.695 Prec@5 85.695 Error@1 14.305

==>>[2018-03-31 20:00:40] [Epoch=038/250] [Need: 13:11:43] [learning_rate=0.0020] [Best : Accuracy=86.67, Error=13.33]

==>>Epoch=[038/250]], [2018-03-31 20:00:40], LR=[0.002], Batch=[32] [Model=SimpleNet]
  Epoch: [038][000/1475]   Time 0.139 (0.139)   Data 0.104 (0.104)   Loss 0.3914 (0.3914)   Prec@1 87.500 (87.500)   Prec@5 87.500 (87.500)   [2018-03-31 20:00:40]
  Epoch: [038][200/1475]   Time 0.142 (0.141)   Data 0.109 (0.108)   Loss 0.4917 (0.3852)   Prec@1 81.250 (83.256)   Prec@5 81.250 (83.256)   [2018-03-31 20:01:08]
  Epoch: [038][400/1475]   Time 0.140 (0.142)   Data 0.105 (0.108)   Loss 0.3027 (0.3808)   Prec@1 87.500 (83.214)   Prec@5 87.500 (83.214)   [2018-03-31 20:01:36]
  Epoch: [038][600/1475]   Time 0.140 (0.142)   Data 0.107 (0.108)   Loss 0.2300 (0.3812)   Prec@1 87.500 (83.210)   Prec@5 87.500 (83.210)   [2018-03-31 20:02:05]
  Epoch: [038][800/1475]   Time 0.139 (0.142)   Data 0.106 (0.108)   Loss 0.3415 (0.3816)   Prec@1 81.250 (83.197)   Prec@5 81.250 (83.197)   [2018-03-31 20:02:33]
  Epoch: [038][1000/1475]   Time 0.140 (0.142)   Data 0.107 (0.108)   Loss 0.2459 (0.3778)   Prec@1 93.750 (83.467)   Prec@5 93.750 (83.467)   [2018-03-31 20:03:01]
  Epoch: [038][1200/1475]   Time 0.142 (0.142)   Data 0.108 (0.108)   Loss 0.2063 (0.3764)   Prec@1 93.750 (83.550)   Prec@5 93.750 (83.550)   [2018-03-31 20:03:30]
  Epoch: [038][1400/1475]   Time 0.138 (0.142)   Data 0.105 (0.108)   Loss 0.1803 (0.3759)   Prec@1 96.875 (83.652)   Prec@5 96.875 (83.652)   [2018-03-31 20:03:58]
  **Train** Prec@1 83.694 Prec@5 83.694 Error@1 16.306
  **VAL** Prec@1 86.704 Prec@5 86.704 Error@1 13.296

==>>[2018-03-31 20:04:27] [Epoch=039/250] [Need: 13:08:16] [learning_rate=0.0020] [Best : Accuracy=86.70, Error=13.30]

==>>Epoch=[039/250]], [2018-03-31 20:04:27], LR=[0.002], Batch=[32] [Model=SimpleNet]
  Epoch: [039][000/1475]   Time 0.142 (0.142)   Data 0.108 (0.108)   Loss 0.6868 (0.6868)   Prec@1 71.875 (71.875)   Prec@5 71.875 (71.875)   [2018-03-31 20:04:27]
  Epoch: [039][200/1475]   Time 0.138 (0.142)   Data 0.105 (0.108)   Loss 0.2127 (0.3770)   Prec@1 90.625 (83.660)   Prec@5 90.625 (83.660)   [2018-03-31 20:04:56]
  Epoch: [039][400/1475]   Time 0.144 (0.142)   Data 0.111 (0.108)   Loss 0.3257 (0.3760)   Prec@1 87.500 (83.494)   Prec@5 87.500 (83.494)   [2018-03-31 20:05:24]
  Epoch: [039][600/1475]   Time 0.141 (0.142)   Data 0.106 (0.108)   Loss 0.3885 (0.3717)   Prec@1 81.250 (83.876)   Prec@5 81.250 (83.876)   [2018-03-31 20:05:52]
  Epoch: [039][800/1475]   Time 0.139 (0.142)   Data 0.105 (0.108)   Loss 0.6181 (0.3715)   Prec@1 68.750 (84.082)   Prec@5 68.750 (84.082)   [2018-03-31 20:06:21]
  Epoch: [039][1000/1475]   Time 0.141 (0.142)   Data 0.106 (0.108)   Loss 0.3571 (0.3732)   Prec@1 81.250 (83.966)   Prec@5 81.250 (83.966)   [2018-03-31 20:06:49]
  Epoch: [039][1200/1475]   Time 0.140 (0.142)   Data 0.106 (0.108)   Loss 0.3642 (0.3719)   Prec@1 87.500 (84.065)   Prec@5 87.500 (84.065)   [2018-03-31 20:07:17]
  Epoch: [039][1400/1475]   Time 0.147 (0.142)   Data 0.113 (0.108)   Loss 0.3426 (0.3722)   Prec@1 81.250 (84.058)   Prec@5 81.250 (84.058)   [2018-03-31 20:07:46]
  **Train** Prec@1 84.044 Prec@5 84.044 Error@1 15.956
  **VAL** Prec@1 86.716 Prec@5 86.716 Error@1 13.284

==>>[2018-03-31 20:08:14] [Epoch=040/250] [Need: 13:04:48] [learning_rate=0.0020] [Best : Accuracy=86.72, Error=13.28]

==>>Epoch=[040/250]], [2018-03-31 20:08:14], LR=[0.002], Batch=[32] [Model=SimpleNet]
  Epoch: [040][000/1475]   Time 0.143 (0.143)   Data 0.109 (0.109)   Loss 0.4132 (0.4132)   Prec@1 81.250 (81.250)   Prec@5 81.250 (81.250)   [2018-03-31 20:08:14]
  Epoch: [040][200/1475]   Time 0.148 (0.142)   Data 0.114 (0.108)   Loss 0.4597 (0.3848)   Prec@1 75.000 (83.147)   Prec@5 75.000 (83.147)   [2018-03-31 20:08:43]
  Epoch: [040][400/1475]   Time 0.148 (0.142)   Data 0.114 (0.108)   Loss 0.3203 (0.3771)   Prec@1 90.625 (83.666)   Prec@5 90.625 (83.666)   [2018-03-31 20:09:11]
  Epoch: [040][600/1475]   Time 0.137 (0.142)   Data 0.104 (0.108)   Loss 0.4712 (0.3771)   Prec@1 75.000 (83.735)   Prec@5 75.000 (83.735)   [2018-03-31 20:09:40]
  Epoch: [040][800/1475]   Time 0.139 (0.142)   Data 0.106 (0.108)   Loss 0.3877 (0.3778)   Prec@1 78.125 (83.669)   Prec@5 78.125 (83.669)   [2018-03-31 20:10:08]
  Epoch: [040][1000/1475]   Time 0.138 (0.142)   Data 0.105 (0.108)   Loss 0.2407 (0.3775)   Prec@1 93.750 (83.716)   Prec@5 93.750 (83.716)   [2018-03-31 20:10:36]
  Epoch: [040][1200/1475]   Time 0.141 (0.142)   Data 0.107 (0.108)   Loss 0.4557 (0.3760)   Prec@1 84.375 (83.766)   Prec@5 84.375 (83.766)   [2018-03-31 20:11:05]
  Epoch: [040][1400/1475]   Time 0.147 (0.142)   Data 0.111 (0.108)   Loss 0.4162 (0.3753)   Prec@1 81.250 (83.766)   Prec@5 81.250 (83.766)   [2018-03-31 20:11:33]
  **Train** Prec@1 83.739 Prec@5 83.739 Error@1 16.261
  **VAL** Prec@1 86.608 Prec@5 86.608 Error@1 13.392

==>>[2018-03-31 20:12:02] [Epoch=041/250] [Need: 13:01:22] [learning_rate=0.0020] [Best : Accuracy=86.72, Error=13.28]

==>>Epoch=[041/250]], [2018-03-31 20:12:02], LR=[0.002], Batch=[32] [Model=SimpleNet]
  Epoch: [041][000/1475]   Time 0.138 (0.138)   Data 0.104 (0.104)   Loss 0.3101 (0.3101)   Prec@1 87.500 (87.500)   Prec@5 87.500 (87.500)   [2018-03-31 20:12:02]
  Epoch: [041][200/1475]   Time 0.139 (0.140)   Data 0.105 (0.107)   Loss 0.3037 (0.3743)   Prec@1 81.250 (84.453)   Prec@5 81.250 (84.453)   [2018-03-31 20:12:30]
  Epoch: [041][400/1475]   Time 0.140 (0.141)   Data 0.106 (0.107)   Loss 0.3422 (0.3752)   Prec@1 87.500 (84.180)   Prec@5 87.500 (84.180)   [2018-03-31 20:12:58]
  Epoch: [041][600/1475]   Time 0.137 (0.141)   Data 0.104 (0.107)   Loss 0.4131 (0.3737)   Prec@1 81.250 (84.120)   Prec@5 81.250 (84.120)   [2018-03-31 20:13:26]
  Epoch: [041][800/1475]   Time 0.138 (0.140)   Data 0.105 (0.107)   Loss 0.4426 (0.3705)   Prec@1 75.000 (84.270)   Prec@5 75.000 (84.270)   [2018-03-31 20:13:54]
  Epoch: [041][1000/1475]   Time 0.140 (0.140)   Data 0.105 (0.107)   Loss 0.2727 (0.3702)   Prec@1 93.750 (84.303)   Prec@5 93.750 (84.303)   [2018-03-31 20:14:22]
  Epoch: [041][1200/1475]   Time 0.138 (0.140)   Data 0.104 (0.107)   Loss 0.4671 (0.3718)   Prec@1 71.875 (84.203)   Prec@5 71.875 (84.203)   [2018-03-31 20:14:51]
  Epoch: [041][1400/1475]   Time 0.142 (0.140)   Data 0.108 (0.107)   Loss 0.3709 (0.3717)   Prec@1 84.375 (84.165)   Prec@5 84.375 (84.165)   [2018-03-31 20:15:19]
  **Train** Prec@1 84.156 Prec@5 84.156 Error@1 15.844
  **VAL** Prec@1 86.836 Prec@5 86.836 Error@1 13.164

==>>[2018-03-31 20:15:47] [Epoch=042/250] [Need: 12:57:42] [learning_rate=0.0020] [Best : Accuracy=86.84, Error=13.16]

==>>Epoch=[042/250]], [2018-03-31 20:15:47], LR=[0.002], Batch=[32] [Model=SimpleNet]
  Epoch: [042][000/1475]   Time 0.140 (0.140)   Data 0.106 (0.106)   Loss 0.4798 (0.4798)   Prec@1 75.000 (75.000)   Prec@5 75.000 (75.000)   [2018-03-31 20:15:47]
  Epoch: [042][200/1475]   Time 0.141 (0.140)   Data 0.108 (0.107)   Loss 0.2981 (0.3936)   Prec@1 87.500 (82.991)   Prec@5 87.500 (82.991)   [2018-03-31 20:16:15]
  Epoch: [042][400/1475]   Time 0.138 (0.141)   Data 0.105 (0.107)   Loss 0.2485 (0.3838)   Prec@1 93.750 (83.510)   Prec@5 93.750 (83.510)   [2018-03-31 20:16:44]
  Epoch: [042][600/1475]   Time 0.141 (0.141)   Data 0.108 (0.107)   Loss 0.3315 (0.3801)   Prec@1 84.375 (83.657)   Prec@5 84.375 (83.657)   [2018-03-31 20:17:12]
  Epoch: [042][800/1475]   Time 0.139 (0.141)   Data 0.106 (0.107)   Loss 0.3056 (0.3800)   Prec@1 87.500 (83.634)   Prec@5 87.500 (83.634)   [2018-03-31 20:17:40]
  Epoch: [042][1000/1475]   Time 0.141 (0.140)   Data 0.107 (0.107)   Loss 0.4257 (0.3774)   Prec@1 75.000 (83.838)   Prec@5 75.000 (83.838)   [2018-03-31 20:18:08]
  Epoch: [042][1200/1475]   Time 0.139 (0.140)   Data 0.106 (0.107)   Loss 0.4302 (0.3776)   Prec@1 78.125 (83.842)   Prec@5 78.125 (83.842)   [2018-03-31 20:18:36]
  Epoch: [042][1400/1475]   Time 0.141 (0.140)   Data 0.106 (0.107)   Loss 0.3640 (0.3779)   Prec@1 84.375 (83.773)   Prec@5 84.375 (83.773)   [2018-03-31 20:19:04]
  **Train** Prec@1 83.785 Prec@5 83.785 Error@1 16.215
  **VAL** Prec@1 87.053 Prec@5 87.053 Error@1 12.947

==>>[2018-03-31 20:19:32] [Epoch=043/250] [Need: 12:54:02] [learning_rate=0.0020] [Best : Accuracy=87.05, Error=12.95]

==>>Epoch=[043/250]], [2018-03-31 20:19:32], LR=[0.002], Batch=[32] [Model=SimpleNet]
  Epoch: [043][000/1475]   Time 0.137 (0.137)   Data 0.103 (0.103)   Loss 0.4251 (0.4251)   Prec@1 84.375 (84.375)   Prec@5 84.375 (84.375)   [2018-03-31 20:19:33]
  Epoch: [043][200/1475]   Time 0.139 (0.140)   Data 0.106 (0.107)   Loss 0.5135 (0.3626)   Prec@1 75.000 (84.733)   Prec@5 75.000 (84.733)   [2018-03-31 20:20:01]
  Epoch: [043][400/1475]   Time 0.139 (0.140)   Data 0.106 (0.106)   Loss 0.3816 (0.3610)   Prec@1 90.625 (84.484)   Prec@5 90.625 (84.484)   [2018-03-31 20:20:29]
  Epoch: [043][600/1475]   Time 0.138 (0.140)   Data 0.105 (0.106)   Loss 0.5289 (0.3704)   Prec@1 84.375 (83.954)   Prec@5 84.375 (83.954)   [2018-03-31 20:20:56]
  Epoch: [043][800/1475]   Time 0.149 (0.140)   Data 0.116 (0.106)   Loss 0.2163 (0.3724)   Prec@1 100.000 (83.766)   Prec@5 100.000 (83.766)   [2018-03-31 20:21:24]
  Epoch: [043][1000/1475]   Time 0.143 (0.140)   Data 0.109 (0.106)   Loss 0.3727 (0.3740)   Prec@1 84.375 (83.760)   Prec@5 84.375 (83.760)   [2018-03-31 20:21:52]
  Epoch: [043][1200/1475]   Time 0.141 (0.140)   Data 0.108 (0.106)   Loss 0.2614 (0.3733)   Prec@1 90.625 (83.813)   Prec@5 90.625 (83.813)   [2018-03-31 20:22:21]
  Epoch: [043][1400/1475]   Time 0.139 (0.140)   Data 0.106 (0.107)   Loss 0.2649 (0.3718)   Prec@1 90.625 (83.882)   Prec@5 90.625 (83.882)   [2018-03-31 20:22:49]
  **Train** Prec@1 83.914 Prec@5 83.914 Error@1 16.086
  **VAL** Prec@1 86.836 Prec@5 86.836 Error@1 13.164

==>>[2018-03-31 20:23:17] [Epoch=044/250] [Need: 12:50:18] [learning_rate=0.0020] [Best : Accuracy=87.05, Error=12.95]

==>>Epoch=[044/250]], [2018-03-31 20:23:17], LR=[0.002], Batch=[32] [Model=SimpleNet]
  Epoch: [044][000/1475]   Time 0.140 (0.140)   Data 0.106 (0.106)   Loss 0.2076 (0.2076)   Prec@1 93.750 (93.750)   Prec@5 93.750 (93.750)   [2018-03-31 20:23:17]
  Epoch: [044][200/1475]   Time 0.144 (0.140)   Data 0.107 (0.106)   Loss 0.4591 (0.3820)   Prec@1 78.125 (83.535)   Prec@5 78.125 (83.535)   [2018-03-31 20:23:45]
  Epoch: [044][400/1475]   Time 0.140 (0.140)   Data 0.107 (0.106)   Loss 0.4755 (0.3763)   Prec@1 84.375 (83.845)   Prec@5 84.375 (83.845)   [2018-03-31 20:24:13]
  Epoch: [044][600/1475]   Time 0.138 (0.140)   Data 0.105 (0.106)   Loss 0.3592 (0.3755)   Prec@1 87.500 (83.943)   Prec@5 87.500 (83.943)   [2018-03-31 20:24:41]
  Epoch: [044][800/1475]   Time 0.138 (0.140)   Data 0.104 (0.106)   Loss 0.3672 (0.3717)   Prec@1 81.250 (84.125)   Prec@5 81.250 (84.125)   [2018-03-31 20:25:09]
  Epoch: [044][1000/1475]   Time 0.139 (0.140)   Data 0.105 (0.107)   Loss 0.5723 (0.3721)   Prec@1 68.750 (84.063)   Prec@5 68.750 (84.063)   [2018-03-31 20:25:37]
  Epoch: [044][1200/1475]   Time 0.138 (0.140)   Data 0.105 (0.107)   Loss 0.3292 (0.3718)   Prec@1 84.375 (84.089)   Prec@5 84.375 (84.089)   [2018-03-31 20:26:05]
  Epoch: [044][1400/1475]   Time 0.140 (0.140)   Data 0.107 (0.106)   Loss 0.3283 (0.3732)   Prec@1 78.125 (84.072)   Prec@5 78.125 (84.072)   [2018-03-31 20:26:33]
  **Train** Prec@1 84.006 Prec@5 84.006 Error@1 15.994
  **VAL** Prec@1 86.116 Prec@5 86.116 Error@1 13.884

==>>[2018-03-31 20:27:01] [Epoch=045/250] [Need: 12:46:35] [learning_rate=0.0020] [Best : Accuracy=87.05, Error=12.95]

==>>Epoch=[045/250]], [2018-03-31 20:27:01], LR=[0.002], Batch=[32] [Model=SimpleNet]
  Epoch: [045][000/1475]   Time 0.138 (0.138)   Data 0.104 (0.104)   Loss 0.3300 (0.3300)   Prec@1 87.500 (87.500)   Prec@5 87.500 (87.500)   [2018-03-31 20:27:01]
  Epoch: [045][200/1475]   Time 0.138 (0.140)   Data 0.105 (0.106)   Loss 0.1943 (0.3648)   Prec@1 93.750 (84.375)   Prec@5 93.750 (84.375)   [2018-03-31 20:27:29]
  Epoch: [045][400/1475]   Time 0.141 (0.140)   Data 0.108 (0.106)   Loss 0.3283 (0.3706)   Prec@1 84.375 (84.118)   Prec@5 84.375 (84.118)   [2018-03-31 20:27:57]
  Epoch: [045][600/1475]   Time 0.140 (0.140)   Data 0.107 (0.106)   Loss 0.2383 (0.3706)   Prec@1 87.500 (84.214)   Prec@5 87.500 (84.214)   [2018-03-31 20:28:25]
  Epoch: [045][800/1475]   Time 0.138 (0.140)   Data 0.105 (0.106)   Loss 0.4254 (0.3709)   Prec@1 78.125 (84.160)   Prec@5 78.125 (84.160)   [2018-03-31 20:28:53]
  Epoch: [045][1000/1475]   Time 0.146 (0.140)   Data 0.111 (0.107)   Loss 0.4744 (0.3714)   Prec@1 84.375 (84.091)   Prec@5 84.375 (84.091)   [2018-03-31 20:29:22]
  Epoch: [045][1200/1475]   Time 0.142 (0.141)   Data 0.107 (0.107)   Loss 0.4241 (0.3717)   Prec@1 81.250 (84.133)   Prec@5 81.250 (84.133)   [2018-03-31 20:29:50]
  Epoch: [045][1400/1475]   Time 0.144 (0.141)   Data 0.109 (0.107)   Loss 0.3782 (0.3730)   Prec@1 87.500 (84.025)   Prec@5 87.500 (84.025)   [2018-03-31 20:30:19]
  **Train** Prec@1 83.993 Prec@5 83.993 Error@1 16.007
  **VAL** Prec@1 87.089 Prec@5 87.089 Error@1 12.911

==>>[2018-03-31 20:30:47] [Epoch=046/250] [Need: 12:42:56] [learning_rate=0.0020] [Best : Accuracy=87.09, Error=12.91]

==>>Epoch=[046/250]], [2018-03-31 20:30:47], LR=[0.002], Batch=[32] [Model=SimpleNet]
  Epoch: [046][000/1475]   Time 0.140 (0.140)   Data 0.106 (0.106)   Loss 0.3560 (0.3560)   Prec@1 81.250 (81.250)   Prec@5 81.250 (81.250)   [2018-03-31 20:30:47]
  Epoch: [046][200/1475]   Time 0.147 (0.141)   Data 0.109 (0.107)   Loss 0.2568 (0.3675)   Prec@1 87.500 (84.593)   Prec@5 87.500 (84.593)   [2018-03-31 20:31:15]
  Epoch: [046][400/1475]   Time 0.139 (0.141)   Data 0.105 (0.107)   Loss 0.3090 (0.3723)   Prec@1 87.500 (84.094)   Prec@5 87.500 (84.094)   [2018-03-31 20:31:44]
  Epoch: [046][600/1475]   Time 0.143 (0.141)   Data 0.109 (0.107)   Loss 0.2489 (0.3677)   Prec@1 90.625 (84.292)   Prec@5 90.625 (84.292)   [2018-03-31 20:32:12]
  Epoch: [046][800/1475]   Time 0.138 (0.140)   Data 0.105 (0.107)   Loss 0.3274 (0.3682)   Prec@1 90.625 (84.211)   Prec@5 90.625 (84.211)   [2018-03-31 20:32:40]
  Epoch: [046][1000/1475]   Time 0.137 (0.140)   Data 0.104 (0.107)   Loss 0.2744 (0.3694)   Prec@1 96.875 (84.241)   Prec@5 96.875 (84.241)   [2018-03-31 20:33:08]
  Epoch: [046][1200/1475]   Time 0.142 (0.140)   Data 0.108 (0.107)   Loss 0.4856 (0.3688)   Prec@1 78.125 (84.263)   Prec@5 78.125 (84.263)   [2018-03-31 20:33:36]
  Epoch: [046][1400/1475]   Time 0.138 (0.140)   Data 0.105 (0.107)   Loss 0.2424 (0.3704)   Prec@1 87.500 (84.069)   Prec@5 87.500 (84.069)   [2018-03-31 20:34:04]
  **Train** Prec@1 84.059 Prec@5 84.059 Error@1 15.941
  **VAL** Prec@1 86.380 Prec@5 86.380 Error@1 13.620

==>>[2018-03-31 20:34:32] [Epoch=047/250] [Need: 12:39:15] [learning_rate=0.0020] [Best : Accuracy=87.09, Error=12.91]

==>>Epoch=[047/250]], [2018-03-31 20:34:32], LR=[0.002], Batch=[32] [Model=SimpleNet]
  Epoch: [047][000/1475]   Time 0.138 (0.138)   Data 0.104 (0.104)   Loss 0.4661 (0.4661)   Prec@1 78.125 (78.125)   Prec@5 78.125 (78.125)   [2018-03-31 20:34:32]
  Epoch: [047][200/1475]   Time 0.141 (0.140)   Data 0.108 (0.106)   Loss 0.3152 (0.3685)   Prec@1 93.750 (84.080)   Prec@5 93.750 (84.080)   [2018-03-31 20:35:00]
  Epoch: [047][400/1475]   Time 0.140 (0.140)   Data 0.107 (0.106)   Loss 0.4118 (0.3696)   Prec@1 84.375 (84.118)   Prec@5 84.375 (84.118)   [2018-03-31 20:35:28]
  Epoch: [047][600/1475]   Time 0.137 (0.140)   Data 0.104 (0.106)   Loss 0.3405 (0.3697)   Prec@1 84.375 (84.172)   Prec@5 84.375 (84.172)   [2018-03-31 20:35:56]
  Epoch: [047][800/1475]   Time 0.138 (0.140)   Data 0.105 (0.106)   Loss 0.3715 (0.3737)   Prec@1 84.375 (83.997)   Prec@5 84.375 (83.997)   [2018-03-31 20:36:24]
  Epoch: [047][1000/1475]   Time 0.138 (0.140)   Data 0.105 (0.106)   Loss 0.3444 (0.3732)   Prec@1 87.500 (84.010)   Prec@5 87.500 (84.010)   [2018-03-31 20:36:52]
  Epoch: [047][1200/1475]   Time 0.137 (0.140)   Data 0.104 (0.106)   Loss 0.4333 (0.3724)   Prec@1 75.000 (84.019)   Prec@5 75.000 (84.019)   [2018-03-31 20:37:20]
  Epoch: [047][1400/1475]   Time 0.137 (0.140)   Data 0.104 (0.106)   Loss 0.2571 (0.3724)   Prec@1 90.625 (84.081)   Prec@5 90.625 (84.081)   [2018-03-31 20:37:48]
  **Train** Prec@1 84.078 Prec@5 84.078 Error@1 15.922
  **VAL** Prec@1 86.344 Prec@5 86.344 Error@1 13.656

==>>[2018-03-31 20:38:16] [Epoch=048/250] [Need: 12:35:29] [learning_rate=0.0020] [Best : Accuracy=87.09, Error=12.91]

==>>Epoch=[048/250]], [2018-03-31 20:38:16], LR=[0.002], Batch=[32] [Model=SimpleNet]
  Epoch: [048][000/1475]   Time 0.138 (0.138)   Data 0.104 (0.104)   Loss 0.2728 (0.2728)   Prec@1 87.500 (87.500)   Prec@5 87.500 (87.500)   [2018-03-31 20:38:16]
  Epoch: [048][200/1475]   Time 0.142 (0.140)   Data 0.109 (0.107)   Loss 0.3777 (0.3781)   Prec@1 84.375 (83.753)   Prec@5 84.375 (83.753)   [2018-03-31 20:38:44]
  Epoch: [048][400/1475]   Time 0.142 (0.140)   Data 0.108 (0.107)   Loss 0.3525 (0.3698)   Prec@1 90.625 (84.297)   Prec@5 90.625 (84.297)   [2018-03-31 20:39:12]
  Epoch: [048][600/1475]   Time 0.141 (0.140)   Data 0.108 (0.107)   Loss 0.5846 (0.3714)   Prec@1 75.000 (84.240)   Prec@5 75.000 (84.240)   [2018-03-31 20:39:41]
  Epoch: [048][800/1475]   Time 0.139 (0.140)   Data 0.106 (0.107)   Loss 0.2970 (0.3681)   Prec@1 87.500 (84.305)   Prec@5 87.500 (84.305)   [2018-03-31 20:40:08]
  Epoch: [048][1000/1475]   Time 0.138 (0.140)   Data 0.105 (0.107)   Loss 0.6018 (0.3694)   Prec@1 75.000 (84.247)   Prec@5 75.000 (84.247)   [2018-03-31 20:40:36]
  Epoch: [048][1200/1475]   Time 0.140 (0.140)   Data 0.107 (0.107)   Loss 0.5808 (0.3691)   Prec@1 71.875 (84.242)   Prec@5 71.875 (84.242)   [2018-03-31 20:41:04]
  Epoch: [048][1400/1475]   Time 0.138 (0.140)   Data 0.105 (0.107)   Loss 0.3346 (0.3712)   Prec@1 84.375 (84.203)   Prec@5 84.375 (84.203)   [2018-03-31 20:41:32]
  **Train** Prec@1 84.156 Prec@5 84.156 Error@1 15.844
  **VAL** Prec@1 86.680 Prec@5 86.680 Error@1 13.320

==>>[2018-03-31 20:42:00] [Epoch=049/250] [Need: 12:31:44] [learning_rate=0.0020] [Best : Accuracy=87.09, Error=12.91]

==>>Epoch=[049/250]], [2018-03-31 20:42:00], LR=[0.002], Batch=[32] [Model=SimpleNet]
  Epoch: [049][000/1475]   Time 0.137 (0.137)   Data 0.103 (0.103)   Loss 0.2723 (0.2723)   Prec@1 90.625 (90.625)   Prec@5 90.625 (90.625)   [2018-03-31 20:42:01]
  Epoch: [049][200/1475]   Time 0.139 (0.140)   Data 0.106 (0.107)   Loss 0.4934 (0.3704)   Prec@1 75.000 (84.002)   Prec@5 75.000 (84.002)   [2018-03-31 20:42:29]
  Epoch: [049][400/1475]   Time 0.139 (0.140)   Data 0.105 (0.107)   Loss 0.5011 (0.3724)   Prec@1 78.125 (83.907)   Prec@5 78.125 (83.907)   [2018-03-31 20:42:57]
  Epoch: [049][600/1475]   Time 0.138 (0.140)   Data 0.104 (0.106)   Loss 0.2157 (0.3718)   Prec@1 93.750 (83.886)   Prec@5 93.750 (83.886)   [2018-03-31 20:43:24]
  Epoch: [049][800/1475]   Time 0.140 (0.140)   Data 0.107 (0.107)   Loss 0.3929 (0.3730)   Prec@1 81.250 (83.942)   Prec@5 81.250 (83.942)   [2018-03-31 20:43:52]
  Epoch: [049][1000/1475]   Time 0.138 (0.140)   Data 0.105 (0.106)   Loss 0.3189 (0.3738)   Prec@1 87.500 (83.929)   Prec@5 87.500 (83.929)   [2018-03-31 20:44:20]
  Epoch: [049][1200/1475]   Time 0.143 (0.140)   Data 0.110 (0.106)   Loss 0.2930 (0.3730)   Prec@1 90.625 (83.956)   Prec@5 90.625 (83.956)   [2018-03-31 20:44:48]
  Epoch: [049][1400/1475]   Time 0.143 (0.140)   Data 0.109 (0.106)   Loss 0.3265 (0.3727)   Prec@1 87.500 (83.982)   Prec@5 87.500 (83.982)   [2018-03-31 20:45:16]
  **Train** Prec@1 84.018 Prec@5 84.018 Error@1 15.982
  **VAL** Prec@1 86.344 Prec@5 86.344 Error@1 13.656

==>>[2018-03-31 20:45:45] [Epoch=050/250] [Need: 12:27:58] [learning_rate=0.0020] [Best : Accuracy=87.09, Error=12.91]

==>>Epoch=[050/250]], [2018-03-31 20:45:45], LR=[0.002], Batch=[32] [Model=SimpleNet]
  Epoch: [050][000/1475]   Time 0.138 (0.138)   Data 0.103 (0.103)   Loss 0.4714 (0.4714)   Prec@1 71.875 (71.875)   Prec@5 71.875 (71.875)   [2018-03-31 20:45:45]
  Epoch: [050][200/1475]   Time 0.142 (0.141)   Data 0.109 (0.107)   Loss 0.5354 (0.3801)   Prec@1 75.000 (83.225)   Prec@5 75.000 (83.225)   [2018-03-31 20:46:13]
  Epoch: [050][400/1475]   Time 0.140 (0.140)   Data 0.107 (0.107)   Loss 0.4785 (0.3736)   Prec@1 84.375 (83.892)   Prec@5 84.375 (83.892)   [2018-03-31 20:46:41]
  Epoch: [050][600/1475]   Time 0.141 (0.140)   Data 0.108 (0.107)   Loss 0.3392 (0.3740)   Prec@1 84.375 (83.980)   Prec@5 84.375 (83.980)   [2018-03-31 20:47:09]
  Epoch: [050][800/1475]   Time 0.141 (0.140)   Data 0.107 (0.107)   Loss 0.3792 (0.3737)   Prec@1 84.375 (83.977)   Prec@5 84.375 (83.977)   [2018-03-31 20:47:37]
  Epoch: [050][1000/1475]   Time 0.139 (0.140)   Data 0.105 (0.107)   Loss 0.3462 (0.3760)   Prec@1 84.375 (83.854)   Prec@5 84.375 (83.854)   [2018-03-31 20:48:05]
  Epoch: [050][1200/1475]   Time 0.150 (0.140)   Data 0.117 (0.107)   Loss 0.5552 (0.3752)   Prec@1 75.000 (83.894)   Prec@5 75.000 (83.894)   [2018-03-31 20:48:33]
  Epoch: [050][1400/1475]   Time 0.144 (0.140)   Data 0.111 (0.107)   Loss 0.4698 (0.3734)   Prec@1 78.125 (84.005)   Prec@5 78.125 (84.005)   [2018-03-31 20:49:01]
  **Train** Prec@1 84.025 Prec@5 84.025 Error@1 15.975
  **VAL** Prec@1 86.548 Prec@5 86.548 Error@1 13.452

==>>[2018-03-31 20:49:29] [Epoch=051/250] [Need: 12:24:14] [learning_rate=0.0020] [Best : Accuracy=87.09, Error=12.91]

==>>Epoch=[051/250]], [2018-03-31 20:49:29], LR=[0.002], Batch=[32] [Model=SimpleNet]
  Epoch: [051][000/1475]   Time 0.137 (0.137)   Data 0.103 (0.103)   Loss 0.3588 (0.3588)   Prec@1 90.625 (90.625)   Prec@5 90.625 (90.625)   [2018-03-31 20:49:29]
  Epoch: [051][200/1475]   Time 0.139 (0.140)   Data 0.105 (0.107)   Loss 0.3309 (0.3568)   Prec@1 84.375 (84.608)   Prec@5 84.375 (84.608)   [2018-03-31 20:49:57]
  Epoch: [051][400/1475]   Time 0.139 (0.140)   Data 0.106 (0.107)   Loss 0.3212 (0.3688)   Prec@1 81.250 (84.281)   Prec@5 81.250 (84.281)   [2018-03-31 20:50:25]
  Epoch: [051][600/1475]   Time 0.139 (0.140)   Data 0.106 (0.106)   Loss 0.4207 (0.3714)   Prec@1 78.125 (84.110)   Prec@5 78.125 (84.110)   [2018-03-31 20:50:53]
  Epoch: [051][800/1475]   Time 0.140 (0.140)   Data 0.107 (0.106)   Loss 0.2637 (0.3721)   Prec@1 90.625 (83.969)   Prec@5 90.625 (83.969)   [2018-03-31 20:51:21]
  Epoch: [051][1000/1475]   Time 0.140 (0.140)   Data 0.107 (0.106)   Loss 0.2412 (0.3736)   Prec@1 96.875 (83.826)   Prec@5 96.875 (83.826)   [2018-03-31 20:51:49]
  Epoch: [051][1200/1475]   Time 0.143 (0.140)   Data 0.109 (0.106)   Loss 0.3002 (0.3713)   Prec@1 90.625 (84.026)   Prec@5 90.625 (84.026)   [2018-03-31 20:52:17]
  Epoch: [051][1400/1475]   Time 0.140 (0.140)   Data 0.107 (0.107)   Loss 0.2407 (0.3706)   Prec@1 87.500 (84.072)   Prec@5 87.500 (84.072)   [2018-03-31 20:52:45]
  **Train** Prec@1 84.084 Prec@5 84.084 Error@1 15.916
  **VAL** Prec@1 86.428 Prec@5 86.428 Error@1 13.572

==>>[2018-03-31 20:53:13] [Epoch=052/250] [Need: 12:20:29] [learning_rate=0.0020] [Best : Accuracy=87.09, Error=12.91]

==>>Epoch=[052/250]], [2018-03-31 20:53:13], LR=[0.002], Batch=[32] [Model=SimpleNet]
  Epoch: [052][000/1475]   Time 0.138 (0.138)   Data 0.104 (0.104)   Loss 0.2627 (0.2627)   Prec@1 90.625 (90.625)   Prec@5 90.625 (90.625)   [2018-03-31 20:53:13]
  Epoch: [052][200/1475]   Time 0.139 (0.140)   Data 0.105 (0.106)   Loss 0.3987 (0.3647)   Prec@1 84.375 (84.562)   Prec@5 84.375 (84.562)   [2018-03-31 20:53:41]
  Epoch: [052][400/1475]   Time 0.139 (0.139)   Data 0.106 (0.106)   Loss 0.3044 (0.3734)   Prec@1 90.625 (83.892)   Prec@5 90.625 (83.892)   [2018-03-31 20:54:09]
  Epoch: [052][600/1475]   Time 0.138 (0.139)   Data 0.105 (0.106)   Loss 0.7400 (0.3715)   Prec@1 68.750 (83.928)   Prec@5 68.750 (83.928)   [2018-03-31 20:54:37]
  Epoch: [052][800/1475]   Time 0.138 (0.139)   Data 0.104 (0.106)   Loss 0.2816 (0.3702)   Prec@1 90.625 (84.016)   Prec@5 90.625 (84.016)   [2018-03-31 20:55:05]
  Epoch: [052][1000/1475]   Time 0.137 (0.139)   Data 0.104 (0.106)   Loss 0.2764 (0.3716)   Prec@1 87.500 (84.010)   Prec@5 87.500 (84.010)   [2018-03-31 20:55:33]
  Epoch: [052][1200/1475]   Time 0.139 (0.139)   Data 0.106 (0.106)   Loss 0.4850 (0.3714)   Prec@1 75.000 (84.024)   Prec@5 75.000 (84.024)   [2018-03-31 20:56:01]
  Epoch: [052][1400/1475]   Time 0.138 (0.139)   Data 0.105 (0.106)   Loss 0.3636 (0.3722)   Prec@1 81.250 (83.994)   Prec@5 81.250 (83.994)   [2018-03-31 20:56:28]
  **Train** Prec@1 83.982 Prec@5 83.982 Error@1 16.018
  **VAL** Prec@1 86.248 Prec@5 86.248 Error@1 13.752

==>>[2018-03-31 20:56:57] [Epoch=053/250] [Need: 12:16:41] [learning_rate=0.0020] [Best : Accuracy=87.09, Error=12.91]

==>>Epoch=[053/250]], [2018-03-31 20:56:57], LR=[0.002], Batch=[32] [Model=SimpleNet]
  Epoch: [053][000/1475]   Time 0.137 (0.137)   Data 0.103 (0.103)   Loss 0.2980 (0.2980)   Prec@1 87.500 (87.500)   Prec@5 87.500 (87.500)   [2018-03-31 20:56:57]
  Epoch: [053][200/1475]   Time 0.138 (0.139)   Data 0.105 (0.106)   Loss 0.3959 (0.3582)   Prec@1 84.375 (84.220)   Prec@5 84.375 (84.220)   [2018-03-31 20:57:25]
  Epoch: [053][400/1475]   Time 0.138 (0.139)   Data 0.105 (0.106)   Loss 0.2886 (0.3690)   Prec@1 87.500 (84.087)   Prec@5 87.500 (84.087)   [2018-03-31 20:57:53]
  Epoch: [053][600/1475]   Time 0.139 (0.139)   Data 0.106 (0.106)   Loss 0.2383 (0.3707)   Prec@1 90.625 (84.079)   Prec@5 90.625 (84.079)   [2018-03-31 20:58:20]
  Epoch: [053][800/1475]   Time 0.141 (0.139)   Data 0.108 (0.106)   Loss 0.1674 (0.3713)   Prec@1 96.875 (84.094)   Prec@5 96.875 (84.094)   [2018-03-31 20:58:48]
  Epoch: [053][1000/1475]   Time 0.138 (0.139)   Data 0.105 (0.106)   Loss 0.3428 (0.3717)   Prec@1 87.500 (84.069)   Prec@5 87.500 (84.069)   [2018-03-31 20:59:16]
  Epoch: [053][1200/1475]   Time 0.142 (0.142)   Data 0.108 (0.108)   Loss 0.2696 (0.3704)   Prec@1 90.625 (84.130)   Prec@5 90.625 (84.130)   [2018-03-31 20:59:47]
  Epoch: [053][1400/1475]   Time 0.147 (0.142)   Data 0.113 (0.109)   Loss 0.6920 (0.3699)   Prec@1 71.875 (84.223)   Prec@5 71.875 (84.223)   [2018-03-31 21:00:16]
  **Train** Prec@1 84.145 Prec@5 84.145 Error@1 15.855
  **VAL** Prec@1 86.128 Prec@5 86.128 Error@1 13.872

==>>[2018-03-31 21:00:45] [Epoch=054/250] [Need: 12:13:10] [learning_rate=0.0020] [Best : Accuracy=87.09, Error=12.91]

==>>Epoch=[054/250]], [2018-03-31 21:00:45], LR=[0.002], Batch=[32] [Model=SimpleNet]
  Epoch: [054][000/1475]   Time 0.143 (0.143)   Data 0.107 (0.107)   Loss 0.5796 (0.5796)   Prec@1 65.625 (65.625)   Prec@5 65.625 (65.625)   [2018-03-31 21:00:45]
  Epoch: [054][200/1475]   Time 0.154 (0.144)   Data 0.120 (0.110)   Loss 0.5297 (0.3838)   Prec@1 75.000 (83.364)   Prec@5 75.000 (83.364)   [2018-03-31 21:01:14]
  Epoch: [054][400/1475]   Time 0.139 (0.142)   Data 0.106 (0.108)   Loss 0.2405 (0.3698)   Prec@1 96.875 (84.063)   Prec@5 96.875 (84.063)   [2018-03-31 21:01:42]
  Epoch: [054][600/1475]   Time 0.138 (0.141)   Data 0.105 (0.108)   Loss 0.2580 (0.3683)   Prec@1 84.375 (84.110)   Prec@5 84.375 (84.110)   [2018-03-31 21:02:09]
  Epoch: [054][800/1475]   Time 0.138 (0.141)   Data 0.105 (0.107)   Loss 0.3201 (0.3699)   Prec@1 84.375 (84.012)   Prec@5 84.375 (84.012)   [2018-03-31 21:02:37]
  Epoch: [054][1000/1475]   Time 0.139 (0.140)   Data 0.105 (0.107)   Loss 0.5912 (0.3684)   Prec@1 71.875 (84.053)   Prec@5 71.875 (84.053)   [2018-03-31 21:03:05]
  Epoch: [054][1200/1475]   Time 0.139 (0.140)   Data 0.105 (0.107)   Loss 0.2993 (0.3696)   Prec@1 90.625 (84.034)   Prec@5 90.625 (84.034)   [2018-03-31 21:03:33]
  Epoch: [054][1400/1475]   Time 0.138 (0.140)   Data 0.104 (0.107)   Loss 0.4384 (0.3702)   Prec@1 75.000 (84.009)   Prec@5 75.000 (84.009)   [2018-03-31 21:04:01]
  **Train** Prec@1 84.001 Prec@5 84.001 Error@1 15.999
  **VAL** Prec@1 87.029 Prec@5 87.029 Error@1 12.971

==>>[2018-03-31 21:04:29] [Epoch=055/250] [Need: 12:09:26] [learning_rate=0.0020] [Best : Accuracy=87.09, Error=12.91]

==>>Epoch=[055/250]], [2018-03-31 21:04:29], LR=[0.002], Batch=[32] [Model=SimpleNet]
  Epoch: [055][000/1475]   Time 0.138 (0.138)   Data 0.104 (0.104)   Loss 0.3508 (0.3508)   Prec@1 87.500 (87.500)   Prec@5 87.500 (87.500)   [2018-03-31 21:04:29]
  Epoch: [055][200/1475]   Time 0.138 (0.140)   Data 0.105 (0.106)   Loss 0.2950 (0.3667)   Prec@1 87.500 (84.188)   Prec@5 87.500 (84.188)   [2018-03-31 21:04:57]
  Epoch: [055][400/1475]   Time 0.142 (0.140)   Data 0.108 (0.106)   Loss 0.3316 (0.3674)   Prec@1 90.625 (84.055)   Prec@5 90.625 (84.055)   [2018-03-31 21:05:25]
  Epoch: [055][600/1475]   Time 0.140 (0.140)   Data 0.107 (0.106)   Loss 0.2488 (0.3655)   Prec@1 87.500 (84.261)   Prec@5 87.500 (84.261)   [2018-03-31 21:05:53]
  Epoch: [055][800/1475]   Time 0.137 (0.140)   Data 0.104 (0.106)   Loss 0.3892 (0.3701)   Prec@1 84.375 (84.024)   Prec@5 84.375 (84.024)   [2018-03-31 21:06:21]
  Epoch: [055][1000/1475]   Time 0.138 (0.140)   Data 0.105 (0.106)   Loss 0.3977 (0.3710)   Prec@1 78.125 (83.960)   Prec@5 78.125 (83.960)   [2018-03-31 21:06:49]
  Epoch: [055][1200/1475]   Time 0.141 (0.140)   Data 0.107 (0.106)   Loss 0.3782 (0.3712)   Prec@1 84.375 (83.930)   Prec@5 84.375 (83.930)   [2018-03-31 21:07:17]
  Epoch: [055][1400/1475]   Time 0.143 (0.140)   Data 0.109 (0.106)   Loss 0.3918 (0.3700)   Prec@1 84.375 (83.982)   Prec@5 84.375 (83.982)   [2018-03-31 21:07:45]
  **Train** Prec@1 84.012 Prec@5 84.012 Error@1 15.988
  **VAL** Prec@1 86.176 Prec@5 86.176 Error@1 13.824

==>>[2018-03-31 21:08:13] [Epoch=056/250] [Need: 12:05:39] [learning_rate=0.0020] [Best : Accuracy=87.09, Error=12.91]

==>>Epoch=[056/250]], [2018-03-31 21:08:13], LR=[0.002], Batch=[32] [Model=SimpleNet]
  Epoch: [056][000/1475]   Time 0.137 (0.137)   Data 0.103 (0.103)   Loss 0.3498 (0.3498)   Prec@1 81.250 (81.250)   Prec@5 81.250 (81.250)   [2018-03-31 21:08:13]
  Epoch: [056][200/1475]   Time 0.140 (0.139)   Data 0.106 (0.106)   Loss 0.7732 (0.3765)   Prec@1 65.625 (83.722)   Prec@5 65.625 (83.722)   [2018-03-31 21:08:41]
  Epoch: [056][400/1475]   Time 0.138 (0.139)   Data 0.105 (0.106)   Loss 0.2204 (0.3668)   Prec@1 93.750 (84.359)   Prec@5 93.750 (84.359)   [2018-03-31 21:09:09]
  Epoch: [056][600/1475]   Time 0.143 (0.139)   Data 0.109 (0.106)   Loss 0.3835 (0.3624)   Prec@1 75.000 (84.495)   Prec@5 75.000 (84.495)   [2018-03-31 21:09:37]
  Epoch: [056][800/1475]   Time 0.143 (0.139)   Data 0.110 (0.106)   Loss 0.3259 (0.3641)   Prec@1 78.125 (84.492)   Prec@5 78.125 (84.492)   [2018-03-31 21:10:05]
  Epoch: [056][1000/1475]   Time 0.138 (0.139)   Data 0.105 (0.106)   Loss 0.2739 (0.3648)   Prec@1 90.625 (84.484)   Prec@5 90.625 (84.484)   [2018-03-31 21:10:33]
  Epoch: [056][1200/1475]   Time 0.137 (0.139)   Data 0.104 (0.106)   Loss 0.2914 (0.3669)   Prec@1 87.500 (84.422)   Prec@5 87.500 (84.422)   [2018-03-31 21:11:00]
  Epoch: [056][1400/1475]   Time 0.138 (0.139)   Data 0.104 (0.106)   Loss 0.2510 (0.3679)   Prec@1 87.500 (84.433)   Prec@5 87.500 (84.433)   [2018-03-31 21:11:28]
  **Train** Prec@1 84.404 Prec@5 84.404 Error@1 15.596
  **VAL** Prec@1 86.308 Prec@5 86.308 Error@1 13.692

==>>[2018-03-31 21:11:57] [Epoch=057/250] [Need: 12:01:52] [learning_rate=0.0020] [Best : Accuracy=87.09, Error=12.91]

==>>Epoch=[057/250]], [2018-03-31 21:11:57], LR=[0.002], Batch=[32] [Model=SimpleNet]
  Epoch: [057][000/1475]   Time 0.141 (0.141)   Data 0.107 (0.107)   Loss 0.3529 (0.3529)   Prec@1 84.375 (84.375)   Prec@5 84.375 (84.375)   [2018-03-31 21:11:57]
  Epoch: [057][200/1475]   Time 0.140 (0.140)   Data 0.107 (0.106)   Loss 0.3674 (0.3716)   Prec@1 84.375 (83.598)   Prec@5 84.375 (83.598)   [2018-03-31 21:12:25]
  Epoch: [057][400/1475]   Time 0.139 (0.140)   Data 0.106 (0.106)   Loss 0.4421 (0.3710)   Prec@1 84.375 (84.126)   Prec@5 84.375 (84.126)   [2018-03-31 21:12:53]
  Epoch: [057][600/1475]   Time 0.138 (0.139)   Data 0.105 (0.106)   Loss 0.4134 (0.3737)   Prec@1 87.500 (83.902)   Prec@5 87.500 (83.902)   [2018-03-31 21:13:21]
  Epoch: [057][800/1475]   Time 0.141 (0.139)   Data 0.108 (0.106)   Loss 0.3579 (0.3728)   Prec@1 81.250 (83.891)   Prec@5 81.250 (83.891)   [2018-03-31 21:13:48]
  Epoch: [057][1000/1475]   Time 0.138 (0.139)   Data 0.105 (0.106)   Loss 0.3062 (0.3719)   Prec@1 84.375 (83.929)   Prec@5 84.375 (83.929)   [2018-03-31 21:14:16]
  Epoch: [057][1200/1475]   Time 0.138 (0.139)   Data 0.105 (0.106)   Loss 0.6074 (0.3722)   Prec@1 68.750 (83.875)   Prec@5 68.750 (83.875)   [2018-03-31 21:14:44]
  Epoch: [057][1400/1475]   Time 0.140 (0.139)   Data 0.107 (0.106)   Loss 0.6679 (0.3711)   Prec@1 62.500 (83.947)   Prec@5 62.500 (83.947)   [2018-03-31 21:15:12]
  **Train** Prec@1 83.967 Prec@5 83.967 Error@1 16.033
  **VAL** Prec@1 86.236 Prec@5 86.236 Error@1 13.764

==>>[2018-03-31 21:15:40] [Epoch=058/250] [Need: 11:58:05] [learning_rate=0.0020] [Best : Accuracy=87.09, Error=12.91]

==>>Epoch=[058/250]], [2018-03-31 21:15:40], LR=[0.002], Batch=[32] [Model=SimpleNet]
  Epoch: [058][000/1475]   Time 0.138 (0.138)   Data 0.104 (0.104)   Loss 0.3477 (0.3477)   Prec@1 81.250 (81.250)   Prec@5 81.250 (81.250)   [2018-03-31 21:15:40]
  Epoch: [058][200/1475]   Time 0.138 (0.139)   Data 0.105 (0.106)   Loss 0.2449 (0.3791)   Prec@1 84.375 (83.831)   Prec@5 84.375 (83.831)   [2018-03-31 21:16:08]
  Epoch: [058][400/1475]   Time 0.139 (0.139)   Data 0.106 (0.106)   Loss 0.6356 (0.3752)   Prec@1 78.125 (83.798)   Prec@5 78.125 (83.798)   [2018-03-31 21:16:36]
  Epoch: [058][600/1475]   Time 0.138 (0.139)   Data 0.105 (0.106)   Loss 0.3733 (0.3734)   Prec@1 87.500 (83.969)   Prec@5 87.500 (83.969)   [2018-03-31 21:17:04]
  Epoch: [058][800/1475]   Time 0.139 (0.139)   Data 0.106 (0.106)   Loss 0.6480 (0.3699)   Prec@1 75.000 (84.184)   Prec@5 75.000 (84.184)   [2018-03-31 21:17:32]
  Epoch: [058][1000/1475]   Time 0.138 (0.139)   Data 0.105 (0.106)   Loss 0.2519 (0.3687)   Prec@1 90.625 (84.334)   Prec@5 90.625 (84.334)   [2018-03-31 21:18:00]
  Epoch: [058][1200/1475]   Time 0.138 (0.139)   Data 0.104 (0.106)   Loss 0.3158 (0.3702)   Prec@1 84.375 (84.287)   Prec@5 84.375 (84.287)   [2018-03-31 21:18:28]
  Epoch: [058][1400/1475]   Time 0.137 (0.139)   Data 0.104 (0.106)   Loss 0.2413 (0.3693)   Prec@1 87.500 (84.315)   Prec@5 87.500 (84.315)   [2018-03-31 21:18:55]
  **Train** Prec@1 84.370 Prec@5 84.370 Error@1 15.630
  **VAL** Prec@1 86.536 Prec@5 86.536 Error@1 13.464

==>>[2018-03-31 21:19:24] [Epoch=059/250] [Need: 11:54:17] [learning_rate=0.0020] [Best : Accuracy=87.09, Error=12.91]

==>>Epoch=[059/250]], [2018-03-31 21:19:24], LR=[0.002], Batch=[32] [Model=SimpleNet]
  Epoch: [059][000/1475]   Time 0.138 (0.138)   Data 0.104 (0.104)   Loss 0.4075 (0.4075)   Prec@1 84.375 (84.375)   Prec@5 84.375 (84.375)   [2018-03-31 21:19:24]
  Epoch: [059][200/1475]   Time 0.138 (0.140)   Data 0.104 (0.106)   Loss 0.2936 (0.3675)   Prec@1 87.500 (84.733)   Prec@5 87.500 (84.733)   [2018-03-31 21:19:52]
  Epoch: [059][400/1475]   Time 0.138 (0.139)   Data 0.105 (0.106)   Loss 0.2942 (0.3654)   Prec@1 93.750 (84.469)   Prec@5 93.750 (84.469)   [2018-03-31 21:20:20]
  Epoch: [059][600/1475]   Time 0.139 (0.139)   Data 0.106 (0.106)   Loss 0.1883 (0.3700)   Prec@1 90.625 (84.276)   Prec@5 90.625 (84.276)   [2018-03-31 21:20:47]
  Epoch: [059][800/1475]   Time 0.138 (0.139)   Data 0.105 (0.106)   Loss 0.3521 (0.3672)   Prec@1 87.500 (84.414)   Prec@5 87.500 (84.414)   [2018-03-31 21:21:15]
  Epoch: [059][1000/1475]   Time 0.141 (0.139)   Data 0.107 (0.106)   Loss 0.3355 (0.3671)   Prec@1 84.375 (84.459)   Prec@5 84.375 (84.459)   [2018-03-31 21:21:43]
  Epoch: [059][1200/1475]   Time 0.138 (0.139)   Data 0.104 (0.106)   Loss 0.2252 (0.3666)   Prec@1 87.500 (84.430)   Prec@5 87.500 (84.430)   [2018-03-31 21:22:11]
  Epoch: [059][1400/1475]   Time 0.138 (0.139)   Data 0.105 (0.106)   Loss 0.5125 (0.3672)   Prec@1 78.125 (84.400)   Prec@5 78.125 (84.400)   [2018-03-31 21:22:39]
  **Train** Prec@1 84.377 Prec@5 84.377 Error@1 15.623
  **VAL** Prec@1 86.260 Prec@5 86.260 Error@1 13.740

==>>[2018-03-31 21:23:07] [Epoch=060/250] [Need: 11:50:30] [learning_rate=0.0020] [Best : Accuracy=87.09, Error=12.91]

==>>Epoch=[060/250]], [2018-03-31 21:23:07], LR=[0.002], Batch=[32] [Model=SimpleNet]
  Epoch: [060][000/1475]   Time 0.139 (0.139)   Data 0.105 (0.105)   Loss 0.3569 (0.3569)   Prec@1 75.000 (75.000)   Prec@5 75.000 (75.000)   [2018-03-31 21:23:07]
  Epoch: [060][200/1475]   Time 0.138 (0.140)   Data 0.105 (0.106)   Loss 0.3339 (0.3746)   Prec@1 84.375 (83.660)   Prec@5 84.375 (83.660)   [2018-03-31 21:23:35]
  Epoch: [060][400/1475]   Time 0.142 (0.139)   Data 0.109 (0.106)   Loss 0.3506 (0.3714)   Prec@1 87.500 (83.954)   Prec@5 87.500 (83.954)   [2018-03-31 21:24:03]
  Epoch: [060][600/1475]   Time 0.139 (0.139)   Data 0.105 (0.106)   Loss 0.4097 (0.3698)   Prec@1 78.125 (84.141)   Prec@5 78.125 (84.141)   [2018-03-31 21:24:31]
  Epoch: [060][800/1475]   Time 0.141 (0.139)   Data 0.107 (0.106)   Loss 0.2162 (0.3666)   Prec@1 90.625 (84.289)   Prec@5 90.625 (84.289)   [2018-03-31 21:24:59]
  Epoch: [060][1000/1475]   Time 0.142 (0.139)   Data 0.108 (0.106)   Loss 0.1951 (0.3660)   Prec@1 96.875 (84.391)   Prec@5 96.875 (84.391)   [2018-03-31 21:25:27]
  Epoch: [060][1200/1475]   Time 0.144 (0.139)   Data 0.111 (0.106)   Loss 0.2424 (0.3662)   Prec@1 90.625 (84.365)   Prec@5 90.625 (84.365)   [2018-03-31 21:25:55]
  Epoch: [060][1400/1475]   Time 0.141 (0.139)   Data 0.107 (0.106)   Loss 0.3997 (0.3685)   Prec@1 87.500 (84.257)   Prec@5 87.500 (84.257)   [2018-03-31 21:26:23]
  **Train** Prec@1 84.245 Prec@5 84.245 Error@1 15.755
  **VAL** Prec@1 86.320 Prec@5 86.320 Error@1 13.680

==>>[2018-03-31 21:26:51] [Epoch=061/250] [Need: 11:46:43] [learning_rate=0.0020] [Best : Accuracy=87.09, Error=12.91]

==>>Epoch=[061/250]], [2018-03-31 21:26:51], LR=[0.002], Batch=[32] [Model=SimpleNet]
  Epoch: [061][000/1475]   Time 0.148 (0.148)   Data 0.114 (0.114)   Loss 0.5062 (0.5062)   Prec@1 78.125 (78.125)   Prec@5 78.125 (78.125)   [2018-03-31 21:26:51]
  Epoch: [061][200/1475]   Time 0.139 (0.140)   Data 0.106 (0.106)   Loss 0.4109 (0.3678)   Prec@1 78.125 (84.251)   Prec@5 78.125 (84.251)   [2018-03-31 21:27:19]
  Epoch: [061][400/1475]   Time 0.138 (0.139)   Data 0.105 (0.106)   Loss 0.3800 (0.3648)   Prec@1 78.125 (84.624)   Prec@5 78.125 (84.624)   [2018-03-31 21:27:47]
  Epoch: [061][600/1475]   Time 0.138 (0.139)   Data 0.105 (0.106)   Loss 0.2379 (0.3676)   Prec@1 93.750 (84.359)   Prec@5 93.750 (84.359)   [2018-03-31 21:28:15]
  Epoch: [061][800/1475]   Time 0.139 (0.139)   Data 0.106 (0.106)   Loss 0.3028 (0.3694)   Prec@1 87.500 (84.235)   Prec@5 87.500 (84.235)   [2018-03-31 21:28:42]
  Epoch: [061][1000/1475]   Time 0.138 (0.139)   Data 0.105 (0.106)   Loss 0.2692 (0.3677)   Prec@1 90.625 (84.325)   Prec@5 90.625 (84.325)   [2018-03-31 21:29:10]
  Epoch: [061][1200/1475]   Time 0.137 (0.139)   Data 0.104 (0.106)   Loss 0.5739 (0.3678)   Prec@1 75.000 (84.268)   Prec@5 75.000 (84.268)   [2018-03-31 21:29:38]
  Epoch: [061][1400/1475]   Time 0.142 (0.139)   Data 0.108 (0.106)   Loss 0.3962 (0.3685)   Prec@1 87.500 (84.201)   Prec@5 87.500 (84.201)   [2018-03-31 21:30:06]
  **Train** Prec@1 84.177 Prec@5 84.177 Error@1 15.823
  **VAL** Prec@1 86.548 Prec@5 86.548 Error@1 13.452

==>>[2018-03-31 21:30:34] [Epoch=062/250] [Need: 11:42:56] [learning_rate=0.0020] [Best : Accuracy=87.09, Error=12.91]

==>>Epoch=[062/250]], [2018-03-31 21:30:34], LR=[0.002], Batch=[32] [Model=SimpleNet]
  Epoch: [062][000/1475]   Time 0.138 (0.138)   Data 0.104 (0.104)   Loss 0.2335 (0.2335)   Prec@1 90.625 (90.625)   Prec@5 90.625 (90.625)   [2018-03-31 21:30:34]
  Epoch: [062][200/1475]   Time 0.139 (0.139)   Data 0.105 (0.106)   Loss 0.2219 (0.3693)   Prec@1 90.625 (84.562)   Prec@5 90.625 (84.562)   [2018-03-31 21:31:02]
  Epoch: [062][400/1475]   Time 0.138 (0.139)   Data 0.104 (0.106)   Loss 0.3282 (0.3712)   Prec@1 84.375 (84.094)   Prec@5 84.375 (84.094)   [2018-03-31 21:31:30]
  Epoch: [062][600/1475]   Time 0.138 (0.139)   Data 0.105 (0.106)   Loss 0.5699 (0.3679)   Prec@1 78.125 (84.136)   Prec@5 78.125 (84.136)   [2018-03-31 21:31:58]
  Epoch: [062][800/1475]   Time 0.138 (0.139)   Data 0.104 (0.106)   Loss 0.3268 (0.3692)   Prec@1 81.250 (84.184)   Prec@5 81.250 (84.184)   [2018-03-31 21:32:26]
  Epoch: [062][1000/1475]   Time 0.151 (0.139)   Data 0.118 (0.106)   Loss 0.4587 (0.3688)   Prec@1 78.125 (84.206)   Prec@5 78.125 (84.206)   [2018-03-31 21:32:54]
  Epoch: [062][1200/1475]   Time 0.137 (0.139)   Data 0.104 (0.106)   Loss 0.2732 (0.3674)   Prec@1 84.375 (84.315)   Prec@5 84.375 (84.315)   [2018-03-31 21:33:22]
  Epoch: [062][1400/1475]   Time 0.137 (0.139)   Data 0.104 (0.106)   Loss 0.3366 (0.3675)   Prec@1 90.625 (84.205)   Prec@5 90.625 (84.205)   [2018-03-31 21:33:50]
  **Train** Prec@1 84.245 Prec@5 84.245 Error@1 15.755
  **VAL** Prec@1 86.632 Prec@5 86.632 Error@1 13.368

==>>[2018-03-31 21:34:18] [Epoch=063/250] [Need: 11:39:10] [learning_rate=0.0020] [Best : Accuracy=87.09, Error=12.91]

==>>Epoch=[063/250]], [2018-03-31 21:34:18], LR=[0.002], Batch=[32] [Model=SimpleNet]
  Epoch: [063][000/1475]   Time 0.139 (0.139)   Data 0.105 (0.105)   Loss 0.5472 (0.5472)   Prec@1 81.250 (81.250)   Prec@5 81.250 (81.250)   [2018-03-31 21:34:18]
  Epoch: [063][200/1475]   Time 0.156 (0.145)   Data 0.123 (0.112)   Loss 0.3750 (0.3761)   Prec@1 84.375 (83.815)   Prec@5 84.375 (83.815)   [2018-03-31 21:34:47]
  Epoch: [063][400/1475]   Time 0.154 (0.151)   Data 0.121 (0.117)   Loss 0.2874 (0.3726)   Prec@1 84.375 (84.040)   Prec@5 84.375 (84.040)   [2018-03-31 21:35:19]
  Epoch: [063][600/1475]   Time 0.160 (0.153)   Data 0.126 (0.119)   Loss 0.1777 (0.3667)   Prec@1 93.750 (84.432)   Prec@5 93.750 (84.432)   [2018-03-31 21:35:50]
  Epoch: [063][800/1475]   Time 0.156 (0.154)   Data 0.123 (0.120)   Loss 0.4571 (0.3682)   Prec@1 84.375 (84.320)   Prec@5 84.375 (84.320)   [2018-03-31 21:36:21]
  Epoch: [063][1000/1475]   Time 0.154 (0.154)   Data 0.121 (0.121)   Loss 0.5228 (0.3690)   Prec@1 75.000 (84.275)   Prec@5 75.000 (84.275)   [2018-03-31 21:36:52]
  Epoch: [063][1200/1475]   Time 0.157 (0.155)   Data 0.124 (0.121)   Loss 0.2949 (0.3712)   Prec@1 90.625 (84.216)   Prec@5 90.625 (84.216)   [2018-03-31 21:37:24]
  Epoch: [063][1400/1475]   Time 0.155 (0.155)   Data 0.122 (0.122)   Loss 0.2673 (0.3702)   Prec@1 90.625 (84.241)   Prec@5 90.625 (84.241)   [2018-03-31 21:37:55]
  **Train** Prec@1 84.288 Prec@5 84.288 Error@1 15.712
  **VAL** Prec@1 87.065 Prec@5 87.065 Error@1 12.935

==>>[2018-03-31 21:38:28] [Epoch=064/250] [Need: 11:36:42] [learning_rate=0.0020] [Best : Accuracy=87.09, Error=12.91]

==>>Epoch=[064/250]], [2018-03-31 21:38:28], LR=[0.002], Batch=[32] [Model=SimpleNet]
  Epoch: [064][000/1475]   Time 0.142 (0.142)   Data 0.108 (0.108)   Loss 0.3171 (0.3171)   Prec@1 84.375 (84.375)   Prec@5 84.375 (84.375)   [2018-03-31 21:38:29]
  Epoch: [064][200/1475]   Time 0.141 (0.142)   Data 0.107 (0.109)   Loss 0.2744 (0.3566)   Prec@1 87.500 (84.904)   Prec@5 87.500 (84.904)   [2018-03-31 21:38:57]
  Epoch: [064][400/1475]   Time 0.156 (0.142)   Data 0.122 (0.109)   Loss 0.4828 (0.3600)   Prec@1 71.875 (84.687)   Prec@5 71.875 (84.687)   [2018-03-31 21:39:25]
  Epoch: [064][600/1475]   Time 0.142 (0.142)   Data 0.108 (0.109)   Loss 0.3356 (0.3660)   Prec@1 84.375 (84.359)   Prec@5 84.375 (84.359)   [2018-03-31 21:39:54]
  Epoch: [064][800/1475]   Time 0.140 (0.142)   Data 0.107 (0.109)   Loss 0.4287 (0.3675)   Prec@1 78.125 (84.188)   Prec@5 78.125 (84.188)   [2018-03-31 21:40:22]
  Epoch: [064][1000/1475]   Time 0.143 (0.142)   Data 0.110 (0.109)   Loss 0.2420 (0.3666)   Prec@1 87.500 (84.197)   Prec@5 87.500 (84.197)   [2018-03-31 21:40:51]
  Epoch: [064][1200/1475]   Time 0.144 (0.142)   Data 0.110 (0.109)   Loss 0.4436 (0.3658)   Prec@1 84.375 (84.287)   Prec@5 84.375 (84.287)   [2018-03-31 21:41:19]
  Epoch: [064][1400/1475]   Time 0.139 (0.142)   Data 0.105 (0.109)   Loss 0.3625 (0.3652)   Prec@1 81.250 (84.373)   Prec@5 81.250 (84.373)   [2018-03-31 21:41:48]
  **Train** Prec@1 84.336 Prec@5 84.336 Error@1 15.664
  **VAL** Prec@1 86.536 Prec@5 86.536 Error@1 13.464

==>>[2018-03-31 21:42:16] [Epoch=065/250] [Need: 11:33:06] [learning_rate=0.0020] [Best : Accuracy=87.09, Error=12.91]

==>>Epoch=[065/250]], [2018-03-31 21:42:16], LR=[0.002], Batch=[32] [Model=SimpleNet]
  Epoch: [065][000/1475]   Time 0.148 (0.148)   Data 0.114 (0.114)   Loss 0.3577 (0.3577)   Prec@1 81.250 (81.250)   Prec@5 81.250 (81.250)   [2018-03-31 21:42:16]
  Epoch: [065][200/1475]   Time 0.138 (0.139)   Data 0.105 (0.106)   Loss 0.4893 (0.3674)   Prec@1 75.000 (84.375)   Prec@5 75.000 (84.375)   [2018-03-31 21:42:44]
  Epoch: [065][400/1475]   Time 0.137 (0.139)   Data 0.104 (0.106)   Loss 0.2935 (0.3691)   Prec@1 87.500 (84.305)   Prec@5 87.500 (84.305)   [2018-03-31 21:43:12]
  Epoch: [065][600/1475]   Time 0.139 (0.139)   Data 0.106 (0.106)   Loss 0.2788 (0.3628)   Prec@1 87.500 (84.578)   Prec@5 87.500 (84.578)   [2018-03-31 21:43:40]
  Epoch: [065][800/1475]   Time 0.138 (0.139)   Data 0.105 (0.106)   Loss 0.3547 (0.3657)   Prec@1 84.375 (84.422)   Prec@5 84.375 (84.422)   [2018-03-31 21:44:08]
  Epoch: [065][1000/1475]   Time 0.142 (0.139)   Data 0.108 (0.106)   Loss 0.3857 (0.3684)   Prec@1 81.250 (84.291)   Prec@5 81.250 (84.291)   [2018-03-31 21:44:36]
  Epoch: [065][1200/1475]   Time 0.141 (0.139)   Data 0.108 (0.106)   Loss 0.3860 (0.3659)   Prec@1 81.250 (84.492)   Prec@5 81.250 (84.492)   [2018-03-31 21:45:04]
  Epoch: [065][1400/1475]   Time 0.138 (0.139)   Data 0.105 (0.106)   Loss 0.3744 (0.3665)   Prec@1 81.250 (84.442)   Prec@5 81.250 (84.442)   [2018-03-31 21:45:32]
  **Train** Prec@1 84.483 Prec@5 84.483 Error@1 15.517
  **VAL** Prec@1 87.017 Prec@5 87.017 Error@1 12.983

==>>[2018-03-31 21:46:00] [Epoch=066/250] [Need: 11:29:18] [learning_rate=0.0020] [Best : Accuracy=87.09, Error=12.91]

==>>Epoch=[066/250]], [2018-03-31 21:46:00], LR=[0.002], Batch=[32] [Model=SimpleNet]
  Epoch: [066][000/1475]   Time 0.145 (0.145)   Data 0.111 (0.111)   Loss 0.4302 (0.4302)   Prec@1 84.375 (84.375)   Prec@5 84.375 (84.375)   [2018-03-31 21:46:00]
  Epoch: [066][200/1475]   Time 0.139 (0.141)   Data 0.106 (0.107)   Loss 0.2764 (0.3672)   Prec@1 84.375 (84.188)   Prec@5 84.375 (84.188)   [2018-03-31 21:46:28]
  Epoch: [066][400/1475]   Time 0.139 (0.140)   Data 0.106 (0.107)   Loss 0.3515 (0.3661)   Prec@1 81.250 (84.469)   Prec@5 81.250 (84.469)   [2018-03-31 21:46:56]
  Epoch: [066][600/1475]   Time 0.139 (0.140)   Data 0.106 (0.107)   Loss 0.4926 (0.3652)   Prec@1 81.250 (84.463)   Prec@5 81.250 (84.463)   [2018-03-31 21:47:24]
  Epoch: [066][800/1475]   Time 0.138 (0.140)   Data 0.104 (0.106)   Loss 0.4161 (0.3703)   Prec@1 81.250 (84.211)   Prec@5 81.250 (84.211)   [2018-03-31 21:47:52]
  Epoch: [066][1000/1475]   Time 0.138 (0.140)   Data 0.105 (0.106)   Loss 0.4427 (0.3684)   Prec@1 84.375 (84.347)   Prec@5 84.375 (84.347)   [2018-03-31 21:48:20]
  Epoch: [066][1200/1475]   Time 0.139 (0.140)   Data 0.106 (0.106)   Loss 0.2352 (0.3698)   Prec@1 87.500 (84.310)   Prec@5 87.500 (84.310)   [2018-03-31 21:48:48]
  Epoch: [066][1400/1475]   Time 0.138 (0.140)   Data 0.105 (0.106)   Loss 0.1335 (0.3690)   Prec@1 96.875 (84.337)   Prec@5 96.875 (84.337)   [2018-03-31 21:49:16]
  **Train** Prec@1 84.351 Prec@5 84.351 Error@1 15.649
  **VAL** Prec@1 87.341 Prec@5 87.341 Error@1 12.659

==>>[2018-03-31 21:49:44] [Epoch=067/250] [Need: 11:25:31] [learning_rate=0.0020] [Best : Accuracy=87.34, Error=12.66]

==>>Epoch=[067/250]], [2018-03-31 21:49:44], LR=[0.002], Batch=[32] [Model=SimpleNet]
  Epoch: [067][000/1475]   Time 0.139 (0.139)   Data 0.105 (0.105)   Loss 0.3429 (0.3429)   Prec@1 90.625 (90.625)   Prec@5 90.625 (90.625)   [2018-03-31 21:49:44]
  Epoch: [067][200/1475]   Time 0.137 (0.140)   Data 0.104 (0.106)   Loss 0.3770 (0.3741)   Prec@1 81.250 (83.660)   Prec@5 81.250 (83.660)   [2018-03-31 21:50:12]
  Epoch: [067][400/1475]   Time 0.137 (0.139)   Data 0.104 (0.106)   Loss 0.2302 (0.3665)   Prec@1 90.625 (84.071)   Prec@5 90.625 (84.071)   [2018-03-31 21:50:40]
  Epoch: [067][600/1475]   Time 0.138 (0.139)   Data 0.105 (0.106)   Loss 0.3558 (0.3649)   Prec@1 84.375 (84.266)   Prec@5 84.375 (84.266)   [2018-03-31 21:51:08]
  Epoch: [067][800/1475]   Time 0.142 (0.139)   Data 0.108 (0.106)   Loss 0.4959 (0.3667)   Prec@1 81.250 (84.242)   Prec@5 81.250 (84.242)   [2018-03-31 21:51:35]
  Epoch: [067][1000/1475]   Time 0.138 (0.139)   Data 0.105 (0.106)   Loss 0.5367 (0.3656)   Prec@1 71.875 (84.369)   Prec@5 71.875 (84.369)   [2018-03-31 21:52:03]
  Epoch: [067][1200/1475]   Time 0.141 (0.139)   Data 0.107 (0.106)   Loss 0.2575 (0.3683)   Prec@1 90.625 (84.263)   Prec@5 90.625 (84.263)   [2018-03-31 21:52:31]
  Epoch: [067][1400/1475]   Time 0.139 (0.139)   Data 0.105 (0.106)   Loss 0.2675 (0.3679)   Prec@1 87.500 (84.270)   Prec@5 87.500 (84.270)   [2018-03-31 21:52:59]
  **Train** Prec@1 84.275 Prec@5 84.275 Error@1 15.725
  **VAL** Prec@1 86.200 Prec@5 86.200 Error@1 13.800

==>>[2018-03-31 21:53:27] [Epoch=068/250] [Need: 11:21:43] [learning_rate=0.0020] [Best : Accuracy=87.34, Error=12.66]

==>>Epoch=[068/250]], [2018-03-31 21:53:27], LR=[0.002], Batch=[32] [Model=SimpleNet]
  Epoch: [068][000/1475]   Time 0.138 (0.138)   Data 0.104 (0.104)   Loss 0.3950 (0.3950)   Prec@1 78.125 (78.125)   Prec@5 78.125 (78.125)   [2018-03-31 21:53:28]
  Epoch: [068][200/1475]   Time 0.139 (0.140)   Data 0.106 (0.106)   Loss 0.2625 (0.3801)   Prec@1 90.625 (83.504)   Prec@5 90.625 (83.504)   [2018-03-31 21:53:55]
  Epoch: [068][400/1475]   Time 0.138 (0.140)   Data 0.105 (0.106)   Loss 0.4556 (0.3760)   Prec@1 75.000 (83.759)   Prec@5 75.000 (83.759)   [2018-03-31 21:54:23]
  Epoch: [068][600/1475]   Time 0.139 (0.140)   Data 0.106 (0.106)   Loss 0.4837 (0.3661)   Prec@1 75.000 (84.261)   Prec@5 75.000 (84.261)   [2018-03-31 21:54:51]
  Epoch: [068][800/1475]   Time 0.138 (0.140)   Data 0.105 (0.106)   Loss 0.3061 (0.3667)   Prec@1 84.375 (84.203)   Prec@5 84.375 (84.203)   [2018-03-31 21:55:19]
  Epoch: [068][1000/1475]   Time 0.138 (0.140)   Data 0.105 (0.106)   Loss 0.3619 (0.3668)   Prec@1 81.250 (84.263)   Prec@5 81.250 (84.263)   [2018-03-31 21:55:47]
  Epoch: [068][1200/1475]   Time 0.141 (0.140)   Data 0.108 (0.106)   Loss 0.4282 (0.3682)   Prec@1 78.125 (84.245)   Prec@5 78.125 (84.245)   [2018-03-31 21:56:15]
  Epoch: [068][1400/1475]   Time 0.141 (0.139)   Data 0.108 (0.106)   Loss 0.4019 (0.3670)   Prec@1 81.250 (84.288)   Prec@5 81.250 (84.288)   [2018-03-31 21:56:43]
  **Train** Prec@1 84.235 Prec@5 84.235 Error@1 15.765
  **VAL** Prec@1 87.089 Prec@5 87.089 Error@1 12.911

==>>[2018-03-31 21:57:11] [Epoch=069/250] [Need: 11:17:55] [learning_rate=0.0020] [Best : Accuracy=87.34, Error=12.66]

==>>Epoch=[069/250]], [2018-03-31 21:57:11], LR=[0.002], Batch=[32] [Model=SimpleNet]
  Epoch: [069][000/1475]   Time 0.141 (0.141)   Data 0.106 (0.106)   Loss 0.2245 (0.2245)   Prec@1 93.750 (93.750)   Prec@5 93.750 (93.750)   [2018-03-31 21:57:11]
  Epoch: [069][200/1475]   Time 0.138 (0.140)   Data 0.105 (0.106)   Loss 0.2780 (0.3688)   Prec@1 87.500 (84.453)   Prec@5 87.500 (84.453)   [2018-03-31 21:57:39]
  Epoch: [069][400/1475]   Time 0.137 (0.140)   Data 0.104 (0.106)   Loss 0.2823 (0.3691)   Prec@1 90.625 (84.367)   Prec@5 90.625 (84.367)   [2018-03-31 21:58:07]
  Epoch: [069][600/1475]   Time 0.138 (0.140)   Data 0.105 (0.106)   Loss 0.5254 (0.3668)   Prec@1 75.000 (84.417)   Prec@5 75.000 (84.417)   [2018-03-31 21:58:35]
  Epoch: [069][800/1475]   Time 0.138 (0.139)   Data 0.104 (0.106)   Loss 0.3277 (0.3656)   Prec@1 81.250 (84.422)   Prec@5 81.250 (84.422)   [2018-03-31 21:59:03]
  Epoch: [069][1000/1475]   Time 0.139 (0.139)   Data 0.106 (0.106)   Loss 0.5748 (0.3675)   Prec@1 75.000 (84.294)   Prec@5 75.000 (84.294)   [2018-03-31 21:59:31]
  Epoch: [069][1200/1475]   Time 0.138 (0.139)   Data 0.105 (0.106)   Loss 0.4936 (0.3684)   Prec@1 68.750 (84.263)   Prec@5 68.750 (84.263)   [2018-03-31 21:59:59]
  Epoch: [069][1400/1475]   Time 0.138 (0.139)   Data 0.105 (0.106)   Loss 0.1715 (0.3690)   Prec@1 93.750 (84.297)   Prec@5 93.750 (84.297)   [2018-03-31 22:00:26]
  **Train** Prec@1 84.328 Prec@5 84.328 Error@1 15.672
  **VAL** Prec@1 86.957 Prec@5 86.957 Error@1 13.043

==>>[2018-03-31 22:00:55] [Epoch=070/250] [Need: 11:14:08] [learning_rate=0.0020] [Best : Accuracy=87.34, Error=12.66]

==>>Epoch=[070/250]], [2018-03-31 22:00:55], LR=[0.002], Batch=[32] [Model=SimpleNet]
  Epoch: [070][000/1475]   Time 0.138 (0.138)   Data 0.104 (0.104)   Loss 0.3092 (0.3092)   Prec@1 87.500 (87.500)   Prec@5 87.500 (87.500)   [2018-03-31 22:00:55]
  Epoch: [070][200/1475]   Time 0.141 (0.140)   Data 0.108 (0.107)   Loss 0.4098 (0.3649)   Prec@1 75.000 (84.391)   Prec@5 75.000 (84.391)   [2018-03-31 22:01:23]
  Epoch: [070][400/1475]   Time 0.141 (0.140)   Data 0.107 (0.106)   Loss 0.4515 (0.3618)   Prec@1 78.125 (84.500)   Prec@5 78.125 (84.500)   [2018-03-31 22:01:51]
  Epoch: [070][600/1475]   Time 0.140 (0.140)   Data 0.106 (0.107)   Loss 0.4932 (0.3652)   Prec@1 75.000 (84.193)   Prec@5 75.000 (84.193)   [2018-03-31 22:02:19]
  Epoch: [070][800/1475]   Time 0.140 (0.140)   Data 0.107 (0.107)   Loss 0.3952 (0.3684)   Prec@1 78.125 (84.012)   Prec@5 78.125 (84.012)   [2018-03-31 22:02:47]
  Epoch: [070][1000/1475]   Time 0.139 (0.140)   Data 0.106 (0.107)   Loss 0.3541 (0.3667)   Prec@1 81.250 (84.153)   Prec@5 81.250 (84.153)   [2018-03-31 22:03:15]
  Epoch: [070][1200/1475]   Time 0.139 (0.140)   Data 0.105 (0.107)   Loss 0.3060 (0.3668)   Prec@1 90.625 (84.242)   Prec@5 90.625 (84.242)   [2018-03-31 22:03:43]
  Epoch: [070][1400/1475]   Time 0.140 (0.140)   Data 0.107 (0.106)   Loss 0.3267 (0.3678)   Prec@1 87.500 (84.219)   Prec@5 87.500 (84.219)   [2018-03-31 22:04:11]
  **Train** Prec@1 84.239 Prec@5 84.239 Error@1 15.761
  **VAL** Prec@1 87.161 Prec@5 87.161 Error@1 12.839

==>>[2018-03-31 22:04:39] [Epoch=071/250] [Need: 11:10:22] [learning_rate=0.0020] [Best : Accuracy=87.34, Error=12.66]

==>>Epoch=[071/250]], [2018-03-31 22:04:39], LR=[0.002], Batch=[32] [Model=SimpleNet]
  Epoch: [071][000/1475]   Time 0.137 (0.137)   Data 0.103 (0.103)   Loss 0.3509 (0.3509)   Prec@1 81.250 (81.250)   Prec@5 81.250 (81.250)   [2018-03-31 22:04:39]
  Epoch: [071][200/1475]   Time 0.139 (0.140)   Data 0.105 (0.106)   Loss 0.4373 (0.3689)   Prec@1 81.250 (83.784)   Prec@5 81.250 (83.784)   [2018-03-31 22:05:07]
  Epoch: [071][400/1475]   Time 0.140 (0.140)   Data 0.107 (0.106)   Loss 0.3082 (0.3742)   Prec@1 84.375 (83.736)   Prec@5 84.375 (83.736)   [2018-03-31 22:05:35]
  Epoch: [071][600/1475]   Time 0.142 (0.140)   Data 0.107 (0.106)   Loss 0.4685 (0.3682)   Prec@1 75.000 (84.131)   Prec@5 75.000 (84.131)   [2018-03-31 22:06:03]
  Epoch: [071][800/1475]   Time 0.140 (0.140)   Data 0.107 (0.106)   Loss 0.3058 (0.3689)   Prec@1 84.375 (84.067)   Prec@5 84.375 (84.067)   [2018-03-31 22:06:31]
  Epoch: [071][1000/1475]   Time 0.138 (0.140)   Data 0.105 (0.106)   Loss 0.2542 (0.3651)   Prec@1 90.625 (84.306)   Prec@5 90.625 (84.306)   [2018-03-31 22:06:59]
  Epoch: [071][1200/1475]   Time 0.138 (0.140)   Data 0.105 (0.106)   Loss 0.4476 (0.3646)   Prec@1 78.125 (84.375)   Prec@5 78.125 (84.375)   [2018-03-31 22:07:27]
  Epoch: [071][1400/1475]   Time 0.142 (0.140)   Data 0.109 (0.106)   Loss 0.3112 (0.3662)   Prec@1 84.375 (84.292)   Prec@5 84.375 (84.292)   [2018-03-31 22:07:54]
  **Train** Prec@1 84.243 Prec@5 84.243 Error@1 15.757
  **VAL** Prec@1 86.224 Prec@5 86.224 Error@1 13.776

==>>[2018-03-31 22:08:23] [Epoch=072/250] [Need: 11:06:35] [learning_rate=0.0020] [Best : Accuracy=87.34, Error=12.66]

==>>Epoch=[072/250]], [2018-03-31 22:08:23], LR=[0.002], Batch=[32] [Model=SimpleNet]
  Epoch: [072][000/1475]   Time 0.138 (0.138)   Data 0.104 (0.104)   Loss 0.3288 (0.3288)   Prec@1 84.375 (84.375)   Prec@5 84.375 (84.375)   [2018-03-31 22:08:23]
  Epoch: [072][200/1475]   Time 0.140 (0.140)   Data 0.107 (0.106)   Loss 0.3552 (0.3678)   Prec@1 81.250 (83.909)   Prec@5 81.250 (83.909)   [2018-03-31 22:08:51]
  Epoch: [072][400/1475]   Time 0.144 (0.140)   Data 0.110 (0.106)   Loss 0.2487 (0.3603)   Prec@1 90.625 (84.484)   Prec@5 90.625 (84.484)   [2018-03-31 22:09:19]
  Epoch: [072][600/1475]   Time 0.137 (0.140)   Data 0.104 (0.106)   Loss 0.3797 (0.3594)   Prec@1 84.375 (84.510)   Prec@5 84.375 (84.510)   [2018-03-31 22:09:47]
  Epoch: [072][800/1475]   Time 0.141 (0.140)   Data 0.107 (0.106)   Loss 0.2635 (0.3647)   Prec@1 90.625 (84.293)   Prec@5 90.625 (84.293)   [2018-03-31 22:10:15]
  Epoch: [072][1000/1475]   Time 0.137 (0.140)   Data 0.104 (0.106)   Loss 0.4096 (0.3650)   Prec@1 78.125 (84.350)   Prec@5 78.125 (84.350)   [2018-03-31 22:10:43]
  Epoch: [072][1200/1475]   Time 0.141 (0.140)   Data 0.108 (0.106)   Loss 0.2922 (0.3633)   Prec@1 90.625 (84.453)   Prec@5 90.625 (84.453)   [2018-03-31 22:11:10]
  Epoch: [072][1400/1475]   Time 0.138 (0.140)   Data 0.105 (0.106)   Loss 0.4267 (0.3629)   Prec@1 78.125 (84.500)   Prec@5 78.125 (84.500)   [2018-03-31 22:11:38]
  **Train** Prec@1 84.459 Prec@5 84.459 Error@1 15.541
  **VAL** Prec@1 86.452 Prec@5 86.452 Error@1 13.548

==>>[2018-03-31 22:12:07] [Epoch=073/250] [Need: 11:02:48] [learning_rate=0.0020] [Best : Accuracy=87.34, Error=12.66]

==>>Epoch=[073/250]], [2018-03-31 22:12:07], LR=[0.002], Batch=[32] [Model=SimpleNet]
  Epoch: [073][000/1475]   Time 0.138 (0.138)   Data 0.104 (0.104)   Loss 0.3519 (0.3519)   Prec@1 87.500 (87.500)   Prec@5 87.500 (87.500)   [2018-03-31 22:12:07]
  Epoch: [073][200/1475]   Time 0.140 (0.140)   Data 0.107 (0.107)   Loss 0.3498 (0.3679)   Prec@1 87.500 (84.328)   Prec@5 87.500 (84.328)   [2018-03-31 22:12:35]
  Epoch: [073][400/1475]   Time 0.142 (0.140)   Data 0.108 (0.107)   Loss 0.3579 (0.3630)   Prec@1 87.500 (84.609)   Prec@5 87.500 (84.609)   [2018-03-31 22:13:03]
  Epoch: [073][600/1475]   Time 0.139 (0.140)   Data 0.105 (0.106)   Loss 0.3769 (0.3630)   Prec@1 87.500 (84.739)   Prec@5 87.500 (84.739)   [2018-03-31 22:13:31]
  Epoch: [073][800/1475]   Time 0.140 (0.140)   Data 0.107 (0.106)   Loss 0.2476 (0.3630)   Prec@1 90.625 (84.734)   Prec@5 90.625 (84.734)   [2018-03-31 22:13:59]
  Epoch: [073][1000/1475]   Time 0.138 (0.140)   Data 0.105 (0.107)   Loss 0.3497 (0.3648)   Prec@1 84.375 (84.572)   Prec@5 84.375 (84.572)   [2018-03-31 22:14:27]
  Epoch: [073][1200/1475]   Time 0.138 (0.140)   Data 0.105 (0.106)   Loss 0.2473 (0.3654)   Prec@1 93.750 (84.479)   Prec@5 93.750 (84.479)   [2018-03-31 22:14:55]
  Epoch: [073][1400/1475]   Time 0.140 (0.140)   Data 0.107 (0.107)   Loss 0.3190 (0.3658)   Prec@1 90.625 (84.431)   Prec@5 90.625 (84.431)   [2018-03-31 22:15:23]
  **Train** Prec@1 84.332 Prec@5 84.332 Error@1 15.668
  **VAL** Prec@1 86.236 Prec@5 86.236 Error@1 13.764

==>>[2018-03-31 22:15:51] [Epoch=074/250] [Need: 10:59:03] [learning_rate=0.0020] [Best : Accuracy=87.34, Error=12.66]

==>>Epoch=[074/250]], [2018-03-31 22:15:51], LR=[0.002], Batch=[32] [Model=SimpleNet]
  Epoch: [074][000/1475]   Time 0.138 (0.138)   Data 0.104 (0.104)   Loss 0.3913 (0.3913)   Prec@1 84.375 (84.375)   Prec@5 84.375 (84.375)   [2018-03-31 22:15:51]
  Epoch: [074][200/1475]   Time 0.139 (0.140)   Data 0.106 (0.106)   Loss 0.1801 (0.3638)   Prec@1 96.875 (84.095)   Prec@5 96.875 (84.095)   [2018-03-31 22:16:19]
  Epoch: [074][400/1475]   Time 0.143 (0.140)   Data 0.109 (0.107)   Loss 0.3182 (0.3744)   Prec@1 87.500 (83.705)   Prec@5 87.500 (83.705)   [2018-03-31 22:16:47]
  Epoch: [074][600/1475]   Time 0.139 (0.140)   Data 0.106 (0.107)   Loss 0.2543 (0.3687)   Prec@1 90.625 (84.021)   Prec@5 90.625 (84.021)   [2018-03-31 22:17:15]
  Epoch: [074][800/1475]   Time 0.140 (0.140)   Data 0.107 (0.107)   Loss 0.2873 (0.3643)   Prec@1 90.625 (84.398)   Prec@5 90.625 (84.398)   [2018-03-31 22:17:43]
  Epoch: [074][1000/1475]   Time 0.143 (0.140)   Data 0.109 (0.107)   Loss 0.2827 (0.3623)   Prec@1 87.500 (84.472)   Prec@5 87.500 (84.472)   [2018-03-31 22:18:11]
  Epoch: [074][1200/1475]   Time 0.141 (0.140)   Data 0.107 (0.107)   Loss 0.3130 (0.3647)   Prec@1 87.500 (84.388)   Prec@5 87.500 (84.388)   [2018-03-31 22:18:39]
  Epoch: [074][1400/1475]   Time 0.140 (0.140)   Data 0.107 (0.107)   Loss 0.4815 (0.3672)   Prec@1 78.125 (84.306)   Prec@5 78.125 (84.306)   [2018-03-31 22:19:07]
  **Train** Prec@1 84.334 Prec@5 84.334 Error@1 15.666
  **VAL** Prec@1 86.188 Prec@5 86.188 Error@1 13.812

==>>[2018-03-31 22:19:36] [Epoch=075/250] [Need: 10:55:18] [learning_rate=0.0020] [Best : Accuracy=87.34, Error=12.66]

==>>Epoch=[075/250]], [2018-03-31 22:19:36], LR=[0.002], Batch=[32] [Model=SimpleNet]
  Epoch: [075][000/1475]   Time 0.138 (0.138)   Data 0.104 (0.104)   Loss 0.3033 (0.3033)   Prec@1 87.500 (87.500)   Prec@5 87.500 (87.500)   [2018-03-31 22:19:36]
  Epoch: [075][200/1475]   Time 0.138 (0.139)   Data 0.104 (0.106)   Loss 0.3282 (0.3471)   Prec@1 87.500 (85.743)   Prec@5 87.500 (85.743)   [2018-03-31 22:20:04]
  Epoch: [075][400/1475]   Time 0.141 (0.139)   Data 0.109 (0.106)   Loss 0.4239 (0.3541)   Prec@1 75.000 (85.310)   Prec@5 75.000 (85.310)   [2018-03-31 22:20:31]
  Epoch: [075][600/1475]   Time 0.141 (0.139)   Data 0.109 (0.106)   Loss 0.2487 (0.3598)   Prec@1 93.750 (85.087)   Prec@5 93.750 (85.087)   [2018-03-31 22:20:59]
  Epoch: [075][800/1475]   Time 0.125 (0.139)   Data 0.094 (0.105)   Loss 0.2717 (0.3596)   Prec@1 87.500 (85.140)   Prec@5 87.500 (85.140)   [2018-03-31 22:21:27]
  Epoch: [075][1000/1475]   Time 0.141 (0.139)   Data 0.094 (0.105)   Loss 0.3259 (0.3605)   Prec@1 87.500 (84.971)   Prec@5 87.500 (84.971)   [2018-03-31 22:21:55]
  Epoch: [075][1200/1475]   Time 0.141 (0.139)   Data 0.109 (0.105)   Loss 0.2758 (0.3620)   Prec@1 87.500 (84.815)   Prec@5 87.500 (84.815)   [2018-03-31 22:22:22]
  Epoch: [075][1400/1475]   Time 0.125 (0.139)   Data 0.094 (0.106)   Loss 0.4013 (0.3619)   Prec@1 84.375 (84.852)   Prec@5 84.375 (84.852)   [2018-03-31 22:22:50]
  **Train** Prec@1 84.773 Prec@5 84.773 Error@1 15.227
  **VAL** Prec@1 87.449 Prec@5 87.449 Error@1 12.551

==>>[2018-03-31 22:23:18] [Epoch=076/250] [Need: 10:51:28] [learning_rate=0.0020] [Best : Accuracy=87.45, Error=12.55]

==>>Epoch=[076/250]], [2018-03-31 22:23:18], LR=[0.002], Batch=[32] [Model=SimpleNet]
  Epoch: [076][000/1475]   Time 0.141 (0.141)   Data 0.109 (0.109)   Loss 0.3749 (0.3749)   Prec@1 84.375 (84.375)   Prec@5 84.375 (84.375)   [2018-03-31 22:23:18]
  Epoch: [076][200/1475]   Time 0.141 (0.139)   Data 0.109 (0.106)   Loss 0.4296 (0.3589)   Prec@1 84.375 (84.717)   Prec@5 84.375 (84.717)   [2018-03-31 22:23:46]
  Epoch: [076][400/1475]   Time 0.141 (0.139)   Data 0.109 (0.105)   Loss 0.4091 (0.3567)   Prec@1 81.250 (84.944)   Prec@5 81.250 (84.944)   [2018-03-31 22:24:14]
  Epoch: [076][600/1475]   Time 0.138 (0.139)   Data 0.094 (0.105)   Loss 0.3758 (0.3606)   Prec@1 81.250 (84.703)   Prec@5 81.250 (84.703)   [2018-03-31 22:24:41]
  Epoch: [076][800/1475]   Time 0.143 (0.139)   Data 0.112 (0.105)   Loss 0.4000 (0.3652)   Prec@1 78.125 (84.387)   Prec@5 78.125 (84.387)   [2018-03-31 22:25:09]
  Epoch: [076][1000/1475]   Time 0.141 (0.139)   Data 0.109 (0.105)   Loss 0.4953 (0.3638)   Prec@1 81.250 (84.491)   Prec@5 81.250 (84.491)   [2018-03-31 22:25:37]
  Epoch: [076][1200/1475]   Time 0.141 (0.139)   Data 0.094 (0.105)   Loss 0.3473 (0.3644)   Prec@1 84.375 (84.510)   Prec@5 84.375 (84.510)   [2018-03-31 22:26:05]
  Epoch: [076][1400/1475]   Time 0.141 (0.139)   Data 0.109 (0.105)   Loss 0.3441 (0.3654)   Prec@1 84.375 (84.424)   Prec@5 84.375 (84.424)   [2018-03-31 22:26:32]
  **Train** Prec@1 84.480 Prec@5 84.480 Error@1 15.520
  **VAL** Prec@1 87.197 Prec@5 87.197 Error@1 12.803

==>>[2018-03-31 22:27:00] [Epoch=077/250] [Need: 10:47:38] [learning_rate=0.0020] [Best : Accuracy=87.45, Error=12.55]

==>>Epoch=[077/250]], [2018-03-31 22:27:00], LR=[0.002], Batch=[32] [Model=SimpleNet]
  Epoch: [077][000/1475]   Time 0.141 (0.141)   Data 0.094 (0.094)   Loss 0.4250 (0.4250)   Prec@1 84.375 (84.375)   Prec@5 84.375 (84.375)   [2018-03-31 22:27:01]
  Epoch: [077][200/1475]   Time 0.144 (0.139)   Data 0.109 (0.106)   Loss 0.3891 (0.3696)   Prec@1 87.500 (84.049)   Prec@5 87.500 (84.049)   [2018-03-31 22:27:28]
  Epoch: [077][400/1475]   Time 0.133 (0.139)   Data 0.097 (0.105)   Loss 0.3157 (0.3644)   Prec@1 93.750 (84.383)   Prec@5 93.750 (84.383)   [2018-03-31 22:27:56]
  Epoch: [077][600/1475]   Time 0.141 (0.139)   Data 0.094 (0.105)   Loss 0.4153 (0.3642)   Prec@1 87.500 (84.547)   Prec@5 87.500 (84.547)   [2018-03-31 22:28:24]
  Epoch: [077][800/1475]   Time 0.141 (0.139)   Data 0.109 (0.105)   Loss 0.4479 (0.3634)   Prec@1 75.000 (84.570)   Prec@5 75.000 (84.570)   [2018-03-31 22:28:52]
  Epoch: [077][1000/1475]   Time 0.141 (0.139)   Data 0.109 (0.105)   Loss 0.3187 (0.3646)   Prec@1 90.625 (84.453)   Prec@5 90.625 (84.453)   [2018-03-31 22:29:19]
  Epoch: [077][1200/1475]   Time 0.125 (0.139)   Data 0.094 (0.105)   Loss 0.3446 (0.3636)   Prec@1 81.250 (84.489)   Prec@5 81.250 (84.489)   [2018-03-31 22:29:47]
  Epoch: [077][1400/1475]   Time 0.141 (0.139)   Data 0.109 (0.105)   Loss 0.6079 (0.3647)   Prec@1 68.750 (84.406)   Prec@5 68.750 (84.406)   [2018-03-31 22:30:15]
  **Train** Prec@1 84.393 Prec@5 84.393 Error@1 15.607
  **VAL** Prec@1 86.644 Prec@5 86.644 Error@1 13.356

==>>[2018-03-31 22:30:43] [Epoch=078/250] [Need: 10:43:49] [learning_rate=0.0020] [Best : Accuracy=87.45, Error=12.55]

==>>Epoch=[078/250]], [2018-03-31 22:30:43], LR=[0.002], Batch=[32] [Model=SimpleNet]
  Epoch: [078][000/1475]   Time 0.141 (0.141)   Data 0.109 (0.109)   Loss 0.3703 (0.3703)   Prec@1 84.375 (84.375)   Prec@5 84.375 (84.375)   [2018-03-31 22:30:43]
  Epoch: [078][200/1475]   Time 0.149 (0.139)   Data 0.109 (0.105)   Loss 0.3225 (0.3549)   Prec@1 90.625 (85.432)   Prec@5 90.625 (85.432)   [2018-03-31 22:31:11]
  Epoch: [078][400/1475]   Time 0.141 (0.139)   Data 0.109 (0.105)   Loss 0.3931 (0.3576)   Prec@1 78.125 (85.053)   Prec@5 78.125 (85.053)   [2018-03-31 22:31:38]
  Epoch: [078][600/1475]   Time 0.145 (0.139)   Data 0.114 (0.105)   Loss 0.2534 (0.3595)   Prec@1 93.750 (84.895)   Prec@5 93.750 (84.895)   [2018-03-31 22:32:06]
  Epoch: [078][800/1475]   Time 0.141 (0.139)   Data 0.109 (0.105)   Loss 0.2473 (0.3614)   Prec@1 93.750 (84.664)   Prec@5 93.750 (84.664)   [2018-03-31 22:32:34]
  Epoch: [078][1000/1475]   Time 0.141 (0.139)   Data 0.109 (0.105)   Loss 0.3265 (0.3636)   Prec@1 84.375 (84.531)   Prec@5 84.375 (84.531)   [2018-03-31 22:33:02]
  Epoch: [078][1200/1475]   Time 0.141 (0.139)   Data 0.109 (0.105)   Loss 0.4115 (0.3664)   Prec@1 81.250 (84.385)   Prec@5 81.250 (84.385)   [2018-03-31 22:33:30]
  Epoch: [078][1400/1475]   Time 0.141 (0.139)   Data 0.094 (0.105)   Loss 0.2176 (0.3661)   Prec@1 93.750 (84.413)   Prec@5 93.750 (84.413)   [2018-03-31 22:33:57]
  **Train** Prec@1 84.430 Prec@5 84.430 Error@1 15.570
  **VAL** Prec@1 85.743 Prec@5 85.743 Error@1 14.257

==>>[2018-03-31 22:34:25] [Epoch=079/250] [Need: 10:40:00] [learning_rate=0.0020] [Best : Accuracy=87.45, Error=12.55]

==>>Epoch=[079/250]], [2018-03-31 22:34:25], LR=[0.002], Batch=[32] [Model=SimpleNet]
  Epoch: [079][000/1475]   Time 0.141 (0.141)   Data 0.094 (0.094)   Loss 0.4014 (0.4014)   Prec@1 81.250 (81.250)   Prec@5 81.250 (81.250)   [2018-03-31 22:34:26]
  Epoch: [079][200/1475]   Time 0.144 (0.139)   Data 0.113 (0.105)   Loss 0.3608 (0.3596)   Prec@1 90.625 (84.297)   Prec@5 90.625 (84.297)   [2018-03-31 22:34:53]
  Epoch: [079][400/1475]   Time 0.132 (0.139)   Data 0.111 (0.106)   Loss 0.2759 (0.3567)   Prec@1 87.500 (84.632)   Prec@5 87.500 (84.632)   [2018-03-31 22:35:21]
  Epoch: [079][600/1475]   Time 0.141 (0.139)   Data 0.109 (0.106)   Loss 0.2109 (0.3616)   Prec@1 87.500 (84.370)   Prec@5 87.500 (84.370)   [2018-03-31 22:35:49]
  Epoch: [079][800/1475]   Time 0.127 (0.139)   Data 0.096 (0.106)   Loss 0.2785 (0.3642)   Prec@1 90.625 (84.313)   Prec@5 90.625 (84.313)   [2018-03-31 22:36:17]
  Epoch: [079][1000/1475]   Time 0.141 (0.139)   Data 0.109 (0.106)   Loss 0.2205 (0.3666)   Prec@1 96.875 (84.256)   Prec@5 96.875 (84.256)   [2018-03-31 22:36:44]
  Epoch: [079][1200/1475]   Time 0.141 (0.139)   Data 0.109 (0.106)   Loss 0.2133 (0.3656)   Prec@1 87.500 (84.326)   Prec@5 87.500 (84.326)   [2018-03-31 22:37:12]
  Epoch: [079][1400/1475]   Time 0.141 (0.139)   Data 0.109 (0.106)   Loss 0.4248 (0.3671)   Prec@1 81.250 (84.268)   Prec@5 81.250 (84.268)   [2018-03-31 22:37:40]
  **Train** Prec@1 84.362 Prec@5 84.362 Error@1 15.638
  **VAL** Prec@1 85.551 Prec@5 85.551 Error@1 14.449

==>>[2018-03-31 22:38:08] [Epoch=080/250] [Need: 10:36:11] [learning_rate=0.0020] [Best : Accuracy=87.45, Error=12.55]

==>>Epoch=[080/250]], [2018-03-31 22:38:08], LR=[0.002], Batch=[32] [Model=SimpleNet]
  Epoch: [080][000/1475]   Time 0.128 (0.128)   Data 0.097 (0.097)   Loss 0.5498 (0.5498)   Prec@1 78.125 (78.125)   Prec@5 78.125 (78.125)   [2018-03-31 22:38:08]
  Epoch: [080][200/1475]   Time 0.125 (0.139)   Data 0.094 (0.106)   Loss 0.5513 (0.3725)   Prec@1 81.250 (84.142)   Prec@5 81.250 (84.142)   [2018-03-31 22:38:36]
  Epoch: [080][400/1475]   Time 0.141 (0.139)   Data 0.094 (0.106)   Loss 0.4062 (0.3643)   Prec@1 84.375 (84.648)   Prec@5 84.375 (84.648)   [2018-03-31 22:39:04]
  Epoch: [080][600/1475]   Time 0.131 (0.139)   Data 0.100 (0.105)   Loss 0.2723 (0.3648)   Prec@1 93.750 (84.625)   Prec@5 93.750 (84.625)   [2018-03-31 22:39:31]
  Epoch: [080][800/1475]   Time 0.141 (0.139)   Data 0.094 (0.106)   Loss 0.5373 (0.3664)   Prec@1 78.125 (84.449)   Prec@5 78.125 (84.449)   [2018-03-31 22:39:59]
  Epoch: [080][1000/1475]   Time 0.141 (0.139)   Data 0.109 (0.106)   Loss 0.5371 (0.3658)   Prec@1 75.000 (84.500)   Prec@5 75.000 (84.500)   [2018-03-31 22:40:27]
  Epoch: [080][1200/1475]   Time 0.149 (0.139)   Data 0.117 (0.106)   Loss 0.3043 (0.3649)   Prec@1 84.375 (84.562)   Prec@5 84.375 (84.562)   [2018-03-31 22:40:55]
  Epoch: [080][1400/1475]   Time 0.145 (0.139)   Data 0.114 (0.106)   Loss 0.2577 (0.3645)   Prec@1 87.500 (84.603)   Prec@5 87.500 (84.603)   [2018-03-31 22:41:22]
  **Train** Prec@1 84.593 Prec@5 84.593 Error@1 15.407
  **VAL** Prec@1 87.089 Prec@5 87.089 Error@1 12.911

==>>[2018-03-31 22:41:50] [Epoch=081/250] [Need: 10:32:22] [learning_rate=0.0020] [Best : Accuracy=87.45, Error=12.55]

==>>Epoch=[081/250]], [2018-03-31 22:41:50], LR=[0.002], Batch=[32] [Model=SimpleNet]
  Epoch: [081][000/1475]   Time 0.126 (0.126)   Data 0.095 (0.095)   Loss 0.3895 (0.3895)   Prec@1 90.625 (90.625)   Prec@5 90.625 (90.625)   [2018-03-31 22:41:51]
  Epoch: [081][200/1475]   Time 0.129 (0.139)   Data 0.097 (0.106)   Loss 0.3292 (0.3654)   Prec@1 90.625 (84.624)   Prec@5 90.625 (84.624)   [2018-03-31 22:42:18]
  Epoch: [081][400/1475]   Time 0.145 (0.139)   Data 0.109 (0.106)   Loss 0.5032 (0.3695)   Prec@1 81.250 (84.406)   Prec@5 81.250 (84.406)   [2018-03-31 22:42:46]
  Epoch: [081][600/1475]   Time 0.144 (0.139)   Data 0.113 (0.105)   Loss 0.4402 (0.3643)   Prec@1 78.125 (84.718)   Prec@5 78.125 (84.718)   [2018-03-31 22:43:14]
  Epoch: [081][800/1475]   Time 0.141 (0.139)   Data 0.109 (0.106)   Loss 0.3486 (0.3622)   Prec@1 87.500 (84.726)   Prec@5 87.500 (84.726)   [2018-03-31 22:43:42]
  Epoch: [081][1000/1475]   Time 0.141 (0.139)   Data 0.109 (0.106)   Loss 0.4632 (0.3650)   Prec@1 84.375 (84.565)   Prec@5 84.375 (84.565)   [2018-03-31 22:44:09]
  Epoch: [081][1200/1475]   Time 0.136 (0.139)   Data 0.105 (0.105)   Loss 0.3437 (0.3644)   Prec@1 81.250 (84.547)   Prec@5 81.250 (84.547)   [2018-03-31 22:44:37]
  Epoch: [081][1400/1475]   Time 0.141 (0.139)   Data 0.109 (0.105)   Loss 0.2959 (0.3647)   Prec@1 87.500 (84.551)   Prec@5 87.500 (84.551)   [2018-03-31 22:45:05]
  **Train** Prec@1 84.525 Prec@5 84.525 Error@1 15.475
  **VAL** Prec@1 87.317 Prec@5 87.317 Error@1 12.683

==>>[2018-03-31 22:45:33] [Epoch=082/250] [Need: 10:28:34] [learning_rate=0.0020] [Best : Accuracy=87.45, Error=12.55]

==>>Epoch=[082/250]], [2018-03-31 22:45:33], LR=[0.002], Batch=[32] [Model=SimpleNet]
  Epoch: [082][000/1475]   Time 0.148 (0.148)   Data 0.117 (0.117)   Loss 0.2597 (0.2597)   Prec@1 90.625 (90.625)   Prec@5 90.625 (90.625)   [2018-03-31 22:45:33]
  Epoch: [082][200/1475]   Time 0.141 (0.139)   Data 0.109 (0.106)   Loss 0.4784 (0.3598)   Prec@1 93.750 (85.075)   Prec@5 93.750 (85.075)   [2018-03-31 22:46:01]
  Epoch: [082][400/1475]   Time 0.141 (0.139)   Data 0.109 (0.106)   Loss 0.2196 (0.3645)   Prec@1 90.625 (84.609)   Prec@5 90.625 (84.609)   [2018-03-31 22:46:29]
  Epoch: [082][600/1475]   Time 0.141 (0.139)   Data 0.109 (0.106)   Loss 0.2385 (0.3658)   Prec@1 93.750 (84.505)   Prec@5 93.750 (84.505)   [2018-03-31 22:46:57]
  Epoch: [082][800/1475]   Time 0.141 (0.139)   Data 0.094 (0.106)   Loss 0.4201 (0.3615)   Prec@1 84.375 (84.652)   Prec@5 84.375 (84.652)   [2018-03-31 22:47:24]
  Epoch: [082][1000/1475]   Time 0.141 (0.139)   Data 0.109 (0.106)   Loss 0.4441 (0.3590)   Prec@1 78.125 (84.793)   Prec@5 78.125 (84.793)   [2018-03-31 22:47:52]
  Epoch: [082][1200/1475]   Time 0.125 (0.139)   Data 0.094 (0.106)   Loss 0.4033 (0.3601)   Prec@1 78.125 (84.773)   Prec@5 78.125 (84.773)   [2018-03-31 22:48:20]
  Epoch: [082][1400/1475]   Time 0.141 (0.139)   Data 0.109 (0.106)   Loss 0.2458 (0.3628)   Prec@1 90.625 (84.567)   Prec@5 90.625 (84.567)   [2018-03-31 22:48:48]
  **Train** Prec@1 84.550 Prec@5 84.550 Error@1 15.450
  **VAL** Prec@1 86.896 Prec@5 86.896 Error@1 13.104

==>>[2018-03-31 22:49:16] [Epoch=083/250] [Need: 10:24:46] [learning_rate=0.0020] [Best : Accuracy=87.45, Error=12.55]

==>>Epoch=[083/250]], [2018-03-31 22:49:16], LR=[0.002], Batch=[32] [Model=SimpleNet]
  Epoch: [083][000/1475]   Time 0.128 (0.128)   Data 0.107 (0.107)   Loss 0.3604 (0.3604)   Prec@1 81.250 (81.250)   Prec@5 81.250 (81.250)   [2018-03-31 22:49:16]
  Epoch: [083][200/1475]   Time 0.141 (0.139)   Data 0.109 (0.106)   Loss 0.3857 (0.3696)   Prec@1 81.250 (83.831)   Prec@5 81.250 (83.831)   [2018-03-31 22:49:44]
  Epoch: [083][400/1475]   Time 0.141 (0.139)   Data 0.109 (0.105)   Loss 0.2998 (0.3654)   Prec@1 87.500 (84.344)   Prec@5 87.500 (84.344)   [2018-03-31 22:50:11]
  Epoch: [083][600/1475]   Time 0.141 (0.138)   Data 0.094 (0.105)   Loss 0.4288 (0.3622)   Prec@1 81.250 (84.552)   Prec@5 81.250 (84.552)   [2018-03-31 22:50:39]
  Epoch: [083][800/1475]   Time 0.141 (0.139)   Data 0.109 (0.105)   Loss 0.2095 (0.3603)   Prec@1 93.750 (84.605)   Prec@5 93.750 (84.605)   [2018-03-31 22:51:07]
  Epoch: [083][1000/1475]   Time 0.144 (0.139)   Data 0.109 (0.105)   Loss 0.3900 (0.3636)   Prec@1 75.000 (84.416)   Prec@5 75.000 (84.416)   [2018-03-31 22:51:34]
  Epoch: [083][1200/1475]   Time 0.128 (0.138)   Data 0.097 (0.105)   Loss 0.2589 (0.3638)   Prec@1 84.375 (84.372)   Prec@5 84.375 (84.372)   [2018-03-31 22:52:02]
  Epoch: [083][1400/1475]   Time 0.141 (0.139)   Data 0.109 (0.105)   Loss 0.3887 (0.3638)   Prec@1 75.000 (84.355)   Prec@5 75.000 (84.355)   [2018-03-31 22:52:30]
  **Train** Prec@1 84.381 Prec@5 84.381 Error@1 15.619
  **VAL** Prec@1 86.728 Prec@5 86.728 Error@1 13.272

==>>[2018-03-31 22:52:58] [Epoch=084/250] [Need: 10:20:57] [learning_rate=0.0020] [Best : Accuracy=87.45, Error=12.55]

==>>Epoch=[084/250]], [2018-03-31 22:52:58], LR=[0.002], Batch=[32] [Model=SimpleNet]
  Epoch: [084][000/1475]   Time 0.137 (0.137)   Data 0.106 (0.106)   Loss 0.4977 (0.4977)   Prec@1 78.125 (78.125)   Prec@5 78.125 (78.125)   [2018-03-31 22:52:58]
  Epoch: [084][200/1475]   Time 0.141 (0.139)   Data 0.109 (0.105)   Loss 0.5364 (0.3742)   Prec@1 71.875 (84.142)   Prec@5 71.875 (84.142)   [2018-03-31 22:53:26]
  Epoch: [084][400/1475]   Time 0.141 (0.139)   Data 0.109 (0.105)   Loss 0.5307 (0.3691)   Prec@1 68.750 (84.188)   Prec@5 68.750 (84.188)   [2018-03-31 22:53:54]
  Epoch: [084][600/1475]   Time 0.141 (0.139)   Data 0.109 (0.105)   Loss 0.5232 (0.3671)   Prec@1 75.000 (84.240)   Prec@5 75.000 (84.240)   [2018-03-31 22:54:21]
  Epoch: [084][800/1475]   Time 0.141 (0.139)   Data 0.109 (0.105)   Loss 0.2725 (0.3659)   Prec@1 84.375 (84.301)   Prec@5 84.375 (84.301)   [2018-03-31 22:54:49]
  Epoch: [084][1000/1475]   Time 0.141 (0.139)   Data 0.109 (0.105)   Loss 0.3691 (0.3649)   Prec@1 81.250 (84.316)   Prec@5 81.250 (84.316)   [2018-03-31 22:55:17]
  Epoch: [084][1200/1475]   Time 0.141 (0.139)   Data 0.109 (0.105)   Loss 0.4923 (0.3639)   Prec@1 81.250 (84.320)   Prec@5 81.250 (84.320)   [2018-03-31 22:55:45]
  Epoch: [084][1400/1475]   Time 0.134 (0.139)   Data 0.103 (0.105)   Loss 0.1867 (0.3624)   Prec@1 96.875 (84.442)   Prec@5 96.875 (84.442)   [2018-03-31 22:56:12]
  **Train** Prec@1 84.446 Prec@5 84.446 Error@1 15.554
  **VAL** Prec@1 87.161 Prec@5 87.161 Error@1 12.839

==>>[2018-03-31 22:56:40] [Epoch=085/250] [Need: 10:17:08] [learning_rate=0.0020] [Best : Accuracy=87.45, Error=12.55]

==>>Epoch=[085/250]], [2018-03-31 22:56:40], LR=[0.002], Batch=[32] [Model=SimpleNet]
  Epoch: [085][000/1475]   Time 0.141 (0.141)   Data 0.094 (0.094)   Loss 0.3349 (0.3349)   Prec@1 84.375 (84.375)   Prec@5 84.375 (84.375)   [2018-03-31 22:56:41]
  Epoch: [085][200/1475]   Time 0.141 (0.138)   Data 0.109 (0.106)   Loss 0.3945 (0.3622)   Prec@1 81.250 (84.670)   Prec@5 81.250 (84.670)   [2018-03-31 22:57:08]
  Epoch: [085][400/1475]   Time 0.141 (0.139)   Data 0.109 (0.106)   Loss 0.2526 (0.3650)   Prec@1 90.625 (84.546)   Prec@5 90.625 (84.546)   [2018-03-31 22:57:36]
  Epoch: [085][600/1475]   Time 0.141 (0.139)   Data 0.109 (0.106)   Loss 0.3119 (0.3644)   Prec@1 84.375 (84.604)   Prec@5 84.375 (84.604)   [2018-03-31 22:58:04]
  Epoch: [085][800/1475]   Time 0.141 (0.139)   Data 0.109 (0.105)   Loss 0.3427 (0.3607)   Prec@1 81.250 (84.687)   Prec@5 81.250 (84.687)   [2018-03-31 22:58:31]
  Epoch: [085][1000/1475]   Time 0.147 (0.139)   Data 0.109 (0.105)   Loss 0.3057 (0.3600)   Prec@1 90.625 (84.703)   Prec@5 90.625 (84.703)   [2018-03-31 22:58:59]
  Epoch: [085][1200/1475]   Time 0.147 (0.139)   Data 0.116 (0.105)   Loss 0.4967 (0.3618)   Prec@1 78.125 (84.583)   Prec@5 78.125 (84.583)   [2018-03-31 22:59:27]
  Epoch: [085][1400/1475]   Time 0.141 (0.139)   Data 0.109 (0.105)   Loss 0.3926 (0.3614)   Prec@1 84.375 (84.591)   Prec@5 84.375 (84.591)   [2018-03-31 22:59:54]
  **Train** Prec@1 84.555 Prec@5 84.555 Error@1 15.445
  **VAL** Prec@1 86.800 Prec@5 86.800 Error@1 13.200

==>>[2018-03-31 23:00:23] [Epoch=086/250] [Need: 10:13:20] [learning_rate=0.0020] [Best : Accuracy=87.45, Error=12.55]

==>>Epoch=[086/250]], [2018-03-31 23:00:23], LR=[0.002], Batch=[32] [Model=SimpleNet]
  Epoch: [086][000/1475]   Time 0.141 (0.141)   Data 0.094 (0.094)   Loss 0.2081 (0.2081)   Prec@1 93.750 (93.750)   Prec@5 93.750 (93.750)   [2018-03-31 23:00:23]
  Epoch: [086][200/1475]   Time 0.141 (0.139)   Data 0.109 (0.105)   Loss 0.4317 (0.3654)   Prec@1 78.125 (84.375)   Prec@5 78.125 (84.375)   [2018-03-31 23:00:51]
  Epoch: [086][400/1475]   Time 0.147 (0.139)   Data 0.116 (0.105)   Loss 0.4055 (0.3659)   Prec@1 78.125 (84.476)   Prec@5 78.125 (84.476)   [2018-03-31 23:01:18]
  Epoch: [086][600/1475]   Time 0.141 (0.139)   Data 0.109 (0.105)   Loss 0.3558 (0.3664)   Prec@1 87.500 (84.469)   Prec@5 87.500 (84.469)   [2018-03-31 23:01:46]
  Epoch: [086][800/1475]   Time 0.141 (0.139)   Data 0.109 (0.105)   Loss 0.1790 (0.3658)   Prec@1 90.625 (84.461)   Prec@5 90.625 (84.461)   [2018-03-31 23:02:14]
  Epoch: [086][1000/1475]   Time 0.141 (0.139)   Data 0.109 (0.105)   Loss 0.4285 (0.3674)   Prec@1 71.875 (84.403)   Prec@5 71.875 (84.403)   [2018-03-31 23:02:41]
  Epoch: [086][1200/1475]   Time 0.148 (0.139)   Data 0.101 (0.105)   Loss 0.3256 (0.3663)   Prec@1 90.625 (84.497)   Prec@5 90.625 (84.497)   [2018-03-31 23:03:09]
  Epoch: [086][1400/1475]   Time 0.141 (0.139)   Data 0.109 (0.105)   Loss 0.2653 (0.3646)   Prec@1 84.375 (84.538)   Prec@5 84.375 (84.538)   [2018-03-31 23:03:37]
  **Train** Prec@1 84.533 Prec@5 84.533 Error@1 15.467
  **VAL** Prec@1 86.644 Prec@5 86.644 Error@1 13.356

==>>[2018-03-31 23:04:05] [Epoch=087/250] [Need: 10:09:32] [learning_rate=0.0020] [Best : Accuracy=87.45, Error=12.55]

==>>Epoch=[087/250]], [2018-03-31 23:04:05], LR=[0.002], Batch=[32] [Model=SimpleNet]
  Epoch: [087][000/1475]   Time 0.124 (0.124)   Data 0.093 (0.093)   Loss 0.4274 (0.4274)   Prec@1 81.250 (81.250)   Prec@5 81.250 (81.250)   [2018-03-31 23:04:05]
  Epoch: [087][200/1475]   Time 0.146 (0.139)   Data 0.109 (0.106)   Loss 0.2709 (0.3628)   Prec@1 93.750 (84.313)   Prec@5 93.750 (84.313)   [2018-03-31 23:04:33]
  Epoch: [087][400/1475]   Time 0.144 (0.139)   Data 0.113 (0.105)   Loss 0.5466 (0.3651)   Prec@1 78.125 (84.313)   Prec@5 78.125 (84.313)   [2018-03-31 23:05:01]
  Epoch: [087][600/1475]   Time 0.141 (0.139)   Data 0.109 (0.105)   Loss 0.3634 (0.3659)   Prec@1 84.375 (84.203)   Prec@5 84.375 (84.203)   [2018-03-31 23:05:28]
  Epoch: [087][800/1475]   Time 0.145 (0.139)   Data 0.114 (0.105)   Loss 0.3500 (0.3685)   Prec@1 78.125 (84.125)   Prec@5 78.125 (84.125)   [2018-03-31 23:05:56]
  Epoch: [087][1000/1475]   Time 0.144 (0.139)   Data 0.109 (0.105)   Loss 0.2134 (0.3657)   Prec@1 90.625 (84.391)   Prec@5 90.625 (84.391)   [2018-03-31 23:06:24]
  Epoch: [087][1200/1475]   Time 0.125 (0.139)   Data 0.094 (0.105)   Loss 0.2931 (0.3649)   Prec@1 90.625 (84.469)   Prec@5 90.625 (84.469)   [2018-03-31 23:06:52]
  Epoch: [087][1400/1475]   Time 0.141 (0.139)   Data 0.109 (0.105)   Loss 0.4211 (0.3647)   Prec@1 84.375 (84.437)   Prec@5 84.375 (84.437)   [2018-03-31 23:07:19]
  **Train** Prec@1 84.404 Prec@5 84.404 Error@1 15.596
  **VAL** Prec@1 86.981 Prec@5 86.981 Error@1 13.019

==>>[2018-03-31 23:07:47] [Epoch=088/250] [Need: 10:05:44] [learning_rate=0.0020] [Best : Accuracy=87.45, Error=12.55]

==>>Epoch=[088/250]], [2018-03-31 23:07:47], LR=[0.002], Batch=[32] [Model=SimpleNet]
  Epoch: [088][000/1475]   Time 0.125 (0.125)   Data 0.094 (0.094)   Loss 0.5284 (0.5284)   Prec@1 71.875 (71.875)   Prec@5 71.875 (71.875)   [2018-03-31 23:07:48]
  Epoch: [088][200/1475]   Time 0.141 (0.139)   Data 0.109 (0.105)   Loss 0.2546 (0.3695)   Prec@1 93.750 (84.515)   Prec@5 93.750 (84.515)   [2018-03-31 23:08:15]
  Epoch: [088][400/1475]   Time 0.141 (0.139)   Data 0.109 (0.105)   Loss 0.5365 (0.3683)   Prec@1 78.125 (84.320)   Prec@5 78.125 (84.320)   [2018-03-31 23:08:43]
  Epoch: [088][600/1475]   Time 0.141 (0.139)   Data 0.109 (0.105)   Loss 0.3075 (0.3621)   Prec@1 87.500 (84.630)   Prec@5 87.500 (84.630)   [2018-03-31 23:09:11]
  Epoch: [088][800/1475]   Time 0.125 (0.139)   Data 0.094 (0.105)   Loss 0.4033 (0.3625)   Prec@1 78.125 (84.625)   Prec@5 78.125 (84.625)   [2018-03-31 23:09:39]
  Epoch: [088][1000/1475]   Time 0.129 (0.139)   Data 0.098 (0.105)   Loss 0.3749 (0.3643)   Prec@1 87.500 (84.453)   Prec@5 87.500 (84.453)   [2018-03-31 23:10:06]
  Epoch: [088][1200/1475]   Time 0.141 (0.139)   Data 0.109 (0.105)   Loss 0.3444 (0.3649)   Prec@1 84.375 (84.417)   Prec@5 84.375 (84.417)   [2018-03-31 23:10:34]
  Epoch: [088][1400/1475]   Time 0.135 (0.139)   Data 0.104 (0.105)   Loss 0.3251 (0.3644)   Prec@1 84.375 (84.391)   Prec@5 84.375 (84.391)   [2018-03-31 23:11:02]
  **Train** Prec@1 84.357 Prec@5 84.357 Error@1 15.643
  **VAL** Prec@1 87.185 Prec@5 87.185 Error@1 12.815

==>>[2018-03-31 23:11:30] [Epoch=089/250] [Need: 10:01:56] [learning_rate=0.0020] [Best : Accuracy=87.45, Error=12.55]

==>>Epoch=[089/250]], [2018-03-31 23:11:30], LR=[0.002], Batch=[32] [Model=SimpleNet]
  Epoch: [089][000/1475]   Time 0.141 (0.141)   Data 0.109 (0.109)   Loss 0.3571 (0.3571)   Prec@1 81.250 (81.250)   Prec@5 81.250 (81.250)   [2018-03-31 23:11:30]
  Epoch: [089][200/1475]   Time 0.141 (0.139)   Data 0.109 (0.106)   Loss 0.3202 (0.3765)   Prec@1 87.500 (83.364)   Prec@5 87.500 (83.364)   [2018-03-31 23:11:58]
  Epoch: [089][400/1475]   Time 0.141 (0.139)   Data 0.109 (0.106)   Loss 0.2107 (0.3677)   Prec@1 90.625 (83.868)   Prec@5 90.625 (83.868)   [2018-03-31 23:12:25]
  Epoch: [089][600/1475]   Time 0.141 (0.139)   Data 0.094 (0.105)   Loss 0.4905 (0.3666)   Prec@1 75.000 (84.042)   Prec@5 75.000 (84.042)   [2018-03-31 23:12:53]
  Epoch: [089][800/1475]   Time 0.141 (0.139)   Data 0.109 (0.105)   Loss 0.3708 (0.3653)   Prec@1 84.375 (84.157)   Prec@5 84.375 (84.157)   [2018-03-31 23:13:21]
  Epoch: [089][1000/1475]   Time 0.141 (0.139)   Data 0.094 (0.105)   Loss 0.4067 (0.3638)   Prec@1 81.250 (84.328)   Prec@5 81.250 (84.328)   [2018-03-31 23:13:49]
  Epoch: [089][1200/1475]   Time 0.127 (0.139)   Data 0.096 (0.105)   Loss 0.3470 (0.3646)   Prec@1 84.375 (84.297)   Prec@5 84.375 (84.297)   [2018-03-31 23:14:16]
  Epoch: [089][1400/1475]   Time 0.141 (0.139)   Data 0.109 (0.105)   Loss 0.3829 (0.3632)   Prec@1 87.500 (84.377)   Prec@5 87.500 (84.377)   [2018-03-31 23:14:44]
  **Train** Prec@1 84.391 Prec@5 84.391 Error@1 15.609
  **VAL** Prec@1 87.665 Prec@5 87.665 Error@1 12.335

==>>[2018-03-31 23:15:12] [Epoch=090/250] [Need: 09:58:08] [learning_rate=0.0020] [Best : Accuracy=87.67, Error=12.33]

==>>Epoch=[090/250]], [2018-03-31 23:15:12], LR=[0.002], Batch=[32] [Model=SimpleNet]
  Epoch: [090][000/1475]   Time 0.138 (0.138)   Data 0.103 (0.103)   Loss 0.4471 (0.4471)   Prec@1 75.000 (75.000)   Prec@5 75.000 (75.000)   [2018-03-31 23:15:12]
  Epoch: [090][200/1475]   Time 0.141 (0.139)   Data 0.109 (0.105)   Loss 0.5466 (0.3624)   Prec@1 81.250 (84.670)   Prec@5 81.250 (84.670)   [2018-03-31 23:15:40]
  Epoch: [090][400/1475]   Time 0.141 (0.139)   Data 0.109 (0.105)   Loss 0.4156 (0.3640)   Prec@1 75.000 (84.671)   Prec@5 75.000 (84.671)   [2018-03-31 23:16:08]
  Epoch: [090][600/1475]   Time 0.141 (0.139)   Data 0.109 (0.105)   Loss 0.6743 (0.3645)   Prec@1 81.250 (84.541)   Prec@5 81.250 (84.541)   [2018-03-31 23:16:35]
  Epoch: [090][800/1475]   Time 0.141 (0.139)   Data 0.109 (0.105)   Loss 0.4335 (0.3627)   Prec@1 84.375 (84.664)   Prec@5 84.375 (84.664)   [2018-03-31 23:17:03]
  Epoch: [090][1000/1475]   Time 0.141 (0.139)   Data 0.109 (0.105)   Loss 0.2756 (0.3624)   Prec@1 90.625 (84.684)   Prec@5 90.625 (84.684)   [2018-03-31 23:17:31]
  Epoch: [090][1200/1475]   Time 0.141 (0.139)   Data 0.109 (0.105)   Loss 0.2190 (0.3633)   Prec@1 90.625 (84.607)   Prec@5 90.625 (84.607)   [2018-03-31 23:17:59]
  Epoch: [090][1400/1475]   Time 0.125 (0.139)   Data 0.094 (0.105)   Loss 0.5120 (0.3616)   Prec@1 81.250 (84.672)   Prec@5 81.250 (84.672)   [2018-03-31 23:18:26]
  **Train** Prec@1 84.718 Prec@5 84.718 Error@1 15.282
  **VAL** Prec@1 87.269 Prec@5 87.269 Error@1 12.731

==>>[2018-03-31 23:18:54] [Epoch=091/250] [Need: 09:54:20] [learning_rate=0.0020] [Best : Accuracy=87.67, Error=12.33]

==>>Epoch=[091/250]], [2018-03-31 23:18:54], LR=[0.002], Batch=[32] [Model=SimpleNet]
  Epoch: [091][000/1475]   Time 0.141 (0.141)   Data 0.109 (0.109)   Loss 0.2909 (0.2909)   Prec@1 90.625 (90.625)   Prec@5 90.625 (90.625)   [2018-03-31 23:18:55]
  Epoch: [091][200/1475]   Time 0.141 (0.139)   Data 0.109 (0.105)   Loss 0.2782 (0.3525)   Prec@1 87.500 (84.919)   Prec@5 87.500 (84.919)   [2018-03-31 23:19:22]
  Epoch: [091][400/1475]   Time 0.142 (0.139)   Data 0.109 (0.105)   Loss 0.2563 (0.3607)   Prec@1 90.625 (84.531)   Prec@5 90.625 (84.531)   [2018-03-31 23:19:50]
  Epoch: [091][600/1475]   Time 0.141 (0.139)   Data 0.109 (0.105)   Loss 0.4228 (0.3585)   Prec@1 81.250 (84.749)   Prec@5 81.250 (84.749)   [2018-03-31 23:20:18]
  Epoch: [091][800/1475]   Time 0.141 (0.139)   Data 0.094 (0.105)   Loss 0.3386 (0.3626)   Prec@1 87.500 (84.492)   Prec@5 87.500 (84.492)   [2018-03-31 23:20:45]
  Epoch: [091][1000/1475]   Time 0.136 (0.139)   Data 0.105 (0.105)   Loss 0.4220 (0.3617)   Prec@1 81.250 (84.584)   Prec@5 81.250 (84.584)   [2018-03-31 23:21:13]
  Epoch: [091][1200/1475]   Time 0.141 (0.139)   Data 0.109 (0.105)   Loss 0.3083 (0.3604)   Prec@1 87.500 (84.604)   Prec@5 87.500 (84.604)   [2018-03-31 23:21:41]
  Epoch: [091][1400/1475]   Time 0.141 (0.139)   Data 0.109 (0.105)   Loss 0.4435 (0.3604)   Prec@1 81.250 (84.627)   Prec@5 81.250 (84.627)   [2018-03-31 23:22:09]
  **Train** Prec@1 84.656 Prec@5 84.656 Error@1 15.344
  **VAL** Prec@1 87.461 Prec@5 87.461 Error@1 12.539

==>>[2018-03-31 23:22:37] [Epoch=092/250] [Need: 09:50:33] [learning_rate=0.0020] [Best : Accuracy=87.67, Error=12.33]

==>>Epoch=[092/250]], [2018-03-31 23:22:37], LR=[0.002], Batch=[32] [Model=SimpleNet]
  Epoch: [092][000/1475]   Time 0.125 (0.125)   Data 0.094 (0.094)   Loss 0.1840 (0.1840)   Prec@1 93.750 (93.750)   Prec@5 93.750 (93.750)   [2018-03-31 23:22:37]
  Epoch: [092][200/1475]   Time 0.144 (0.139)   Data 0.117 (0.106)   Loss 0.3519 (0.3571)   Prec@1 84.375 (85.012)   Prec@5 84.375 (85.012)   [2018-03-31 23:23:05]
  Epoch: [092][400/1475]   Time 0.141 (0.139)   Data 0.109 (0.105)   Loss 0.2729 (0.3561)   Prec@1 90.625 (84.874)   Prec@5 90.625 (84.874)   [2018-03-31 23:23:32]
  Epoch: [092][600/1475]   Time 0.135 (0.139)   Data 0.104 (0.105)   Loss 0.2861 (0.3580)   Prec@1 90.625 (84.703)   Prec@5 90.625 (84.703)   [2018-03-31 23:24:00]
  Epoch: [092][800/1475]   Time 0.128 (0.139)   Data 0.094 (0.105)   Loss 0.4163 (0.3627)   Prec@1 84.375 (84.484)   Prec@5 84.375 (84.484)   [2018-03-31 23:24:28]
  Epoch: [092][1000/1475]   Time 0.141 (0.139)   Data 0.109 (0.105)   Loss 0.2458 (0.3611)   Prec@1 93.750 (84.643)   Prec@5 93.750 (84.643)   [2018-03-31 23:24:56]
  Epoch: [092][1200/1475]   Time 0.141 (0.139)   Data 0.109 (0.105)   Loss 0.4442 (0.3607)   Prec@1 81.250 (84.713)   Prec@5 81.250 (84.713)   [2018-03-31 23:25:23]
  Epoch: [092][1400/1475]   Time 0.141 (0.139)   Data 0.109 (0.105)   Loss 0.3306 (0.3606)   Prec@1 81.250 (84.656)   Prec@5 81.250 (84.656)   [2018-03-31 23:25:51]
  **Train** Prec@1 84.546 Prec@5 84.546 Error@1 15.454
  **VAL** Prec@1 87.149 Prec@5 87.149 Error@1 12.851

==>>[2018-03-31 23:26:19] [Epoch=093/250] [Need: 09:46:45] [learning_rate=0.0020] [Best : Accuracy=87.67, Error=12.33]

==>>Epoch=[093/250]], [2018-03-31 23:26:19], LR=[0.002], Batch=[32] [Model=SimpleNet]
  Epoch: [093][000/1475]   Time 0.134 (0.134)   Data 0.103 (0.103)   Loss 0.2913 (0.2913)   Prec@1 90.625 (90.625)   Prec@5 90.625 (90.625)   [2018-03-31 23:26:19]
  Epoch: [093][200/1475]   Time 0.143 (0.139)   Data 0.109 (0.105)   Loss 0.3135 (0.3507)   Prec@1 84.375 (85.075)   Prec@5 84.375 (85.075)   [2018-03-31 23:26:47]
  Epoch: [093][400/1475]   Time 0.141 (0.141)   Data 0.094 (0.107)   Loss 0.4604 (0.3547)   Prec@1 78.125 (84.936)   Prec@5 78.125 (84.936)   [2018-03-31 23:27:16]
  Epoch: [093][600/1475]   Time 0.141 (0.140)   Data 0.109 (0.106)   Loss 0.2912 (0.3575)   Prec@1 93.750 (84.853)   Prec@5 93.750 (84.853)   [2018-03-31 23:27:43]
  Epoch: [093][800/1475]   Time 0.141 (0.140)   Data 0.109 (0.106)   Loss 0.2062 (0.3589)   Prec@1 93.750 (84.757)   Prec@5 93.750 (84.757)   [2018-03-31 23:28:11]
  Epoch: [093][1000/1475]   Time 0.148 (0.140)   Data 0.109 (0.106)   Loss 0.3234 (0.3611)   Prec@1 87.500 (84.587)   Prec@5 87.500 (84.587)   [2018-03-31 23:28:39]
  Epoch: [093][1200/1475]   Time 0.142 (0.141)   Data 0.108 (0.107)   Loss 0.3097 (0.3614)   Prec@1 87.500 (84.578)   Prec@5 87.500 (84.578)   [2018-03-31 23:29:08]
  Epoch: [093][1400/1475]   Time 0.125 (0.141)   Data 0.094 (0.107)   Loss 0.2666 (0.3615)   Prec@1 87.500 (84.531)   Prec@5 87.500 (84.531)   [2018-03-31 23:29:36]
  **Train** Prec@1 84.476 Prec@5 84.476 Error@1 15.524
  **VAL** Prec@1 87.617 Prec@5 87.617 Error@1 12.383

==>>[2018-03-31 23:30:05] [Epoch=094/250] [Need: 09:43:03] [learning_rate=0.0020] [Best : Accuracy=87.67, Error=12.33]

==>>Epoch=[094/250]], [2018-03-31 23:30:05], LR=[0.002], Batch=[32] [Model=SimpleNet]
  Epoch: [094][000/1475]   Time 0.143 (0.143)   Data 0.098 (0.098)   Loss 0.2985 (0.2985)   Prec@1 87.500 (87.500)   Prec@5 87.500 (87.500)   [2018-03-31 23:30:05]
  Epoch: [094][200/1475]   Time 0.132 (0.139)   Data 0.108 (0.105)   Loss 0.3354 (0.3593)   Prec@1 87.500 (84.593)   Prec@5 87.500 (84.593)   [2018-03-31 23:30:33]
  Epoch: [094][400/1475]   Time 0.131 (0.141)   Data 0.111 (0.107)   Loss 0.4203 (0.3628)   Prec@1 81.250 (84.289)   Prec@5 81.250 (84.289)   [2018-03-31 23:31:01]
  Epoch: [094][600/1475]   Time 0.136 (0.141)   Data 0.105 (0.108)   Loss 0.4124 (0.3612)   Prec@1 81.250 (84.229)   Prec@5 81.250 (84.229)   [2018-03-31 23:31:30]
  Epoch: [094][800/1475]   Time 0.150 (0.142)   Data 0.115 (0.108)   Loss 0.5674 (0.3636)   Prec@1 75.000 (84.211)   Prec@5 75.000 (84.211)   [2018-03-31 23:31:58]
  Epoch: [094][1000/1475]   Time 0.141 (0.143)   Data 0.109 (0.109)   Loss 0.3363 (0.3641)   Prec@1 87.500 (84.191)   Prec@5 87.500 (84.191)   [2018-03-31 23:32:28]
  Epoch: [094][1200/1475]   Time 0.141 (0.142)   Data 0.109 (0.109)   Loss 0.1450 (0.3637)   Prec@1 96.875 (84.276)   Prec@5 96.875 (84.276)   [2018-03-31 23:32:56]
  Epoch: [094][1400/1475]   Time 0.141 (0.142)   Data 0.109 (0.108)   Loss 0.2489 (0.3631)   Prec@1 87.500 (84.348)   Prec@5 87.500 (84.348)   [2018-03-31 23:33:23]
  **Train** Prec@1 84.343 Prec@5 84.343 Error@1 15.657
  **VAL** Prec@1 87.197 Prec@5 87.197 Error@1 12.803

==>>[2018-03-31 23:33:51] [Epoch=095/250] [Need: 09:39:23] [learning_rate=0.0020] [Best : Accuracy=87.67, Error=12.33]

==>>Epoch=[095/250]], [2018-03-31 23:33:51], LR=[0.002], Batch=[32] [Model=SimpleNet]
  Epoch: [095][000/1475]   Time 0.141 (0.141)   Data 0.109 (0.109)   Loss 0.3193 (0.3193)   Prec@1 84.375 (84.375)   Prec@5 84.375 (84.375)   [2018-03-31 23:33:52]
  Epoch: [095][200/1475]   Time 0.144 (0.139)   Data 0.109 (0.106)   Loss 0.7046 (0.3573)   Prec@1 65.625 (84.530)   Prec@5 65.625 (84.530)   [2018-03-31 23:34:19]
  Epoch: [095][400/1475]   Time 0.130 (0.139)   Data 0.099 (0.105)   Loss 0.2739 (0.3587)   Prec@1 93.750 (84.609)   Prec@5 93.750 (84.609)   [2018-03-31 23:34:47]
  Epoch: [095][600/1475]   Time 0.141 (0.139)   Data 0.109 (0.105)   Loss 0.3716 (0.3602)   Prec@1 81.250 (84.526)   Prec@5 81.250 (84.526)   [2018-03-31 23:35:15]
  Epoch: [095][800/1475]   Time 0.141 (0.139)   Data 0.094 (0.106)   Loss 0.2257 (0.3589)   Prec@1 93.750 (84.597)   Prec@5 93.750 (84.597)   [2018-03-31 23:35:43]
  Epoch: [095][1000/1475]   Time 0.141 (0.139)   Data 0.109 (0.106)   Loss 0.3329 (0.3606)   Prec@1 87.500 (84.556)   Prec@5 87.500 (84.556)   [2018-03-31 23:36:10]
  Epoch: [095][1200/1475]   Time 0.146 (0.139)   Data 0.115 (0.106)   Loss 0.4725 (0.3625)   Prec@1 78.125 (84.544)   Prec@5 78.125 (84.544)   [2018-03-31 23:36:38]
  Epoch: [095][1400/1475]   Time 0.145 (0.139)   Data 0.109 (0.105)   Loss 0.3135 (0.3633)   Prec@1 87.500 (84.518)   Prec@5 87.500 (84.518)   [2018-03-31 23:37:06]
  **Train** Prec@1 84.555 Prec@5 84.555 Error@1 15.445
  **VAL** Prec@1 87.437 Prec@5 87.437 Error@1 12.563

==>>[2018-03-31 23:37:34] [Epoch=096/250] [Need: 09:35:36] [learning_rate=0.0020] [Best : Accuracy=87.67, Error=12.33]

==>>Epoch=[096/250]], [2018-03-31 23:37:34], LR=[0.002], Batch=[32] [Model=SimpleNet]
  Epoch: [096][000/1475]   Time 0.141 (0.141)   Data 0.109 (0.109)   Loss 0.4970 (0.4970)   Prec@1 84.375 (84.375)   Prec@5 84.375 (84.375)   [2018-03-31 23:37:34]
  Epoch: [096][200/1475]   Time 0.147 (0.139)   Data 0.109 (0.106)   Loss 0.3179 (0.3721)   Prec@1 81.250 (84.453)   Prec@5 81.250 (84.453)   [2018-03-31 23:38:02]
  Epoch: [096][400/1475]   Time 0.141 (0.139)   Data 0.094 (0.106)   Loss 0.2901 (0.3582)   Prec@1 87.500 (84.804)   Prec@5 87.500 (84.804)   [2018-03-31 23:38:30]
  Epoch: [096][600/1475]   Time 0.141 (0.139)   Data 0.109 (0.106)   Loss 0.5494 (0.3607)   Prec@1 75.000 (84.656)   Prec@5 75.000 (84.656)   [2018-03-31 23:38:58]
  Epoch: [096][800/1475]   Time 0.128 (0.139)   Data 0.110 (0.106)   Loss 0.3306 (0.3635)   Prec@1 81.250 (84.476)   Prec@5 81.250 (84.476)   [2018-03-31 23:39:25]
  Epoch: [096][1000/1475]   Time 0.144 (0.139)   Data 0.113 (0.106)   Loss 0.3890 (0.3631)   Prec@1 87.500 (84.531)   Prec@5 87.500 (84.531)   [2018-03-31 23:39:53]
  Epoch: [096][1200/1475]   Time 0.141 (0.139)   Data 0.109 (0.106)   Loss 0.3643 (0.3633)   Prec@1 78.125 (84.466)   Prec@5 78.125 (84.466)   [2018-03-31 23:40:21]
  Epoch: [096][1400/1475]   Time 0.140 (0.139)   Data 0.094 (0.106)   Loss 0.4235 (0.3626)   Prec@1 81.250 (84.527)   Prec@5 81.250 (84.527)   [2018-03-31 23:40:49]
  **Train** Prec@1 84.597 Prec@5 84.597 Error@1 15.403
  **VAL** Prec@1 87.197 Prec@5 87.197 Error@1 12.803

==>>[2018-03-31 23:41:17] [Epoch=097/250] [Need: 09:31:49] [learning_rate=0.0020] [Best : Accuracy=87.67, Error=12.33]

==>>Epoch=[097/250]], [2018-03-31 23:41:17], LR=[0.002], Batch=[32] [Model=SimpleNet]
  Epoch: [097][000/1475]   Time 0.125 (0.125)   Data 0.094 (0.094)   Loss 0.2432 (0.2432)   Prec@1 90.625 (90.625)   Prec@5 90.625 (90.625)   [2018-03-31 23:41:17]
  Epoch: [097][200/1475]   Time 0.141 (0.139)   Data 0.109 (0.106)   Loss 0.4016 (0.3748)   Prec@1 84.375 (84.282)   Prec@5 84.375 (84.282)   [2018-03-31 23:41:45]
  Epoch: [097][400/1475]   Time 0.141 (0.140)   Data 0.109 (0.107)   Loss 0.3883 (0.3680)   Prec@1 84.375 (84.437)   Prec@5 84.375 (84.437)   [2018-03-31 23:42:13]
  Epoch: [097][600/1475]   Time 0.144 (0.141)   Data 0.098 (0.107)   Loss 0.3757 (0.3634)   Prec@1 84.375 (84.729)   Prec@5 84.375 (84.729)   [2018-03-31 23:42:41]
  Epoch: [097][800/1475]   Time 0.147 (0.142)   Data 0.111 (0.108)   Loss 0.3841 (0.3590)   Prec@1 84.375 (84.980)   Prec@5 84.375 (84.980)   [2018-03-31 23:43:10]
  Epoch: [097][1000/1475]   Time 0.142 (0.144)   Data 0.109 (0.110)   Loss 0.3183 (0.3622)   Prec@1 90.625 (84.753)   Prec@5 90.625 (84.753)   [2018-03-31 23:43:41]
  Epoch: [097][1200/1475]   Time 0.141 (0.144)   Data 0.109 (0.110)   Loss 0.3687 (0.3628)   Prec@1 84.375 (84.666)   Prec@5 84.375 (84.666)   [2018-03-31 23:44:09]
  Epoch: [097][1400/1475]   Time 0.135 (0.143)   Data 0.109 (0.109)   Loss 0.3693 (0.3608)   Prec@1 78.125 (84.776)   Prec@5 78.125 (84.776)   [2018-03-31 23:44:38]
  **Train** Prec@1 84.769 Prec@5 84.769 Error@1 15.231
  **VAL** Prec@1 87.185 Prec@5 87.185 Error@1 12.815

==>>[2018-03-31 23:45:07] [Epoch=098/250] [Need: 09:28:13] [learning_rate=0.0020] [Best : Accuracy=87.67, Error=12.33]

==>>Epoch=[098/250]], [2018-03-31 23:45:07], LR=[0.002], Batch=[32] [Model=SimpleNet]
  Epoch: [098][000/1475]   Time 0.141 (0.141)   Data 0.109 (0.109)   Loss 0.4397 (0.4397)   Prec@1 81.250 (81.250)   Prec@5 81.250 (81.250)   [2018-03-31 23:45:07]
  Epoch: [098][200/1475]   Time 0.147 (0.141)   Data 0.116 (0.108)   Loss 0.2576 (0.3608)   Prec@1 87.500 (84.748)   Prec@5 87.500 (84.748)   [2018-03-31 23:45:35]
  Epoch: [098][400/1475]   Time 0.139 (0.143)   Data 0.106 (0.109)   Loss 0.3148 (0.3593)   Prec@1 87.500 (84.804)   Prec@5 87.500 (84.804)   [2018-03-31 23:46:04]
  Epoch: [098][600/1475]   Time 0.138 (0.143)   Data 0.105 (0.110)   Loss 0.5485 (0.3582)   Prec@1 68.750 (84.708)   Prec@5 68.750 (84.708)   [2018-03-31 23:46:33]
  Epoch: [098][800/1475]   Time 0.181 (0.143)   Data 0.146 (0.109)   Loss 0.3505 (0.3580)   Prec@1 78.125 (84.738)   Prec@5 78.125 (84.738)   [2018-03-31 23:47:01]
  Epoch: [098][1000/1475]   Time 0.141 (0.143)   Data 0.109 (0.109)   Loss 0.7884 (0.3595)   Prec@1 71.875 (84.668)   Prec@5 71.875 (84.668)   [2018-03-31 23:47:30]
  Epoch: [098][1200/1475]   Time 0.156 (0.143)   Data 0.109 (0.109)   Loss 0.2396 (0.3605)   Prec@1 90.625 (84.604)   Prec@5 90.625 (84.604)   [2018-03-31 23:47:59]
  Epoch: [098][1400/1475]   Time 0.139 (0.144)   Data 0.106 (0.109)   Loss 0.2999 (0.3602)   Prec@1 84.375 (84.614)   Prec@5 84.375 (84.614)   [2018-03-31 23:48:28]
  **Train** Prec@1 84.678 Prec@5 84.678 Error@1 15.322
  **VAL** Prec@1 87.485 Prec@5 87.485 Error@1 12.515

==>>[2018-03-31 23:48:56] [Epoch=099/250] [Need: 09:24:37] [learning_rate=0.0020] [Best : Accuracy=87.67, Error=12.33]

==>>Epoch=[099/250]], [2018-03-31 23:48:56], LR=[0.002], Batch=[32] [Model=SimpleNet]
  Epoch: [099][000/1475]   Time 0.139 (0.139)   Data 0.105 (0.105)   Loss 0.4271 (0.4271)   Prec@1 75.000 (75.000)   Prec@5 75.000 (75.000)   [2018-03-31 23:48:56]
  Epoch: [099][200/1475]   Time 0.139 (0.140)   Data 0.106 (0.107)   Loss 0.5690 (0.3486)   Prec@1 81.250 (85.028)   Prec@5 81.250 (85.028)   [2018-03-31 23:49:24]
  Epoch: [099][400/1475]   Time 0.141 (0.141)   Data 0.109 (0.107)   Loss 0.3211 (0.3533)   Prec@1 84.375 (85.030)   Prec@5 84.375 (85.030)   [2018-03-31 23:49:53]
  Epoch: [099][600/1475]   Time 0.144 (0.145)   Data 0.109 (0.111)   Loss 0.2917 (0.3610)   Prec@1 84.375 (84.609)   Prec@5 84.375 (84.609)   [2018-03-31 23:50:23]
  Epoch: [099][800/1475]   Time 0.150 (0.144)   Data 0.120 (0.110)   Loss 0.5311 (0.3604)   Prec@1 71.875 (84.699)   Prec@5 71.875 (84.699)   [2018-03-31 23:50:51]
  Epoch: [099][1000/1475]   Time 0.128 (0.143)   Data 0.097 (0.109)   Loss 0.2739 (0.3627)   Prec@1 90.625 (84.637)   Prec@5 90.625 (84.637)   [2018-03-31 23:51:19]
  Epoch: [099][1200/1475]   Time 0.141 (0.142)   Data 0.109 (0.109)   Loss 0.2625 (0.3623)   Prec@1 87.500 (84.679)   Prec@5 87.500 (84.679)   [2018-03-31 23:51:47]
  Epoch: [099][1400/1475]   Time 0.139 (0.142)   Data 0.108 (0.108)   Loss 0.3172 (0.3626)   Prec@1 81.250 (84.656)   Prec@5 81.250 (84.656)   [2018-03-31 23:52:15]
  **Train** Prec@1 84.688 Prec@5 84.688 Error@1 15.312
  **VAL** Prec@1 86.824 Prec@5 86.824 Error@1 13.176

==>>[2018-03-31 23:52:43] [Epoch=100/250] [Need: 09:20:57] [learning_rate=0.0020] [Best : Accuracy=87.67, Error=12.33]

==>>Epoch=[100/250]], [2018-03-31 23:52:43], LR=[0.002], Batch=[32] [Model=SimpleNet]
  Epoch: [100][000/1475]   Time 0.130 (0.130)   Data 0.099 (0.099)   Loss 0.2434 (0.2434)   Prec@1 90.625 (90.625)   Prec@5 90.625 (90.625)   [2018-03-31 23:52:43]
  Epoch: [100][200/1475]   Time 0.129 (0.139)   Data 0.098 (0.106)   Loss 0.3884 (0.3677)   Prec@1 81.250 (84.328)   Prec@5 81.250 (84.328)   [2018-03-31 23:53:11]
  Epoch: [100][400/1475]   Time 0.132 (0.139)   Data 0.100 (0.106)   Loss 0.3905 (0.3660)   Prec@1 81.250 (84.281)   Prec@5 81.250 (84.281)   [2018-03-31 23:53:39]
  Epoch: [100][600/1475]   Time 0.141 (0.139)   Data 0.109 (0.106)   Loss 0.4226 (0.3619)   Prec@1 90.625 (84.443)   Prec@5 90.625 (84.443)   [2018-03-31 23:54:06]
  Epoch: [100][800/1475]   Time 0.142 (0.139)   Data 0.109 (0.106)   Loss 0.4003 (0.3597)   Prec@1 81.250 (84.527)   Prec@5 81.250 (84.527)   [2018-03-31 23:54:34]
  Epoch: [100][1000/1475]   Time 0.145 (0.139)   Data 0.114 (0.106)   Loss 0.3970 (0.3563)   Prec@1 81.250 (84.728)   Prec@5 81.250 (84.728)   [2018-03-31 23:55:02]
  Epoch: [100][1200/1475]   Time 0.125 (0.139)   Data 0.094 (0.106)   Loss 0.5548 (0.3571)   Prec@1 71.875 (84.739)   Prec@5 71.875 (84.739)   [2018-03-31 23:55:30]
  Epoch: [100][1400/1475]   Time 0.142 (0.139)   Data 0.095 (0.106)   Loss 0.2466 (0.3593)   Prec@1 93.750 (84.665)   Prec@5 93.750 (84.665)   [2018-03-31 23:55:57]
  **Train** Prec@1 84.622 Prec@5 84.622 Error@1 15.378
  **VAL** Prec@1 87.365 Prec@5 87.365 Error@1 12.635

==>>[2018-03-31 23:56:26] [Epoch=101/250] [Need: 09:17:10] [learning_rate=0.0020] [Best : Accuracy=87.67, Error=12.33]

==>>Epoch=[101/250]], [2018-03-31 23:56:26], LR=[0.002], Batch=[32] [Model=SimpleNet]
  Epoch: [101][000/1475]   Time 0.141 (0.141)   Data 0.109 (0.109)   Loss 0.2902 (0.2902)   Prec@1 90.625 (90.625)   Prec@5 90.625 (90.625)   [2018-03-31 23:56:26]
  Epoch: [101][200/1475]   Time 0.125 (0.139)   Data 0.094 (0.106)   Loss 0.2282 (0.3484)   Prec@1 96.875 (85.246)   Prec@5 96.875 (85.246)   [2018-03-31 23:56:54]
  Epoch: [101][400/1475]   Time 0.133 (0.139)   Data 0.109 (0.106)   Loss 0.3308 (0.3552)   Prec@1 84.375 (84.827)   Prec@5 84.375 (84.827)   [2018-03-31 23:57:21]
  Epoch: [101][600/1475]   Time 0.144 (0.139)   Data 0.113 (0.106)   Loss 0.3980 (0.3595)   Prec@1 87.500 (84.692)   Prec@5 87.500 (84.692)   [2018-03-31 23:57:49]
  Epoch: [101][800/1475]   Time 0.141 (0.139)   Data 0.109 (0.106)   Loss 0.3791 (0.3598)   Prec@1 87.500 (84.632)   Prec@5 87.500 (84.632)   [2018-03-31 23:58:17]
  Epoch: [101][1000/1475]   Time 0.125 (0.139)   Data 0.094 (0.106)   Loss 0.3075 (0.3602)   Prec@1 87.500 (84.609)   Prec@5 87.500 (84.609)   [2018-03-31 23:58:45]
  Epoch: [101][1200/1475]   Time 0.151 (0.139)   Data 0.117 (0.106)   Loss 0.4084 (0.3594)   Prec@1 87.500 (84.690)   Prec@5 87.500 (84.690)   [2018-03-31 23:59:12]
  Epoch: [101][1400/1475]   Time 0.127 (0.139)   Data 0.096 (0.106)   Loss 0.3481 (0.3600)   Prec@1 84.375 (84.719)   Prec@5 84.375 (84.719)   [2018-03-31 23:59:40]
  **Train** Prec@1 84.773 Prec@5 84.773 Error@1 15.227
  **VAL** Prec@1 86.969 Prec@5 86.969 Error@1 13.031

==>>[2018-04-01 00:00:08] [Epoch=102/250] [Need: 09:13:23] [learning_rate=0.0020] [Best : Accuracy=87.67, Error=12.33]

==>>Epoch=[102/250]], [2018-04-01 00:00:08], LR=[0.002], Batch=[32] [Model=SimpleNet]
  Epoch: [102][000/1475]   Time 0.129 (0.129)   Data 0.098 (0.098)   Loss 0.4950 (0.4950)   Prec@1 81.250 (81.250)   Prec@5 81.250 (81.250)   [2018-04-01 00:00:09]
  Epoch: [102][200/1475]   Time 0.141 (0.139)   Data 0.109 (0.106)   Loss 0.2739 (0.3576)   Prec@1 93.750 (84.810)   Prec@5 93.750 (84.810)   [2018-04-01 00:00:36]
  Epoch: [102][400/1475]   Time 0.145 (0.139)   Data 0.113 (0.106)   Loss 0.2608 (0.3610)   Prec@1 90.625 (84.523)   Prec@5 90.625 (84.523)   [2018-04-01 00:01:04]
  Epoch: [102][600/1475]   Time 0.141 (0.139)   Data 0.109 (0.106)   Loss 0.3405 (0.3652)   Prec@1 84.375 (84.375)   Prec@5 84.375 (84.375)   [2018-04-01 00:01:32]
  Epoch: [102][800/1475]   Time 0.141 (0.139)   Data 0.109 (0.106)   Loss 0.3747 (0.3659)   Prec@1 84.375 (84.313)   Prec@5 84.375 (84.313)   [2018-04-01 00:02:00]
  Epoch: [102][1000/1475]   Time 0.141 (0.139)   Data 0.110 (0.106)   Loss 0.2345 (0.3644)   Prec@1 93.750 (84.391)   Prec@5 93.750 (84.391)   [2018-04-01 00:02:27]
  Epoch: [102][1200/1475]   Time 0.125 (0.139)   Data 0.094 (0.106)   Loss 0.3930 (0.3627)   Prec@1 84.375 (84.489)   Prec@5 84.375 (84.489)   [2018-04-01 00:02:55]
  Epoch: [102][1400/1475]   Time 0.144 (0.139)   Data 0.097 (0.106)   Loss 0.3629 (0.3616)   Prec@1 81.250 (84.591)   Prec@5 81.250 (84.591)   [2018-04-01 00:03:23]
  **Train** Prec@1 84.550 Prec@5 84.550 Error@1 15.450
  **VAL** Prec@1 87.401 Prec@5 87.401 Error@1 12.599

==>>[2018-04-01 00:03:51] [Epoch=103/250] [Need: 09:09:36] [learning_rate=0.0020] [Best : Accuracy=87.67, Error=12.33]

==>>Epoch=[103/250]], [2018-04-01 00:03:51], LR=[0.002], Batch=[32] [Model=SimpleNet]
  Epoch: [103][000/1475]   Time 0.145 (0.145)   Data 0.114 (0.114)   Loss 0.1656 (0.1656)   Prec@1 96.875 (96.875)   Prec@5 96.875 (96.875)   [2018-04-01 00:03:51]
  Epoch: [103][200/1475]   Time 0.125 (0.139)   Data 0.094 (0.107)   Loss 0.2068 (0.3703)   Prec@1 90.625 (84.095)   Prec@5 90.625 (84.095)   [2018-04-01 00:04:19]
  Epoch: [103][400/1475]   Time 0.131 (0.139)   Data 0.104 (0.106)   Loss 0.4502 (0.3674)   Prec@1 78.125 (84.484)   Prec@5 78.125 (84.484)   [2018-04-01 00:04:47]
  Epoch: [103][600/1475]   Time 0.146 (0.139)   Data 0.115 (0.106)   Loss 0.4717 (0.3610)   Prec@1 75.000 (84.796)   Prec@5 75.000 (84.796)   [2018-04-01 00:05:14]
  Epoch: [103][800/1475]   Time 0.135 (0.139)   Data 0.116 (0.106)   Loss 0.2743 (0.3599)   Prec@1 90.625 (84.878)   Prec@5 90.625 (84.878)   [2018-04-01 00:05:42]
  Epoch: [103][1000/1475]   Time 0.125 (0.139)   Data 0.094 (0.106)   Loss 0.4098 (0.3601)   Prec@1 81.250 (84.915)   Prec@5 81.250 (84.915)   [2018-04-01 00:06:10]
  Epoch: [103][1200/1475]   Time 0.128 (0.139)   Data 0.105 (0.106)   Loss 0.3380 (0.3597)   Prec@1 84.375 (84.937)   Prec@5 84.375 (84.937)   [2018-04-01 00:06:38]
  Epoch: [103][1400/1475]   Time 0.152 (0.139)   Data 0.116 (0.106)   Loss 0.3372 (0.3588)   Prec@1 87.500 (84.959)   Prec@5 87.500 (84.959)   [2018-04-01 00:07:05]
  **Train** Prec@1 84.974 Prec@5 84.974 Error@1 15.026
  **VAL** Prec@1 87.365 Prec@5 87.365 Error@1 12.635

==>>[2018-04-01 00:07:34] [Epoch=104/250] [Need: 09:05:49] [learning_rate=0.0020] [Best : Accuracy=87.67, Error=12.33]

==>>Epoch=[104/250]], [2018-04-01 00:07:34], LR=[0.002], Batch=[32] [Model=SimpleNet]
  Epoch: [104][000/1475]   Time 0.141 (0.141)   Data 0.109 (0.109)   Loss 0.2524 (0.2524)   Prec@1 90.625 (90.625)   Prec@5 90.625 (90.625)   [2018-04-01 00:07:34]
  Epoch: [104][200/1475]   Time 0.141 (0.139)   Data 0.109 (0.106)   Loss 0.7583 (0.3472)   Prec@1 65.625 (85.075)   Prec@5 65.625 (85.075)   [2018-04-01 00:08:02]
  Epoch: [104][400/1475]   Time 0.141 (0.139)   Data 0.109 (0.106)   Loss 0.4065 (0.3552)   Prec@1 81.250 (84.765)   Prec@5 81.250 (84.765)   [2018-04-01 00:08:29]
  Epoch: [104][600/1475]   Time 0.140 (0.139)   Data 0.107 (0.106)   Loss 0.3220 (0.3596)   Prec@1 87.500 (84.505)   Prec@5 87.500 (84.505)   [2018-04-01 00:08:57]
  Epoch: [104][800/1475]   Time 0.143 (0.139)   Data 0.112 (0.106)   Loss 0.2252 (0.3615)   Prec@1 90.625 (84.480)   Prec@5 90.625 (84.480)   [2018-04-01 00:09:25]
  Epoch: [104][1000/1475]   Time 0.141 (0.139)   Data 0.109 (0.105)   Loss 0.5430 (0.3626)   Prec@1 81.250 (84.431)   Prec@5 81.250 (84.431)   [2018-04-01 00:09:53]
  Epoch: [104][1200/1475]   Time 0.141 (0.139)   Data 0.109 (0.106)   Loss 0.3963 (0.3611)   Prec@1 87.500 (84.424)   Prec@5 87.500 (84.424)   [2018-04-01 00:10:20]
  Epoch: [104][1400/1475]   Time 0.145 (0.139)   Data 0.114 (0.105)   Loss 0.3589 (0.3612)   Prec@1 81.250 (84.493)   Prec@5 81.250 (84.493)   [2018-04-01 00:10:48]
  **Train** Prec@1 84.588 Prec@5 84.588 Error@1 15.412
  **VAL** Prec@1 86.836 Prec@5 86.836 Error@1 13.164

==>>[2018-04-01 00:11:16] [Epoch=105/250] [Need: 09:02:03] [learning_rate=0.0020] [Best : Accuracy=87.67, Error=12.33]

==>>Epoch=[105/250]], [2018-04-01 00:11:16], LR=[0.002], Batch=[32] [Model=SimpleNet]
  Epoch: [105][000/1475]   Time 0.125 (0.125)   Data 0.094 (0.094)   Loss 0.4951 (0.4951)   Prec@1 87.500 (87.500)   Prec@5 87.500 (87.500)   [2018-04-01 00:11:16]
  Epoch: [105][200/1475]   Time 0.141 (0.139)   Data 0.109 (0.105)   Loss 0.3056 (0.3622)   Prec@1 87.500 (84.359)   Prec@5 87.500 (84.359)   [2018-04-01 00:11:44]
  Epoch: [105][400/1475]   Time 0.151 (0.139)   Data 0.118 (0.106)   Loss 0.3305 (0.3581)   Prec@1 84.375 (84.523)   Prec@5 84.375 (84.523)   [2018-04-01 00:12:12]
  Epoch: [105][600/1475]   Time 0.129 (0.139)   Data 0.098 (0.105)   Loss 0.4311 (0.3582)   Prec@1 81.250 (84.515)   Prec@5 81.250 (84.515)   [2018-04-01 00:12:39]
  Epoch: [105][800/1475]   Time 0.143 (0.139)   Data 0.111 (0.105)   Loss 0.5188 (0.3585)   Prec@1 71.875 (84.593)   Prec@5 71.875 (84.593)   [2018-04-01 00:13:07]
  Epoch: [105][1000/1475]   Time 0.141 (0.139)   Data 0.109 (0.105)   Loss 0.3275 (0.3584)   Prec@1 87.500 (84.615)   Prec@5 87.500 (84.615)   [2018-04-01 00:13:35]
  Epoch: [105][1200/1475]   Time 0.133 (0.139)   Data 0.102 (0.105)   Loss 0.4449 (0.3587)   Prec@1 84.375 (84.692)   Prec@5 84.375 (84.692)   [2018-04-01 00:14:03]
  Epoch: [105][1400/1475]   Time 0.142 (0.139)   Data 0.109 (0.105)   Loss 0.4394 (0.3593)   Prec@1 84.375 (84.716)   Prec@5 84.375 (84.716)   [2018-04-01 00:14:30]
  **Train** Prec@1 84.688 Prec@5 84.688 Error@1 15.312
  **VAL** Prec@1 86.908 Prec@5 86.908 Error@1 13.092

==>>[2018-04-01 00:14:58] [Epoch=106/250] [Need: 08:58:16] [learning_rate=0.0020] [Best : Accuracy=87.67, Error=12.33]

==>>Epoch=[106/250]], [2018-04-01 00:14:58], LR=[0.002], Batch=[32] [Model=SimpleNet]
  Epoch: [106][000/1475]   Time 0.141 (0.141)   Data 0.109 (0.109)   Loss 0.2750 (0.2750)   Prec@1 87.500 (87.500)   Prec@5 87.500 (87.500)   [2018-04-01 00:14:59]
  Epoch: [106][200/1475]   Time 0.141 (0.139)   Data 0.109 (0.106)   Loss 0.2203 (0.3628)   Prec@1 87.500 (84.608)   Prec@5 87.500 (84.608)   [2018-04-01 00:15:26]
  Epoch: [106][400/1475]   Time 0.142 (0.139)   Data 0.113 (0.105)   Loss 0.3469 (0.3639)   Prec@1 81.250 (84.398)   Prec@5 81.250 (84.398)   [2018-04-01 00:15:54]
  Epoch: [106][600/1475]   Time 0.141 (0.139)   Data 0.109 (0.105)   Loss 0.5190 (0.3639)   Prec@1 71.875 (84.349)   Prec@5 71.875 (84.349)   [2018-04-01 00:16:22]
  Epoch: [106][800/1475]   Time 0.143 (0.139)   Data 0.109 (0.106)   Loss 0.1810 (0.3607)   Prec@1 93.750 (84.535)   Prec@5 93.750 (84.535)   [2018-04-01 00:16:50]
  Epoch: [106][1000/1475]   Time 0.128 (0.139)   Data 0.097 (0.106)   Loss 0.3363 (0.3589)   Prec@1 84.375 (84.597)   Prec@5 84.375 (84.597)   [2018-04-01 00:17:17]
  Epoch: [106][1200/1475]   Time 0.144 (0.139)   Data 0.113 (0.106)   Loss 0.2829 (0.3577)   Prec@1 87.500 (84.698)   Prec@5 87.500 (84.698)   [2018-04-01 00:17:45]
  Epoch: [106][1400/1475]   Time 0.125 (0.139)   Data 0.094 (0.106)   Loss 0.3774 (0.3579)   Prec@1 81.250 (84.683)   Prec@5 81.250 (84.683)   [2018-04-01 00:18:13]
  **Train** Prec@1 84.616 Prec@5 84.616 Error@1 15.384
  **VAL** Prec@1 86.428 Prec@5 86.428 Error@1 13.572

==>>[2018-04-01 00:18:41] [Epoch=107/250] [Need: 08:54:29] [learning_rate=0.0020] [Best : Accuracy=87.67, Error=12.33]

==>>Epoch=[107/250]], [2018-04-01 00:18:41], LR=[0.002], Batch=[32] [Model=SimpleNet]
  Epoch: [107][000/1475]   Time 0.133 (0.133)   Data 0.102 (0.102)   Loss 0.3280 (0.3280)   Prec@1 87.500 (87.500)   Prec@5 87.500 (87.500)   [2018-04-01 00:18:41]
  Epoch: [107][200/1475]   Time 0.141 (0.139)   Data 0.109 (0.106)   Loss 0.4364 (0.3646)   Prec@1 78.125 (84.282)   Prec@5 78.125 (84.282)   [2018-04-01 00:19:09]
  Epoch: [107][400/1475]   Time 0.143 (0.139)   Data 0.115 (0.105)   Loss 0.3947 (0.3597)   Prec@1 84.375 (84.741)   Prec@5 84.375 (84.741)   [2018-04-01 00:19:37]
  Epoch: [107][600/1475]   Time 0.141 (0.139)   Data 0.109 (0.105)   Loss 0.3948 (0.3613)   Prec@1 84.375 (84.734)   Prec@5 84.375 (84.734)   [2018-04-01 00:20:04]
  Epoch: [107][800/1475]   Time 0.149 (0.139)   Data 0.117 (0.105)   Loss 0.5347 (0.3593)   Prec@1 75.000 (84.699)   Prec@5 75.000 (84.699)   [2018-04-01 00:20:32]
  Epoch: [107][1000/1475]   Time 0.135 (0.139)   Data 0.104 (0.105)   Loss 0.3263 (0.3597)   Prec@1 84.375 (84.740)   Prec@5 84.375 (84.740)   [2018-04-01 00:21:00]
  Epoch: [107][1200/1475]   Time 0.138 (0.139)   Data 0.094 (0.105)   Loss 0.3680 (0.3587)   Prec@1 84.375 (84.734)   Prec@5 84.375 (84.734)   [2018-04-01 00:21:28]
  Epoch: [107][1400/1475]   Time 0.140 (0.139)   Data 0.109 (0.105)   Loss 0.3658 (0.3589)   Prec@1 75.000 (84.719)   Prec@5 75.000 (84.719)   [2018-04-01 00:21:55]
  **Train** Prec@1 84.661 Prec@5 84.661 Error@1 15.339
  **VAL** Prec@1 86.981 Prec@5 86.981 Error@1 13.019

==>>[2018-04-01 00:22:24] [Epoch=108/250] [Need: 08:50:43] [learning_rate=0.0020] [Best : Accuracy=87.67, Error=12.33]

==>>Epoch=[108/250]], [2018-04-01 00:22:24], LR=[0.002], Batch=[32] [Model=SimpleNet]
  Epoch: [108][000/1475]   Time 0.130 (0.130)   Data 0.103 (0.103)   Loss 0.3085 (0.3085)   Prec@1 90.625 (90.625)   Prec@5 90.625 (90.625)   [2018-04-01 00:22:24]
  Epoch: [108][200/1475]   Time 0.144 (0.138)   Data 0.113 (0.105)   Loss 0.2857 (0.3582)   Prec@1 90.625 (84.795)   Prec@5 90.625 (84.795)   [2018-04-01 00:22:51]
  Epoch: [108][400/1475]   Time 0.138 (0.139)   Data 0.107 (0.105)   Loss 0.5026 (0.3608)   Prec@1 84.375 (84.531)   Prec@5 84.375 (84.531)   [2018-04-01 00:23:19]
  Epoch: [108][600/1475]   Time 0.141 (0.139)   Data 0.094 (0.105)   Loss 0.2617 (0.3580)   Prec@1 90.625 (84.687)   Prec@5 90.625 (84.687)   [2018-04-01 00:23:47]
  Epoch: [108][800/1475]   Time 0.130 (0.139)   Data 0.099 (0.105)   Loss 0.4784 (0.3596)   Prec@1 81.250 (84.691)   Prec@5 81.250 (84.691)   [2018-04-01 00:24:15]
  Epoch: [108][1000/1475]   Time 0.141 (0.139)   Data 0.109 (0.105)   Loss 0.2102 (0.3583)   Prec@1 90.625 (84.778)   Prec@5 90.625 (84.778)   [2018-04-01 00:24:42]
  Epoch: [108][1200/1475]   Time 0.132 (0.139)   Data 0.109 (0.105)   Loss 0.3755 (0.3583)   Prec@1 87.500 (84.776)   Prec@5 87.500 (84.776)   [2018-04-01 00:25:10]
  Epoch: [108][1400/1475]   Time 0.141 (0.139)   Data 0.109 (0.106)   Loss 0.2884 (0.3595)   Prec@1 87.500 (84.712)   Prec@5 87.500 (84.712)   [2018-04-01 00:25:38]
  **Train** Prec@1 84.701 Prec@5 84.701 Error@1 15.299
  **VAL** Prec@1 87.017 Prec@5 87.017 Error@1 12.983

==>>[2018-04-01 00:26:06] [Epoch=109/250] [Need: 08:46:56] [learning_rate=0.0020] [Best : Accuracy=87.67, Error=12.33]

==>>Epoch=[109/250]], [2018-04-01 00:26:06], LR=[0.002], Batch=[32] [Model=SimpleNet]
  Epoch: [109][000/1475]   Time 0.138 (0.138)   Data 0.107 (0.107)   Loss 0.4972 (0.4972)   Prec@1 78.125 (78.125)   Prec@5 78.125 (78.125)   [2018-04-01 00:26:06]
  Epoch: [109][200/1475]   Time 0.131 (0.139)   Data 0.100 (0.106)   Loss 0.3407 (0.3706)   Prec@1 81.250 (83.660)   Prec@5 81.250 (83.660)   [2018-04-01 00:26:34]
  Epoch: [109][400/1475]   Time 0.141 (0.139)   Data 0.109 (0.106)   Loss 0.3825 (0.3661)   Prec@1 81.250 (84.531)   Prec@5 81.250 (84.531)   [2018-04-01 00:27:02]
  Epoch: [109][600/1475]   Time 0.141 (0.139)   Data 0.109 (0.106)   Loss 0.4378 (0.3609)   Prec@1 78.125 (84.682)   Prec@5 78.125 (84.682)   [2018-04-01 00:27:29]
  Epoch: [109][800/1475]   Time 0.145 (0.139)   Data 0.114 (0.106)   Loss 0.3764 (0.3593)   Prec@1 84.375 (84.773)   Prec@5 84.375 (84.773)   [2018-04-01 00:27:57]
  Epoch: [109][1000/1475]   Time 0.141 (0.139)   Data 0.109 (0.106)   Loss 0.4019 (0.3577)   Prec@1 81.250 (84.862)   Prec@5 81.250 (84.862)   [2018-04-01 00:28:25]
  Epoch: [109][1200/1475]   Time 0.144 (0.139)   Data 0.110 (0.106)   Loss 0.2295 (0.3587)   Prec@1 90.625 (84.768)   Prec@5 90.625 (84.768)   [2018-04-01 00:28:53]
  Epoch: [109][1400/1475]   Time 0.128 (0.139)   Data 0.097 (0.106)   Loss 0.3881 (0.3592)   Prec@1 87.500 (84.748)   Prec@5 87.500 (84.748)   [2018-04-01 00:29:20]
  **Train** Prec@1 84.739 Prec@5 84.739 Error@1 15.261
  **VAL** Prec@1 87.701 Prec@5 87.701 Error@1 12.299

==>>[2018-04-01 00:29:49] [Epoch=110/250] [Need: 08:43:10] [learning_rate=0.0020] [Best : Accuracy=87.70, Error=12.30]

==>>Epoch=[110/250]], [2018-04-01 00:29:49], LR=[0.002], Batch=[32] [Model=SimpleNet]
  Epoch: [110][000/1475]   Time 0.141 (0.141)   Data 0.109 (0.109)   Loss 0.2393 (0.2393)   Prec@1 93.750 (93.750)   Prec@5 93.750 (93.750)   [2018-04-01 00:29:49]
  Epoch: [110][200/1475]   Time 0.141 (0.139)   Data 0.109 (0.105)   Loss 0.3452 (0.3558)   Prec@1 84.375 (84.950)   Prec@5 84.375 (84.950)   [2018-04-01 00:30:16]
  Epoch: [110][400/1475]   Time 0.148 (0.139)   Data 0.101 (0.105)   Loss 0.3018 (0.3661)   Prec@1 87.500 (84.469)   Prec@5 87.500 (84.469)   [2018-04-01 00:30:44]
  Epoch: [110][600/1475]   Time 0.141 (0.139)   Data 0.109 (0.105)   Loss 0.3476 (0.3593)   Prec@1 84.375 (84.874)   Prec@5 84.375 (84.874)   [2018-04-01 00:31:12]
  Epoch: [110][800/1475]   Time 0.141 (0.139)   Data 0.109 (0.106)   Loss 0.4129 (0.3588)   Prec@1 75.000 (84.882)   Prec@5 75.000 (84.882)   [2018-04-01 00:31:40]
  Epoch: [110][1000/1475]   Time 0.144 (0.139)   Data 0.097 (0.105)   Loss 0.4095 (0.3598)   Prec@1 84.375 (84.840)   Prec@5 84.375 (84.840)   [2018-04-01 00:32:07]
  Epoch: [110][1200/1475]   Time 0.141 (0.139)   Data 0.120 (0.105)   Loss 0.3041 (0.3602)   Prec@1 84.375 (84.784)   Prec@5 84.375 (84.784)   [2018-04-01 00:32:35]
  Epoch: [110][1400/1475]   Time 0.133 (0.139)   Data 0.106 (0.105)   Loss 0.3424 (0.3594)   Prec@1 90.625 (84.774)   Prec@5 90.625 (84.774)   [2018-04-01 00:33:03]
  **Train** Prec@1 84.824 Prec@5 84.824 Error@1 15.176
  **VAL** Prec@1 87.329 Prec@5 87.329 Error@1 12.671

==>>[2018-04-01 00:33:31] [Epoch=111/250] [Need: 08:39:23] [learning_rate=0.0020] [Best : Accuracy=87.70, Error=12.30]

==>>Epoch=[111/250]], [2018-04-01 00:33:31], LR=[0.002], Batch=[32] [Model=SimpleNet]
  Epoch: [111][000/1475]   Time 0.122 (0.122)   Data 0.091 (0.091)   Loss 0.2448 (0.2448)   Prec@1 90.625 (90.625)   Prec@5 90.625 (90.625)   [2018-04-01 00:33:31]
  Epoch: [111][200/1475]   Time 0.141 (0.139)   Data 0.109 (0.105)   Loss 0.1914 (0.3693)   Prec@1 93.750 (84.188)   Prec@5 93.750 (84.188)   [2018-04-01 00:33:59]
  Epoch: [111][400/1475]   Time 0.141 (0.139)   Data 0.109 (0.106)   Loss 0.4420 (0.3627)   Prec@1 84.375 (84.469)   Prec@5 84.375 (84.469)   [2018-04-01 00:34:27]
  Epoch: [111][600/1475]   Time 0.136 (0.139)   Data 0.105 (0.106)   Loss 0.3668 (0.3581)   Prec@1 87.500 (84.807)   Prec@5 87.500 (84.807)   [2018-04-01 00:34:54]
  Epoch: [111][800/1475]   Time 0.145 (0.139)   Data 0.109 (0.105)   Loss 0.2990 (0.3586)   Prec@1 84.375 (84.851)   Prec@5 84.375 (84.851)   [2018-04-01 00:35:22]
  Epoch: [111][1000/1475]   Time 0.141 (0.139)   Data 0.109 (0.105)   Loss 0.4147 (0.3571)   Prec@1 81.250 (84.956)   Prec@5 81.250 (84.956)   [2018-04-01 00:35:50]
  Epoch: [111][1200/1475]   Time 0.149 (0.139)   Data 0.118 (0.106)   Loss 0.4623 (0.3576)   Prec@1 81.250 (84.932)   Prec@5 81.250 (84.932)   [2018-04-01 00:36:18]
  Epoch: [111][1400/1475]   Time 0.129 (0.139)   Data 0.105 (0.105)   Loss 0.3802 (0.3587)   Prec@1 84.375 (84.863)   Prec@5 84.375 (84.863)   [2018-04-01 00:36:45]
  **Train** Prec@1 84.851 Prec@5 84.851 Error@1 15.149
  **VAL** Prec@1 87.713 Prec@5 87.713 Error@1 12.287

==>>[2018-04-01 00:37:14] [Epoch=112/250] [Need: 08:35:37] [learning_rate=0.0020] [Best : Accuracy=87.71, Error=12.29]

==>>Epoch=[112/250]], [2018-04-01 00:37:14], LR=[0.002], Batch=[32] [Model=SimpleNet]
  Epoch: [112][000/1475]   Time 0.127 (0.127)   Data 0.095 (0.095)   Loss 0.3702 (0.3702)   Prec@1 84.375 (84.375)   Prec@5 84.375 (84.375)   [2018-04-01 00:37:14]
  Epoch: [112][200/1475]   Time 0.141 (0.139)   Data 0.109 (0.105)   Loss 0.4959 (0.3628)   Prec@1 71.875 (84.593)   Prec@5 71.875 (84.593)   [2018-04-01 00:37:42]
  Epoch: [112][400/1475]   Time 0.148 (0.139)   Data 0.109 (0.106)   Loss 0.3777 (0.3638)   Prec@1 84.375 (84.484)   Prec@5 84.375 (84.484)   [2018-04-01 00:38:09]
  Epoch: [112][600/1475]   Time 0.126 (0.139)   Data 0.095 (0.106)   Loss 0.2610 (0.3601)   Prec@1 90.625 (84.536)   Prec@5 90.625 (84.536)   [2018-04-01 00:38:37]
  Epoch: [112][800/1475]   Time 0.141 (0.139)   Data 0.094 (0.106)   Loss 0.4487 (0.3570)   Prec@1 84.375 (84.808)   Prec@5 84.375 (84.808)   [2018-04-01 00:39:05]
  Epoch: [112][1000/1475]   Time 0.141 (0.139)   Data 0.094 (0.106)   Loss 0.5762 (0.3598)   Prec@1 75.000 (84.662)   Prec@5 75.000 (84.662)   [2018-04-01 00:39:33]
  Epoch: [112][1200/1475]   Time 0.144 (0.139)   Data 0.109 (0.106)   Loss 0.3367 (0.3591)   Prec@1 84.375 (84.674)   Prec@5 84.375 (84.674)   [2018-04-01 00:40:01]
  Epoch: [112][1400/1475]   Time 0.144 (0.139)   Data 0.109 (0.106)   Loss 0.2740 (0.3583)   Prec@1 87.500 (84.681)   Prec@5 87.500 (84.681)   [2018-04-01 00:40:28]
  **Train** Prec@1 84.707 Prec@5 84.707 Error@1 15.293
  **VAL** Prec@1 87.845 Prec@5 87.845 Error@1 12.155

==>>[2018-04-01 00:40:57] [Epoch=113/250] [Need: 08:31:51] [learning_rate=0.0020] [Best : Accuracy=87.85, Error=12.15]

==>>Epoch=[113/250]], [2018-04-01 00:40:57], LR=[0.002], Batch=[32] [Model=SimpleNet]
  Epoch: [113][000/1475]   Time 0.126 (0.126)   Data 0.095 (0.095)   Loss 0.2072 (0.2072)   Prec@1 90.625 (90.625)   Prec@5 90.625 (90.625)   [2018-04-01 00:40:57]
  Epoch: [113][200/1475]   Time 0.125 (0.139)   Data 0.094 (0.106)   Loss 0.4298 (0.3612)   Prec@1 78.125 (84.142)   Prec@5 78.125 (84.142)   [2018-04-01 00:41:24]
  Epoch: [113][400/1475]   Time 0.141 (0.139)   Data 0.109 (0.106)   Loss 0.3728 (0.3531)   Prec@1 84.375 (84.780)   Prec@5 84.375 (84.780)   [2018-04-01 00:41:52]
  Epoch: [113][600/1475]   Time 0.141 (0.139)   Data 0.109 (0.106)   Loss 0.3724 (0.3528)   Prec@1 84.375 (84.921)   Prec@5 84.375 (84.921)   [2018-04-01 00:42:20]
  Epoch: [113][800/1475]   Time 0.143 (0.139)   Data 0.109 (0.106)   Loss 0.3665 (0.3542)   Prec@1 90.625 (84.886)   Prec@5 90.625 (84.886)   [2018-04-01 00:42:48]
  Epoch: [113][1000/1475]   Time 0.144 (0.139)   Data 0.113 (0.106)   Loss 0.4378 (0.3566)   Prec@1 78.125 (84.737)   Prec@5 78.125 (84.737)   [2018-04-01 00:43:16]
  Epoch: [113][1200/1475]   Time 0.141 (0.139)   Data 0.109 (0.106)   Loss 0.1986 (0.3573)   Prec@1 90.625 (84.742)   Prec@5 90.625 (84.742)   [2018-04-01 00:43:43]
  Epoch: [113][1400/1475]   Time 0.141 (0.139)   Data 0.109 (0.106)   Loss 0.4747 (0.3573)   Prec@1 78.125 (84.785)   Prec@5 78.125 (84.785)   [2018-04-01 00:44:11]
  **Train** Prec@1 84.820 Prec@5 84.820 Error@1 15.180
  **VAL** Prec@1 87.593 Prec@5 87.593 Error@1 12.407

==>>[2018-04-01 00:44:39] [Epoch=114/250] [Need: 08:28:05] [learning_rate=0.0020] [Best : Accuracy=87.85, Error=12.15]

==>>Epoch=[114/250]], [2018-04-01 00:44:39], LR=[0.002], Batch=[32] [Model=SimpleNet]
  Epoch: [114][000/1475]   Time 0.141 (0.141)   Data 0.109 (0.109)   Loss 0.6852 (0.6852)   Prec@1 65.625 (65.625)   Prec@5 65.625 (65.625)   [2018-04-01 00:44:39]
  Epoch: [114][200/1475]   Time 0.137 (0.139)   Data 0.108 (0.106)   Loss 0.2862 (0.3522)   Prec@1 87.500 (84.904)   Prec@5 87.500 (84.904)   [2018-04-01 00:45:07]
  Epoch: [114][400/1475]   Time 0.128 (0.139)   Data 0.106 (0.106)   Loss 0.2826 (0.3500)   Prec@1 87.500 (85.193)   Prec@5 87.500 (85.193)   [2018-04-01 00:45:35]
  Epoch: [114][600/1475]   Time 0.141 (0.139)   Data 0.109 (0.106)   Loss 0.3277 (0.3559)   Prec@1 90.625 (84.827)   Prec@5 90.625 (84.827)   [2018-04-01 00:46:03]
  Epoch: [114][800/1475]   Time 0.129 (0.139)   Data 0.098 (0.106)   Loss 0.4104 (0.3544)   Prec@1 75.000 (84.831)   Prec@5 75.000 (84.831)   [2018-04-01 00:46:31]
  Epoch: [114][1000/1475]   Time 0.156 (0.140)   Data 0.125 (0.107)   Loss 0.3396 (0.3542)   Prec@1 84.375 (84.884)   Prec@5 84.375 (84.884)   [2018-04-01 00:46:59]
  Epoch: [114][1200/1475]   Time 0.136 (0.140)   Data 0.098 (0.107)   Loss 0.3972 (0.3556)   Prec@1 81.250 (84.742)   Prec@5 81.250 (84.742)   [2018-04-01 00:47:28]
  Epoch: [114][1400/1475]   Time 0.137 (0.140)   Data 0.106 (0.107)   Loss 0.3139 (0.3559)   Prec@1 90.625 (84.683)   Prec@5 90.625 (84.683)   [2018-04-01 00:47:55]
  **Train** Prec@1 84.707 Prec@5 84.707 Error@1 15.293
  **VAL** Prec@1 87.665 Prec@5 87.665 Error@1 12.335

==>>[2018-04-01 00:48:23] [Epoch=115/250] [Need: 08:24:21] [learning_rate=0.0020] [Best : Accuracy=87.85, Error=12.15]

==>>Epoch=[115/250]], [2018-04-01 00:48:23], LR=[0.002], Batch=[32] [Model=SimpleNet]
  Epoch: [115][000/1475]   Time 0.145 (0.145)   Data 0.098 (0.098)   Loss 0.2058 (0.2058)   Prec@1 90.625 (90.625)   Prec@5 90.625 (90.625)   [2018-04-01 00:48:24]
  Epoch: [115][200/1475]   Time 0.141 (0.139)   Data 0.109 (0.106)   Loss 0.2920 (0.3589)   Prec@1 84.375 (84.670)   Prec@5 84.375 (84.670)   [2018-04-01 00:48:51]
  Epoch: [115][400/1475]   Time 0.145 (0.139)   Data 0.114 (0.106)   Loss 0.4169 (0.3591)   Prec@1 84.375 (84.624)   Prec@5 84.375 (84.624)   [2018-04-01 00:49:19]
  Epoch: [115][600/1475]   Time 0.125 (0.139)   Data 0.094 (0.106)   Loss 0.2732 (0.3570)   Prec@1 84.375 (84.718)   Prec@5 84.375 (84.718)   [2018-04-01 00:49:47]
  Epoch: [115][800/1475]   Time 0.129 (0.139)   Data 0.098 (0.106)   Loss 0.3605 (0.3582)   Prec@1 87.500 (84.605)   Prec@5 87.500 (84.605)   [2018-04-01 00:50:15]
  Epoch: [115][1000/1475]   Time 0.125 (0.139)   Data 0.094 (0.106)   Loss 0.7063 (0.3595)   Prec@1 71.875 (84.528)   Prec@5 71.875 (84.528)   [2018-04-01 00:50:42]
  Epoch: [115][1200/1475]   Time 0.143 (0.139)   Data 0.114 (0.106)   Loss 0.3640 (0.3568)   Prec@1 81.250 (84.677)   Prec@5 81.250 (84.677)   [2018-04-01 00:51:10]
  Epoch: [115][1400/1475]   Time 0.141 (0.139)   Data 0.109 (0.106)   Loss 0.5321 (0.3565)   Prec@1 78.125 (84.701)   Prec@5 78.125 (84.701)   [2018-04-01 00:51:38]
  **Train** Prec@1 84.733 Prec@5 84.733 Error@1 15.267
  **VAL** Prec@1 87.437 Prec@5 87.437 Error@1 12.563

==>>[2018-04-01 00:52:06] [Epoch=116/250] [Need: 08:20:35] [learning_rate=0.0020] [Best : Accuracy=87.85, Error=12.15]

==>>Epoch=[116/250]], [2018-04-01 00:52:06], LR=[0.002], Batch=[32] [Model=SimpleNet]
  Epoch: [116][000/1475]   Time 0.146 (0.146)   Data 0.115 (0.115)   Loss 0.3397 (0.3397)   Prec@1 84.375 (84.375)   Prec@5 84.375 (84.375)   [2018-04-01 00:52:06]
  Epoch: [116][200/1475]   Time 0.147 (0.139)   Data 0.116 (0.106)   Loss 0.4103 (0.3514)   Prec@1 81.250 (84.873)   Prec@5 81.250 (84.873)   [2018-04-01 00:52:34]
  Epoch: [116][400/1475]   Time 0.139 (0.139)   Data 0.095 (0.106)   Loss 0.3212 (0.3503)   Prec@1 84.375 (85.170)   Prec@5 84.375 (85.170)   [2018-04-01 00:53:02]
  Epoch: [116][600/1475]   Time 0.130 (0.139)   Data 0.099 (0.106)   Loss 0.2065 (0.3525)   Prec@1 90.625 (84.942)   Prec@5 90.625 (84.942)   [2018-04-01 00:53:29]
  Epoch: [116][800/1475]   Time 0.144 (0.139)   Data 0.097 (0.106)   Loss 0.4732 (0.3538)   Prec@1 81.250 (84.847)   Prec@5 81.250 (84.847)   [2018-04-01 00:53:57]
  Epoch: [116][1000/1475]   Time 0.144 (0.139)   Data 0.113 (0.106)   Loss 0.2306 (0.3552)   Prec@1 93.750 (84.812)   Prec@5 93.750 (84.812)   [2018-04-01 00:54:25]
  Epoch: [116][1200/1475]   Time 0.133 (0.139)   Data 0.102 (0.106)   Loss 0.3414 (0.3547)   Prec@1 87.500 (84.856)   Prec@5 87.500 (84.856)   [2018-04-01 00:54:53]
  Epoch: [116][1400/1475]   Time 0.141 (0.139)   Data 0.109 (0.106)   Loss 0.2446 (0.3576)   Prec@1 93.750 (84.678)   Prec@5 93.750 (84.678)   [2018-04-01 00:55:20]
  **Train** Prec@1 84.709 Prec@5 84.709 Error@1 15.291
  **VAL** Prec@1 87.569 Prec@5 87.569 Error@1 12.431

==>>[2018-04-01 00:55:49] [Epoch=117/250] [Need: 08:16:49] [learning_rate=0.0020] [Best : Accuracy=87.85, Error=12.15]

==>>Epoch=[117/250]], [2018-04-01 00:55:49], LR=[0.002], Batch=[32] [Model=SimpleNet]
  Epoch: [117][000/1475]   Time 0.137 (0.137)   Data 0.094 (0.094)   Loss 0.3533 (0.3533)   Prec@1 84.375 (84.375)   Prec@5 84.375 (84.375)   [2018-04-01 00:55:49]
  Epoch: [117][200/1475]   Time 0.128 (0.139)   Data 0.097 (0.106)   Loss 0.4021 (0.3595)   Prec@1 71.875 (84.375)   Prec@5 71.875 (84.375)   [2018-04-01 00:56:16]
  Epoch: [117][400/1475]   Time 0.141 (0.139)   Data 0.109 (0.106)   Loss 0.1807 (0.3598)   Prec@1 96.875 (84.367)   Prec@5 96.875 (84.367)   [2018-04-01 00:56:44]
  Epoch: [117][600/1475]   Time 0.141 (0.139)   Data 0.109 (0.106)   Loss 0.3914 (0.3603)   Prec@1 90.625 (84.323)   Prec@5 90.625 (84.323)   [2018-04-01 00:57:12]
  Epoch: [117][800/1475]   Time 0.141 (0.139)   Data 0.109 (0.105)   Loss 0.2140 (0.3603)   Prec@1 93.750 (84.441)   Prec@5 93.750 (84.441)   [2018-04-01 00:57:40]
  Epoch: [117][1000/1475]   Time 0.141 (0.139)   Data 0.109 (0.106)   Loss 0.3465 (0.3597)   Prec@1 81.250 (84.537)   Prec@5 81.250 (84.537)   [2018-04-01 00:58:07]
  Epoch: [117][1200/1475]   Time 0.144 (0.139)   Data 0.109 (0.106)   Loss 0.4320 (0.3586)   Prec@1 87.500 (84.627)   Prec@5 87.500 (84.627)   [2018-04-01 00:58:35]
  Epoch: [117][1400/1475]   Time 0.129 (0.139)   Data 0.094 (0.106)   Loss 0.3203 (0.3588)   Prec@1 87.500 (84.643)   Prec@5 87.500 (84.643)   [2018-04-01 00:59:03]
  **Train** Prec@1 84.612 Prec@5 84.612 Error@1 15.388
  **VAL** Prec@1 87.737 Prec@5 87.737 Error@1 12.263

==>>[2018-04-01 00:59:31] [Epoch=118/250] [Need: 08:13:03] [learning_rate=0.0020] [Best : Accuracy=87.85, Error=12.15]

==>>Epoch=[118/250]], [2018-04-01 00:59:31], LR=[0.002], Batch=[32] [Model=SimpleNet]
  Epoch: [118][000/1475]   Time 0.125 (0.125)   Data 0.093 (0.093)   Loss 0.4262 (0.4262)   Prec@1 78.125 (78.125)   Prec@5 78.125 (78.125)   [2018-04-01 00:59:31]
  Epoch: [118][200/1475]   Time 0.127 (0.138)   Data 0.096 (0.105)   Loss 0.3037 (0.3584)   Prec@1 87.500 (84.810)   Prec@5 87.500 (84.810)   [2018-04-01 00:59:59]
  Epoch: [118][400/1475]   Time 0.141 (0.139)   Data 0.109 (0.105)   Loss 0.2611 (0.3612)   Prec@1 90.625 (84.632)   Prec@5 90.625 (84.632)   [2018-04-01 01:00:27]
  Epoch: [118][600/1475]   Time 0.147 (0.139)   Data 0.116 (0.105)   Loss 0.4329 (0.3609)   Prec@1 84.375 (84.609)   Prec@5 84.375 (84.609)   [2018-04-01 01:00:54]
  Epoch: [118][800/1475]   Time 0.129 (0.139)   Data 0.098 (0.105)   Loss 0.4825 (0.3595)   Prec@1 84.375 (84.699)   Prec@5 84.375 (84.699)   [2018-04-01 01:01:22]
  Epoch: [118][1000/1475]   Time 0.141 (0.139)   Data 0.109 (0.105)   Loss 0.2509 (0.3592)   Prec@1 90.625 (84.771)   Prec@5 90.625 (84.771)   [2018-04-01 01:01:50]
  Epoch: [118][1200/1475]   Time 0.144 (0.139)   Data 0.119 (0.105)   Loss 0.4408 (0.3592)   Prec@1 78.125 (84.755)   Prec@5 78.125 (84.755)   [2018-04-01 01:02:18]
  Epoch: [118][1400/1475]   Time 0.133 (0.139)   Data 0.111 (0.105)   Loss 0.3104 (0.3592)   Prec@1 84.375 (84.748)   Prec@5 84.375 (84.748)   [2018-04-01 01:02:45]
  **Train** Prec@1 84.758 Prec@5 84.758 Error@1 15.242
  **VAL** Prec@1 87.473 Prec@5 87.473 Error@1 12.527

==>>[2018-04-01 01:03:14] [Epoch=119/250] [Need: 08:09:18] [learning_rate=0.0020] [Best : Accuracy=87.85, Error=12.15]

==>>Epoch=[119/250]], [2018-04-01 01:03:14], LR=[0.002], Batch=[32] [Model=SimpleNet]
  Epoch: [119][000/1475]   Time 0.142 (0.142)   Data 0.111 (0.111)   Loss 0.4788 (0.4788)   Prec@1 78.125 (78.125)   Prec@5 78.125 (78.125)   [2018-04-01 01:03:14]
  Epoch: [119][200/1475]   Time 0.141 (0.139)   Data 0.109 (0.106)   Loss 0.4704 (0.3633)   Prec@1 78.125 (84.422)   Prec@5 78.125 (84.422)   [2018-04-01 01:03:42]
  Epoch: [119][400/1475]   Time 0.141 (0.139)   Data 0.109 (0.105)   Loss 0.2279 (0.3575)   Prec@1 93.750 (84.866)   Prec@5 93.750 (84.866)   [2018-04-01 01:04:09]
  Epoch: [119][600/1475]   Time 0.145 (0.139)   Data 0.112 (0.106)   Loss 0.2915 (0.3508)   Prec@1 90.625 (85.243)   Prec@5 90.625 (85.243)   [2018-04-01 01:04:37]
  Epoch: [119][800/1475]   Time 0.145 (0.139)   Data 0.109 (0.105)   Loss 0.3231 (0.3541)   Prec@1 90.625 (85.054)   Prec@5 90.625 (85.054)   [2018-04-01 01:05:05]
  Epoch: [119][1000/1475]   Time 0.144 (0.139)   Data 0.113 (0.105)   Loss 0.4921 (0.3545)   Prec@1 71.875 (85.012)   Prec@5 71.875 (85.012)   [2018-04-01 01:05:33]
  Epoch: [119][1200/1475]   Time 0.125 (0.139)   Data 0.094 (0.106)   Loss 0.3804 (0.3560)   Prec@1 78.125 (84.953)   Prec@5 78.125 (84.953)   [2018-04-01 01:06:00]
  Epoch: [119][1400/1475]   Time 0.128 (0.139)   Data 0.110 (0.106)   Loss 0.2555 (0.3556)   Prec@1 93.750 (84.944)   Prec@5 93.750 (84.944)   [2018-04-01 01:06:28]
  **Train** Prec@1 84.976 Prec@5 84.976 Error@1 15.024
  **VAL** Prec@1 87.581 Prec@5 87.581 Error@1 12.419

==>>[2018-04-01 01:06:56] [Epoch=120/250] [Need: 08:05:32] [learning_rate=0.0020] [Best : Accuracy=87.85, Error=12.15]

==>>Epoch=[120/250]], [2018-04-01 01:06:56], LR=[0.002], Batch=[32] [Model=SimpleNet]
  Epoch: [120][000/1475]   Time 0.125 (0.125)   Data 0.094 (0.094)   Loss 0.2384 (0.2384)   Prec@1 90.625 (90.625)   Prec@5 90.625 (90.625)   [2018-04-01 01:06:56]
  Epoch: [120][200/1475]   Time 0.134 (0.139)   Data 0.103 (0.105)   Loss 0.2018 (0.3611)   Prec@1 96.875 (84.826)   Prec@5 96.875 (84.826)   [2018-04-01 01:07:24]
  Epoch: [120][400/1475]   Time 0.141 (0.139)   Data 0.109 (0.106)   Loss 0.1853 (0.3538)   Prec@1 93.750 (84.921)   Prec@5 93.750 (84.921)   [2018-04-01 01:07:52]
  Epoch: [120][600/1475]   Time 0.125 (0.139)   Data 0.094 (0.106)   Loss 0.2014 (0.3533)   Prec@1 93.750 (84.916)   Prec@5 93.750 (84.916)   [2018-04-01 01:08:20]
  Epoch: [120][800/1475]   Time 0.130 (0.139)   Data 0.098 (0.106)   Loss 0.4805 (0.3567)   Prec@1 65.625 (84.777)   Prec@5 65.625 (84.777)   [2018-04-01 01:08:47]
  Epoch: [120][1000/1475]   Time 0.145 (0.139)   Data 0.111 (0.105)   Loss 0.3743 (0.3576)   Prec@1 87.500 (84.690)   Prec@5 87.500 (84.690)   [2018-04-01 01:09:15]
  Epoch: [120][1200/1475]   Time 0.141 (0.139)   Data 0.109 (0.105)   Loss 0.3625 (0.3565)   Prec@1 81.250 (84.815)   Prec@5 81.250 (84.815)   [2018-04-01 01:09:43]
  Epoch: [120][1400/1475]   Time 0.142 (0.139)   Data 0.109 (0.105)   Loss 0.3891 (0.3572)   Prec@1 81.250 (84.774)   Prec@5 81.250 (84.774)   [2018-04-01 01:10:11]
  **Train** Prec@1 84.756 Prec@5 84.756 Error@1 15.244
  **VAL** Prec@1 87.665 Prec@5 87.665 Error@1 12.335

==>>[2018-04-01 01:10:39] [Epoch=121/250] [Need: 08:01:46] [learning_rate=0.0020] [Best : Accuracy=87.85, Error=12.15]

==>>Epoch=[121/250]], [2018-04-01 01:10:39], LR=[0.002], Batch=[32] [Model=SimpleNet]
  Epoch: [121][000/1475]   Time 0.141 (0.141)   Data 0.094 (0.094)   Loss 0.3413 (0.3413)   Prec@1 90.625 (90.625)   Prec@5 90.625 (90.625)   [2018-04-01 01:10:39]
  Epoch: [121][200/1475]   Time 0.141 (0.139)   Data 0.094 (0.105)   Loss 0.4990 (0.3526)   Prec@1 78.125 (85.137)   Prec@5 78.125 (85.137)   [2018-04-01 01:11:07]
  Epoch: [121][400/1475]   Time 0.151 (0.139)   Data 0.118 (0.105)   Loss 0.2705 (0.3534)   Prec@1 87.500 (84.874)   Prec@5 87.500 (84.874)   [2018-04-01 01:11:34]
  Epoch: [121][600/1475]   Time 0.130 (0.139)   Data 0.094 (0.105)   Loss 0.3437 (0.3553)   Prec@1 84.375 (84.817)   Prec@5 84.375 (84.817)   [2018-04-01 01:12:02]
  Epoch: [121][800/1475]   Time 0.125 (0.139)   Data 0.094 (0.106)   Loss 0.3630 (0.3587)   Prec@1 87.500 (84.597)   Prec@5 87.500 (84.597)   [2018-04-01 01:12:30]
  Epoch: [121][1000/1475]   Time 0.141 (0.139)   Data 0.109 (0.105)   Loss 0.4140 (0.3567)   Prec@1 81.250 (84.594)   Prec@5 81.250 (84.594)   [2018-04-01 01:12:58]
  Epoch: [121][1200/1475]   Time 0.127 (0.139)   Data 0.095 (0.105)   Loss 0.4149 (0.3568)   Prec@1 84.375 (84.599)   Prec@5 84.375 (84.599)   [2018-04-01 01:13:25]
  Epoch: [121][1400/1475]   Time 0.141 (0.139)   Data 0.109 (0.105)   Loss 0.2461 (0.3568)   Prec@1 87.500 (84.721)   Prec@5 87.500 (84.721)   [2018-04-01 01:13:53]
  **Train** Prec@1 84.707 Prec@5 84.707 Error@1 15.293
  **VAL** Prec@1 86.824 Prec@5 86.824 Error@1 13.176

==>>[2018-04-01 01:14:21] [Epoch=122/250] [Need: 07:58:00] [learning_rate=0.0020] [Best : Accuracy=87.85, Error=12.15]

==>>Epoch=[122/250]], [2018-04-01 01:14:21], LR=[0.002], Batch=[32] [Model=SimpleNet]
  Epoch: [122][000/1475]   Time 0.141 (0.141)   Data 0.109 (0.109)   Loss 0.4581 (0.4581)   Prec@1 71.875 (71.875)   Prec@5 71.875 (71.875)   [2018-04-01 01:14:21]
  Epoch: [122][200/1475]   Time 0.141 (0.139)   Data 0.109 (0.106)   Loss 0.3069 (0.3621)   Prec@1 84.375 (84.173)   Prec@5 84.375 (84.173)   [2018-04-01 01:14:49]
  Epoch: [122][400/1475]   Time 0.134 (0.139)   Data 0.094 (0.105)   Loss 0.4119 (0.3574)   Prec@1 90.625 (84.663)   Prec@5 90.625 (84.663)   [2018-04-01 01:15:17]
  Epoch: [122][600/1475]   Time 0.141 (0.139)   Data 0.109 (0.106)   Loss 0.1685 (0.3554)   Prec@1 93.750 (84.791)   Prec@5 93.750 (84.791)   [2018-04-01 01:15:45]
  Epoch: [122][800/1475]   Time 0.135 (0.139)   Data 0.103 (0.106)   Loss 0.4023 (0.3566)   Prec@1 81.250 (84.785)   Prec@5 81.250 (84.785)   [2018-04-01 01:16:12]
  Epoch: [122][1000/1475]   Time 0.141 (0.139)   Data 0.094 (0.106)   Loss 0.3116 (0.3561)   Prec@1 87.500 (84.812)   Prec@5 87.500 (84.812)   [2018-04-01 01:16:40]
  Epoch: [122][1200/1475]   Time 0.141 (0.139)   Data 0.109 (0.105)   Loss 0.1991 (0.3556)   Prec@1 93.750 (84.893)   Prec@5 93.750 (84.893)   [2018-04-01 01:17:08]
  Epoch: [122][1400/1475]   Time 0.141 (0.139)   Data 0.094 (0.105)   Loss 0.4823 (0.3556)   Prec@1 81.250 (84.904)   Prec@5 81.250 (84.904)   [2018-04-01 01:17:35]
  **Train** Prec@1 84.940 Prec@5 84.940 Error@1 15.060
  **VAL** Prec@1 87.713 Prec@5 87.713 Error@1 12.287

==>>[2018-04-01 01:18:04] [Epoch=123/250] [Need: 07:54:14] [learning_rate=0.0020] [Best : Accuracy=87.85, Error=12.15]

==>>Epoch=[123/250]], [2018-04-01 01:18:04], LR=[0.002], Batch=[32] [Model=SimpleNet]
  Epoch: [123][000/1475]   Time 0.125 (0.125)   Data 0.094 (0.094)   Loss 0.4380 (0.4380)   Prec@1 75.000 (75.000)   Prec@5 75.000 (75.000)   [2018-04-01 01:18:04]
  Epoch: [123][200/1475]   Time 0.141 (0.139)   Data 0.094 (0.105)   Loss 0.4266 (0.3494)   Prec@1 81.250 (85.183)   Prec@5 81.250 (85.183)   [2018-04-01 01:18:32]
  Epoch: [123][400/1475]   Time 0.141 (0.139)   Data 0.094 (0.105)   Loss 0.4373 (0.3492)   Prec@1 81.250 (85.115)   Prec@5 81.250 (85.115)   [2018-04-01 01:18:59]
  Epoch: [123][600/1475]   Time 0.145 (0.139)   Data 0.113 (0.105)   Loss 0.4615 (0.3534)   Prec@1 75.000 (84.687)   Prec@5 75.000 (84.687)   [2018-04-01 01:19:27]
  Epoch: [123][800/1475]   Time 0.141 (0.139)   Data 0.110 (0.105)   Loss 0.2593 (0.3527)   Prec@1 90.625 (84.902)   Prec@5 90.625 (84.902)   [2018-04-01 01:19:55]
  Epoch: [123][1000/1475]   Time 0.149 (0.139)   Data 0.118 (0.105)   Loss 0.2416 (0.3556)   Prec@1 87.500 (84.815)   Prec@5 87.500 (84.815)   [2018-04-01 01:20:22]
  Epoch: [123][1200/1475]   Time 0.150 (0.139)   Data 0.117 (0.105)   Loss 0.3099 (0.3539)   Prec@1 90.625 (84.924)   Prec@5 90.625 (84.924)   [2018-04-01 01:20:50]
  Epoch: [123][1400/1475]   Time 0.141 (0.139)   Data 0.109 (0.105)   Loss 0.3327 (0.3559)   Prec@1 84.375 (84.790)   Prec@5 84.375 (84.790)   [2018-04-01 01:21:18]
  **Train** Prec@1 84.811 Prec@5 84.811 Error@1 15.189
  **VAL** Prec@1 88.074 Prec@5 88.074 Error@1 11.926

==>>[2018-04-01 01:21:46] [Epoch=124/250] [Need: 07:50:29] [learning_rate=0.0020] [Best : Accuracy=88.07, Error=11.93]

==>>Epoch=[124/250]], [2018-04-01 01:21:46], LR=[0.002], Batch=[32] [Model=SimpleNet]
  Epoch: [124][000/1475]   Time 0.145 (0.145)   Data 0.113 (0.113)   Loss 0.3241 (0.3241)   Prec@1 93.750 (93.750)   Prec@5 93.750 (93.750)   [2018-04-01 01:21:46]
  Epoch: [124][200/1475]   Time 0.141 (0.139)   Data 0.109 (0.106)   Loss 0.1987 (0.3498)   Prec@1 93.750 (85.044)   Prec@5 93.750 (85.044)   [2018-04-01 01:22:14]
  Epoch: [124][400/1475]   Time 0.147 (0.139)   Data 0.109 (0.106)   Loss 0.5313 (0.3457)   Prec@1 78.125 (85.458)   Prec@5 78.125 (85.458)   [2018-04-01 01:22:42]
  Epoch: [124][600/1475]   Time 0.148 (0.139)   Data 0.115 (0.106)   Loss 0.3045 (0.3511)   Prec@1 84.375 (85.176)   Prec@5 84.375 (85.176)   [2018-04-01 01:23:09]
  Epoch: [124][800/1475]   Time 0.141 (0.139)   Data 0.109 (0.106)   Loss 0.4236 (0.3506)   Prec@1 78.125 (85.147)   Prec@5 78.125 (85.147)   [2018-04-01 01:23:37]
  Epoch: [124][1000/1475]   Time 0.145 (0.139)   Data 0.114 (0.106)   Loss 0.4881 (0.3520)   Prec@1 78.125 (85.037)   Prec@5 78.125 (85.037)   [2018-04-01 01:24:05]
  Epoch: [124][1200/1475]   Time 0.140 (0.139)   Data 0.107 (0.106)   Loss 0.2792 (0.3561)   Prec@1 87.500 (84.750)   Prec@5 87.500 (84.750)   [2018-04-01 01:24:32]
  Epoch: [124][1400/1475]   Time 0.139 (0.139)   Data 0.106 (0.106)   Loss 0.3577 (0.3557)   Prec@1 81.250 (84.837)   Prec@5 81.250 (84.837)   [2018-04-01 01:25:00]
  **Train** Prec@1 84.824 Prec@5 84.824 Error@1 15.176
  **VAL** Prec@1 87.737 Prec@5 87.737 Error@1 12.263

==>>[2018-04-01 01:25:28] [Epoch=125/250] [Need: 07:46:43] [learning_rate=0.0020] [Best : Accuracy=88.07, Error=11.93]

==>>Epoch=[125/250]], [2018-04-01 01:25:28], LR=[0.002], Batch=[32] [Model=SimpleNet]
  Epoch: [125][000/1475]   Time 0.126 (0.126)   Data 0.095 (0.095)   Loss 0.4089 (0.4089)   Prec@1 81.250 (81.250)   Prec@5 81.250 (81.250)   [2018-04-01 01:25:28]
  Epoch: [125][200/1475]   Time 0.145 (0.139)   Data 0.114 (0.106)   Loss 0.2209 (0.3500)   Prec@1 87.500 (85.183)   Prec@5 87.500 (85.183)   [2018-04-01 01:25:56]
  Epoch: [125][400/1475]   Time 0.139 (0.139)   Data 0.108 (0.106)   Loss 0.3487 (0.3508)   Prec@1 81.250 (85.217)   Prec@5 81.250 (85.217)   [2018-04-01 01:26:24]
  Epoch: [125][600/1475]   Time 0.144 (0.139)   Data 0.113 (0.105)   Loss 0.4875 (0.3556)   Prec@1 75.000 (84.947)   Prec@5 75.000 (84.947)   [2018-04-01 01:26:52]
  Epoch: [125][800/1475]   Time 0.125 (0.139)   Data 0.094 (0.105)   Loss 0.3762 (0.3593)   Prec@1 90.625 (84.831)   Prec@5 90.625 (84.831)   [2018-04-01 01:27:19]
  Epoch: [125][1000/1475]   Time 0.132 (0.139)   Data 0.101 (0.105)   Loss 0.4108 (0.3572)   Prec@1 87.500 (84.850)   Prec@5 87.500 (84.850)   [2018-04-01 01:27:47]
  Epoch: [125][1200/1475]   Time 0.138 (0.139)   Data 0.091 (0.105)   Loss 0.3014 (0.3569)   Prec@1 87.500 (84.846)   Prec@5 87.500 (84.846)   [2018-04-01 01:28:15]
  Epoch: [125][1400/1475]   Time 0.141 (0.139)   Data 0.094 (0.105)   Loss 0.4320 (0.3583)   Prec@1 75.000 (84.823)   Prec@5 75.000 (84.823)   [2018-04-01 01:28:43]
  **Train** Prec@1 84.828 Prec@5 84.828 Error@1 15.172
  **VAL** Prec@1 87.389 Prec@5 87.389 Error@1 12.611

==>>[2018-04-01 01:29:11] [Epoch=126/250] [Need: 07:42:57] [learning_rate=0.0020] [Best : Accuracy=88.07, Error=11.93]

==>>Epoch=[126/250]], [2018-04-01 01:29:11], LR=[0.002], Batch=[32] [Model=SimpleNet]
  Epoch: [126][000/1475]   Time 0.138 (0.138)   Data 0.094 (0.094)   Loss 0.2329 (0.2329)   Prec@1 87.500 (87.500)   Prec@5 87.500 (87.500)   [2018-04-01 01:29:11]
  Epoch: [126][200/1475]   Time 0.141 (0.139)   Data 0.109 (0.105)   Loss 0.3708 (0.3643)   Prec@1 78.125 (84.717)   Prec@5 78.125 (84.717)   [2018-04-01 01:29:39]
  Epoch: [126][400/1475]   Time 0.153 (0.139)   Data 0.119 (0.105)   Loss 0.5833 (0.3581)   Prec@1 84.375 (84.788)   Prec@5 84.375 (84.788)   [2018-04-01 01:30:06]
  Epoch: [126][600/1475]   Time 0.141 (0.139)   Data 0.109 (0.105)   Loss 0.4426 (0.3587)   Prec@1 75.000 (84.645)   Prec@5 75.000 (84.645)   [2018-04-01 01:30:34]
  Epoch: [126][800/1475]   Time 0.130 (0.139)   Data 0.098 (0.105)   Loss 0.2028 (0.3607)   Prec@1 93.750 (84.593)   Prec@5 93.750 (84.593)   [2018-04-01 01:31:02]
  Epoch: [126][1000/1475]   Time 0.141 (0.139)   Data 0.109 (0.105)   Loss 0.2909 (0.3584)   Prec@1 87.500 (84.650)   Prec@5 87.500 (84.650)   [2018-04-01 01:31:29]
  Epoch: [126][1200/1475]   Time 0.146 (0.139)   Data 0.116 (0.105)   Loss 0.3134 (0.3565)   Prec@1 90.625 (84.791)   Prec@5 90.625 (84.791)   [2018-04-01 01:31:57]
  Epoch: [126][1400/1475]   Time 0.151 (0.139)   Data 0.114 (0.105)   Loss 0.5609 (0.3582)   Prec@1 78.125 (84.685)   Prec@5 78.125 (84.685)   [2018-04-01 01:32:25]
  **Train** Prec@1 84.656 Prec@5 84.656 Error@1 15.344
  **VAL** Prec@1 87.161 Prec@5 87.161 Error@1 12.839

==>>[2018-04-01 01:32:53] [Epoch=127/250] [Need: 07:39:12] [learning_rate=0.0020] [Best : Accuracy=88.07, Error=11.93]

==>>Epoch=[127/250]], [2018-04-01 01:32:53], LR=[0.002], Batch=[32] [Model=SimpleNet]
  Epoch: [127][000/1475]   Time 0.145 (0.145)   Data 0.098 (0.098)   Loss 0.2927 (0.2927)   Prec@1 84.375 (84.375)   Prec@5 84.375 (84.375)   [2018-04-01 01:32:53]
  Epoch: [127][200/1475]   Time 0.130 (0.139)   Data 0.099 (0.105)   Loss 0.4971 (0.3522)   Prec@1 75.000 (84.748)   Prec@5 75.000 (84.748)   [2018-04-01 01:33:21]
  Epoch: [127][400/1475]   Time 0.145 (0.139)   Data 0.114 (0.105)   Loss 0.3065 (0.3556)   Prec@1 84.375 (84.640)   Prec@5 84.375 (84.640)   [2018-04-01 01:33:49]
  Epoch: [127][600/1475]   Time 0.135 (0.138)   Data 0.104 (0.105)   Loss 0.2597 (0.3543)   Prec@1 84.375 (84.869)   Prec@5 84.375 (84.869)   [2018-04-01 01:34:16]
  Epoch: [127][800/1475]   Time 0.149 (0.138)   Data 0.118 (0.105)   Loss 0.2449 (0.3553)   Prec@1 90.625 (84.699)   Prec@5 90.625 (84.699)   [2018-04-01 01:34:44]
  Epoch: [127][1000/1475]   Time 0.134 (0.138)   Data 0.103 (0.105)   Loss 0.1998 (0.3569)   Prec@1 93.750 (84.647)   Prec@5 93.750 (84.647)   [2018-04-01 01:35:12]
  Epoch: [127][1200/1475]   Time 0.129 (0.138)   Data 0.097 (0.105)   Loss 0.3019 (0.3540)   Prec@1 90.625 (84.828)   Prec@5 90.625 (84.828)   [2018-04-01 01:35:39]
  Epoch: [127][1400/1475]   Time 0.125 (0.139)   Data 0.094 (0.105)   Loss 0.4441 (0.3537)   Prec@1 75.000 (84.872)   Prec@5 75.000 (84.872)   [2018-04-01 01:36:07]
  **Train** Prec@1 84.817 Prec@5 84.817 Error@1 15.183
  **VAL** Prec@1 87.317 Prec@5 87.317 Error@1 12.683

==>>[2018-04-01 01:36:35] [Epoch=128/250] [Need: 07:35:26] [learning_rate=0.0020] [Best : Accuracy=88.07, Error=11.93]

==>>Epoch=[128/250]], [2018-04-01 01:36:35], LR=[0.002], Batch=[32] [Model=SimpleNet]
  Epoch: [128][000/1475]   Time 0.144 (0.144)   Data 0.111 (0.111)   Loss 0.1741 (0.1741)   Prec@1 96.875 (96.875)   Prec@5 96.875 (96.875)   [2018-04-01 01:36:35]
  Epoch: [128][200/1475]   Time 0.131 (0.139)   Data 0.094 (0.105)   Loss 0.3153 (0.3527)   Prec@1 81.250 (85.059)   Prec@5 81.250 (85.059)   [2018-04-01 01:37:03]
  Epoch: [128][400/1475]   Time 0.141 (0.139)   Data 0.109 (0.105)   Loss 0.2419 (0.3531)   Prec@1 90.625 (85.061)   Prec@5 90.625 (85.061)   [2018-04-01 01:37:31]
  Epoch: [128][600/1475]   Time 0.141 (0.139)   Data 0.109 (0.105)   Loss 0.2582 (0.3554)   Prec@1 93.750 (84.937)   Prec@5 93.750 (84.937)   [2018-04-01 01:37:59]
  Epoch: [128][800/1475]   Time 0.141 (0.139)   Data 0.109 (0.105)   Loss 0.4736 (0.3527)   Prec@1 75.000 (85.054)   Prec@5 75.000 (85.054)   [2018-04-01 01:38:26]
  Epoch: [128][1000/1475]   Time 0.141 (0.139)   Data 0.109 (0.105)   Loss 0.3911 (0.3520)   Prec@1 78.125 (85.006)   Prec@5 78.125 (85.006)   [2018-04-01 01:38:54]
  Epoch: [128][1200/1475]   Time 0.141 (0.139)   Data 0.109 (0.105)   Loss 0.1929 (0.3540)   Prec@1 93.750 (84.880)   Prec@5 93.750 (84.880)   [2018-04-01 01:39:22]
  Epoch: [128][1400/1475]   Time 0.141 (0.139)   Data 0.109 (0.105)   Loss 0.4745 (0.3541)   Prec@1 78.125 (84.808)   Prec@5 78.125 (84.808)   [2018-04-01 01:39:50]
  **Train** Prec@1 84.754 Prec@5 84.754 Error@1 15.246
  **VAL** Prec@1 87.425 Prec@5 87.425 Error@1 12.575

==>>[2018-04-01 01:40:18] [Epoch=129/250] [Need: 07:31:40] [learning_rate=0.0020] [Best : Accuracy=88.07, Error=11.93]

==>>Epoch=[129/250]], [2018-04-01 01:40:18], LR=[0.002], Batch=[32] [Model=SimpleNet]
  Epoch: [129][000/1475]   Time 0.125 (0.125)   Data 0.094 (0.094)   Loss 0.4064 (0.4064)   Prec@1 84.375 (84.375)   Prec@5 84.375 (84.375)   [2018-04-01 01:40:18]
  Epoch: [129][200/1475]   Time 0.145 (0.138)   Data 0.114 (0.105)   Loss 0.2870 (0.3516)   Prec@1 87.500 (85.199)   Prec@5 87.500 (85.199)   [2018-04-01 01:40:45]
  Epoch: [129][400/1475]   Time 0.133 (0.138)   Data 0.105 (0.106)   Loss 0.4524 (0.3534)   Prec@1 81.250 (85.162)   Prec@5 81.250 (85.162)   [2018-04-01 01:41:13]
  Epoch: [129][600/1475]   Time 0.131 (0.138)   Data 0.100 (0.106)   Loss 0.5159 (0.3576)   Prec@1 75.000 (84.895)   Prec@5 75.000 (84.895)   [2018-04-01 01:41:41]
  Epoch: [129][800/1475]   Time 0.141 (0.139)   Data 0.109 (0.106)   Loss 0.5052 (0.3568)   Prec@1 78.125 (84.972)   Prec@5 78.125 (84.972)   [2018-04-01 01:42:09]
  Epoch: [129][1000/1475]   Time 0.141 (0.139)   Data 0.109 (0.106)   Loss 0.2438 (0.3567)   Prec@1 90.625 (85.074)   Prec@5 90.625 (85.074)   [2018-04-01 01:42:36]
  Epoch: [129][1200/1475]   Time 0.137 (0.139)   Data 0.090 (0.106)   Loss 0.4041 (0.3546)   Prec@1 75.000 (85.083)   Prec@5 75.000 (85.083)   [2018-04-01 01:43:04]
  Epoch: [129][1400/1475]   Time 0.141 (0.139)   Data 0.109 (0.106)   Loss 0.2569 (0.3545)   Prec@1 96.875 (85.111)   Prec@5 96.875 (85.111)   [2018-04-01 01:43:32]
  **Train** Prec@1 85.055 Prec@5 85.055 Error@1 14.945
  **VAL** Prec@1 87.593 Prec@5 87.593 Error@1 12.407

==>>[2018-04-01 01:44:00] [Epoch=130/250] [Need: 07:27:55] [learning_rate=0.0020] [Best : Accuracy=88.07, Error=11.93]

==>>Epoch=[130/250]], [2018-04-01 01:44:00], LR=[0.002], Batch=[32] [Model=SimpleNet]
  Epoch: [130][000/1475]   Time 0.141 (0.141)   Data 0.109 (0.109)   Loss 0.3593 (0.3593)   Prec@1 78.125 (78.125)   Prec@5 78.125 (78.125)   [2018-04-01 01:44:00]
  Epoch: [130][200/1475]   Time 0.141 (0.139)   Data 0.094 (0.106)   Loss 0.5013 (0.3609)   Prec@1 81.250 (84.795)   Prec@5 81.250 (84.795)   [2018-04-01 01:44:28]
  Epoch: [130][400/1475]   Time 0.145 (0.139)   Data 0.114 (0.105)   Loss 0.2823 (0.3556)   Prec@1 87.500 (84.624)   Prec@5 87.500 (84.624)   [2018-04-01 01:44:55]
  Epoch: [130][600/1475]   Time 0.148 (0.139)   Data 0.120 (0.105)   Loss 0.2214 (0.3553)   Prec@1 90.625 (84.604)   Prec@5 90.625 (84.604)   [2018-04-01 01:45:23]
  Epoch: [130][800/1475]   Time 0.141 (0.139)   Data 0.109 (0.105)   Loss 0.3142 (0.3579)   Prec@1 87.500 (84.500)   Prec@5 87.500 (84.500)   [2018-04-01 01:45:51]
  Epoch: [130][1000/1475]   Time 0.141 (0.139)   Data 0.109 (0.106)   Loss 0.3018 (0.3574)   Prec@1 87.500 (84.622)   Prec@5 87.500 (84.622)   [2018-04-01 01:46:19]
  Epoch: [130][1200/1475]   Time 0.141 (0.139)   Data 0.094 (0.105)   Loss 0.4591 (0.3573)   Prec@1 81.250 (84.724)   Prec@5 81.250 (84.724)   [2018-04-01 01:46:47]
  Epoch: [130][1400/1475]   Time 0.150 (0.139)   Data 0.103 (0.105)   Loss 0.4397 (0.3567)   Prec@1 84.375 (84.730)   Prec@5 84.375 (84.730)   [2018-04-01 01:47:14]
  **Train** Prec@1 84.671 Prec@5 84.671 Error@1 15.329
  **VAL** Prec@1 86.884 Prec@5 86.884 Error@1 13.116

==>>[2018-04-01 01:47:43] [Epoch=131/250] [Need: 07:24:10] [learning_rate=0.0020] [Best : Accuracy=88.07, Error=11.93]

==>>Epoch=[131/250]], [2018-04-01 01:47:43], LR=[0.002], Batch=[32] [Model=SimpleNet]
  Epoch: [131][000/1475]   Time 0.141 (0.141)   Data 0.094 (0.094)   Loss 0.3860 (0.3860)   Prec@1 84.375 (84.375)   Prec@5 84.375 (84.375)   [2018-04-01 01:47:43]
  Epoch: [131][200/1475]   Time 0.141 (0.139)   Data 0.109 (0.105)   Loss 0.2684 (0.3571)   Prec@1 90.625 (84.873)   Prec@5 90.625 (84.873)   [2018-04-01 01:48:11]
  Epoch: [131][400/1475]   Time 0.141 (0.139)   Data 0.109 (0.105)   Loss 0.2819 (0.3557)   Prec@1 90.625 (84.998)   Prec@5 90.625 (84.998)   [2018-04-01 01:48:38]
  Epoch: [131][600/1475]   Time 0.141 (0.139)   Data 0.109 (0.105)   Loss 0.5052 (0.3604)   Prec@1 75.000 (84.677)   Prec@5 75.000 (84.677)   [2018-04-01 01:49:06]
  Epoch: [131][800/1475]   Time 0.145 (0.139)   Data 0.109 (0.105)   Loss 0.3103 (0.3588)   Prec@1 90.625 (84.664)   Prec@5 90.625 (84.664)   [2018-04-01 01:49:34]
  Epoch: [131][1000/1475]   Time 0.137 (0.139)   Data 0.106 (0.105)   Loss 0.3941 (0.3562)   Prec@1 84.375 (84.803)   Prec@5 84.375 (84.803)   [2018-04-01 01:50:01]
  Epoch: [131][1200/1475]   Time 0.141 (0.139)   Data 0.109 (0.105)   Loss 0.3134 (0.3562)   Prec@1 84.375 (84.877)   Prec@5 84.375 (84.877)   [2018-04-01 01:50:29]
  Epoch: [131][1400/1475]   Time 0.141 (0.139)   Data 0.109 (0.105)   Loss 0.6302 (0.3536)   Prec@1 78.125 (85.006)   Prec@5 78.125 (85.006)   [2018-04-01 01:50:57]
  **Train** Prec@1 85.000 Prec@5 85.000 Error@1 15.000
  **VAL** Prec@1 87.905 Prec@5 87.905 Error@1 12.095

==>>[2018-04-01 01:51:25] [Epoch=132/250] [Need: 07:20:24] [learning_rate=0.0020] [Best : Accuracy=88.07, Error=11.93]

==>>Epoch=[132/250]], [2018-04-01 01:51:25], LR=[0.002], Batch=[32] [Model=SimpleNet]
  Epoch: [132][000/1475]   Time 0.134 (0.134)   Data 0.107 (0.107)   Loss 0.2439 (0.2439)   Prec@1 93.750 (93.750)   Prec@5 93.750 (93.750)   [2018-04-01 01:51:25]
  Epoch: [132][200/1475]   Time 0.129 (0.139)   Data 0.098 (0.105)   Loss 0.3536 (0.3493)   Prec@1 81.250 (84.966)   Prec@5 81.250 (84.966)   [2018-04-01 01:51:53]
  Epoch: [132][400/1475]   Time 0.152 (0.139)   Data 0.112 (0.105)   Loss 0.3419 (0.3532)   Prec@1 84.375 (84.671)   Prec@5 84.375 (84.671)   [2018-04-01 01:52:21]
  Epoch: [132][600/1475]   Time 0.144 (0.139)   Data 0.113 (0.105)   Loss 0.2711 (0.3545)   Prec@1 90.625 (84.651)   Prec@5 90.625 (84.651)   [2018-04-01 01:52:48]
  Epoch: [132][800/1475]   Time 0.133 (0.139)   Data 0.102 (0.105)   Loss 0.3106 (0.3529)   Prec@1 87.500 (84.726)   Prec@5 87.500 (84.726)   [2018-04-01 01:53:16]
  Epoch: [132][1000/1475]   Time 0.141 (0.139)   Data 0.109 (0.105)   Loss 0.3581 (0.3546)   Prec@1 81.250 (84.759)   Prec@5 81.250 (84.759)   [2018-04-01 01:53:44]
  Epoch: [132][1200/1475]   Time 0.130 (0.139)   Data 0.099 (0.105)   Loss 0.5032 (0.3546)   Prec@1 75.000 (84.763)   Prec@5 75.000 (84.763)   [2018-04-01 01:54:11]
  Epoch: [132][1400/1475]   Time 0.141 (0.139)   Data 0.109 (0.105)   Loss 0.1812 (0.3545)   Prec@1 93.750 (84.823)   Prec@5 93.750 (84.823)   [2018-04-01 01:54:39]
  **Train** Prec@1 84.764 Prec@5 84.764 Error@1 15.236
  **VAL** Prec@1 87.413 Prec@5 87.413 Error@1 12.587

==>>[2018-04-01 01:55:07] [Epoch=133/250] [Need: 07:16:39] [learning_rate=0.0020] [Best : Accuracy=88.07, Error=11.93]

==>>Epoch=[133/250]], [2018-04-01 01:55:07], LR=[0.002], Batch=[32] [Model=SimpleNet]
  Epoch: [133][000/1475]   Time 0.139 (0.139)   Data 0.099 (0.099)   Loss 0.3172 (0.3172)   Prec@1 81.250 (81.250)   Prec@5 81.250 (81.250)   [2018-04-01 01:55:08]
  Epoch: [133][200/1475]   Time 0.129 (0.139)   Data 0.098 (0.106)   Loss 0.3649 (0.3569)   Prec@1 84.375 (84.701)   Prec@5 84.375 (84.701)   [2018-04-01 01:55:35]
  Epoch: [133][400/1475]   Time 0.141 (0.138)   Data 0.109 (0.106)   Loss 0.3925 (0.3585)   Prec@1 81.250 (84.609)   Prec@5 81.250 (84.609)   [2018-04-01 01:56:03]
  Epoch: [133][600/1475]   Time 0.143 (0.139)   Data 0.109 (0.106)   Loss 0.4526 (0.3582)   Prec@1 78.125 (84.640)   Prec@5 78.125 (84.640)   [2018-04-01 01:56:31]
  Epoch: [133][800/1475]   Time 0.141 (0.139)   Data 0.109 (0.105)   Loss 0.2151 (0.3561)   Prec@1 93.750 (84.792)   Prec@5 93.750 (84.792)   [2018-04-01 01:56:58]
  Epoch: [133][1000/1475]   Time 0.125 (0.139)   Data 0.094 (0.105)   Loss 0.3033 (0.3558)   Prec@1 90.625 (84.790)   Prec@5 90.625 (84.790)   [2018-04-01 01:57:26]
  Epoch: [133][1200/1475]   Time 0.141 (0.139)   Data 0.109 (0.105)   Loss 0.2353 (0.3557)   Prec@1 90.625 (84.833)   Prec@5 90.625 (84.833)   [2018-04-01 01:57:54]
  Epoch: [133][1400/1475]   Time 0.141 (0.139)   Data 0.094 (0.105)   Loss 0.2699 (0.3561)   Prec@1 87.500 (84.832)   Prec@5 87.500 (84.832)   [2018-04-01 01:58:21]
  **Train** Prec@1 84.834 Prec@5 84.834 Error@1 15.166
  **VAL** Prec@1 86.945 Prec@5 86.945 Error@1 13.055

==>>[2018-04-01 01:58:50] [Epoch=134/250] [Need: 07:12:54] [learning_rate=0.0020] [Best : Accuracy=88.07, Error=11.93]

==>>Epoch=[134/250]], [2018-04-01 01:58:50], LR=[0.002], Batch=[32] [Model=SimpleNet]
  Epoch: [134][000/1475]   Time 0.129 (0.129)   Data 0.098 (0.098)   Loss 0.5156 (0.5156)   Prec@1 78.125 (78.125)   Prec@5 78.125 (78.125)   [2018-04-01 01:58:50]
  Epoch: [134][200/1475]   Time 0.145 (0.139)   Data 0.109 (0.106)   Loss 0.3387 (0.3544)   Prec@1 87.500 (85.386)   Prec@5 87.500 (85.386)   [2018-04-01 01:59:18]
  Epoch: [134][400/1475]   Time 0.145 (0.139)   Data 0.098 (0.105)   Loss 0.2230 (0.3542)   Prec@1 93.750 (85.256)   Prec@5 93.750 (85.256)   [2018-04-01 01:59:45]
  Epoch: [134][600/1475]   Time 0.145 (0.139)   Data 0.114 (0.105)   Loss 0.4332 (0.3543)   Prec@1 78.125 (85.176)   Prec@5 78.125 (85.176)   [2018-04-01 02:00:13]
  Epoch: [134][800/1475]   Time 0.141 (0.139)   Data 0.094 (0.105)   Loss 0.4379 (0.3560)   Prec@1 84.375 (84.949)   Prec@5 84.375 (84.949)   [2018-04-01 02:00:41]
  Epoch: [134][1000/1475]   Time 0.141 (0.139)   Data 0.109 (0.105)   Loss 0.2132 (0.3567)   Prec@1 96.875 (84.928)   Prec@5 96.875 (84.928)   [2018-04-01 02:01:08]
  Epoch: [134][1200/1475]   Time 0.144 (0.139)   Data 0.112 (0.105)   Loss 0.3377 (0.3568)   Prec@1 87.500 (84.955)   Prec@5 87.500 (84.955)   [2018-04-01 02:01:36]
  Epoch: [134][1400/1475]   Time 0.141 (0.139)   Data 0.109 (0.105)   Loss 0.2176 (0.3555)   Prec@1 93.750 (85.042)   Prec@5 93.750 (85.042)   [2018-04-01 02:02:04]
  **Train** Prec@1 85.027 Prec@5 85.027 Error@1 14.973
  **VAL** Prec@1 87.809 Prec@5 87.809 Error@1 12.191

==>>[2018-04-01 02:02:32] [Epoch=135/250] [Need: 07:09:09] [learning_rate=0.0020] [Best : Accuracy=88.07, Error=11.93]

==>>Epoch=[135/250]], [2018-04-01 02:02:32], LR=[0.002], Batch=[32] [Model=SimpleNet]
  Epoch: [135][000/1475]   Time 0.128 (0.128)   Data 0.105 (0.105)   Loss 0.3647 (0.3647)   Prec@1 84.375 (84.375)   Prec@5 84.375 (84.375)   [2018-04-01 02:02:32]
  Epoch: [135][200/1475]   Time 0.141 (0.139)   Data 0.109 (0.106)   Loss 0.4973 (0.3541)   Prec@1 75.000 (84.935)   Prec@5 75.000 (84.935)   [2018-04-01 02:03:00]
  Epoch: [135][400/1475]   Time 0.128 (0.139)   Data 0.097 (0.105)   Loss 0.5001 (0.3569)   Prec@1 78.125 (84.905)   Prec@5 78.125 (84.905)   [2018-04-01 02:03:28]
  Epoch: [135][600/1475]   Time 0.133 (0.139)   Data 0.109 (0.105)   Loss 0.3231 (0.3599)   Prec@1 90.625 (84.744)   Prec@5 90.625 (84.744)   [2018-04-01 02:03:55]
  Epoch: [135][800/1475]   Time 0.144 (0.139)   Data 0.109 (0.105)   Loss 0.7646 (0.3588)   Prec@1 81.250 (84.859)   Prec@5 81.250 (84.859)   [2018-04-01 02:04:23]
  Epoch: [135][1000/1475]   Time 0.134 (0.139)   Data 0.103 (0.105)   Loss 0.2780 (0.3573)   Prec@1 87.500 (84.956)   Prec@5 87.500 (84.956)   [2018-04-01 02:04:51]
  Epoch: [135][1200/1475]   Time 0.125 (0.139)   Data 0.094 (0.105)   Loss 0.2690 (0.3562)   Prec@1 84.375 (84.908)   Prec@5 84.375 (84.908)   [2018-04-01 02:05:18]
  Epoch: [135][1400/1475]   Time 0.129 (0.139)   Data 0.098 (0.105)   Loss 0.3321 (0.3565)   Prec@1 84.375 (84.910)   Prec@5 84.375 (84.910)   [2018-04-01 02:05:46]
  **Train** Prec@1 84.938 Prec@5 84.938 Error@1 15.062
  **VAL** Prec@1 87.749 Prec@5 87.749 Error@1 12.251

==>>[2018-04-01 02:06:14] [Epoch=136/250] [Need: 07:05:23] [learning_rate=0.0020] [Best : Accuracy=88.07, Error=11.93]

==>>Epoch=[136/250]], [2018-04-01 02:06:14], LR=[0.002], Batch=[32] [Model=SimpleNet]
  Epoch: [136][000/1475]   Time 0.133 (0.133)   Data 0.102 (0.102)   Loss 0.2724 (0.2724)   Prec@1 93.750 (93.750)   Prec@5 93.750 (93.750)   [2018-04-01 02:06:14]
  Epoch: [136][200/1475]   Time 0.134 (0.139)   Data 0.103 (0.106)   Loss 0.3573 (0.3720)   Prec@1 87.500 (84.204)   Prec@5 87.500 (84.204)   [2018-04-01 02:06:42]
  Epoch: [136][400/1475]   Time 0.141 (0.139)   Data 0.109 (0.106)   Loss 0.4359 (0.3595)   Prec@1 78.125 (84.648)   Prec@5 78.125 (84.648)   [2018-04-01 02:07:10]
  Epoch: [136][600/1475]   Time 0.141 (0.139)   Data 0.109 (0.106)   Loss 0.5933 (0.3603)   Prec@1 71.875 (84.573)   Prec@5 71.875 (84.573)   [2018-04-01 02:07:38]
  Epoch: [136][800/1475]   Time 0.141 (0.139)   Data 0.109 (0.106)   Loss 0.2478 (0.3584)   Prec@1 93.750 (84.738)   Prec@5 93.750 (84.738)   [2018-04-01 02:08:05]
  Epoch: [136][1000/1475]   Time 0.139 (0.139)   Data 0.094 (0.106)   Loss 0.2983 (0.3555)   Prec@1 90.625 (84.890)   Prec@5 90.625 (84.890)   [2018-04-01 02:08:33]
  Epoch: [136][1200/1475]   Time 0.149 (0.139)   Data 0.109 (0.106)   Loss 0.4268 (0.3544)   Prec@1 78.125 (84.841)   Prec@5 78.125 (84.841)   [2018-04-01 02:09:01]
  Epoch: [136][1400/1475]   Time 0.165 (0.139)   Data 0.118 (0.106)   Loss 0.3301 (0.3541)   Prec@1 87.500 (84.846)   Prec@5 87.500 (84.846)   [2018-04-01 02:09:29]
  **Train** Prec@1 84.826 Prec@5 84.826 Error@1 15.174
  **VAL** Prec@1 87.941 Prec@5 87.941 Error@1 12.059

==>>[2018-04-01 02:09:57] [Epoch=137/250] [Need: 07:01:38] [learning_rate=0.0020] [Best : Accuracy=88.07, Error=11.93]

==>>Epoch=[137/250]], [2018-04-01 02:09:57], LR=[0.002], Batch=[32] [Model=SimpleNet]
  Epoch: [137][000/1475]   Time 0.141 (0.141)   Data 0.109 (0.109)   Loss 0.2293 (0.2293)   Prec@1 93.750 (93.750)   Prec@5 93.750 (93.750)   [2018-04-01 02:09:57]
  Epoch: [137][200/1475]   Time 0.141 (0.139)   Data 0.109 (0.106)   Loss 0.2036 (0.3551)   Prec@1 93.750 (84.966)   Prec@5 93.750 (84.966)   [2018-04-01 02:10:25]
  Epoch: [137][400/1475]   Time 0.141 (0.139)   Data 0.109 (0.106)   Loss 0.3630 (0.3561)   Prec@1 84.375 (84.772)   Prec@5 84.375 (84.772)   [2018-04-01 02:10:52]
  Epoch: [137][600/1475]   Time 0.143 (0.139)   Data 0.109 (0.106)   Loss 0.2984 (0.3535)   Prec@1 87.500 (84.853)   Prec@5 87.500 (84.853)   [2018-04-01 02:11:20]
  Epoch: [137][800/1475]   Time 0.141 (0.139)   Data 0.109 (0.105)   Loss 0.3602 (0.3574)   Prec@1 81.250 (84.672)   Prec@5 81.250 (84.672)   [2018-04-01 02:11:48]
  Epoch: [137][1000/1475]   Time 0.141 (0.139)   Data 0.109 (0.106)   Loss 0.2214 (0.3579)   Prec@1 87.500 (84.678)   Prec@5 87.500 (84.678)   [2018-04-01 02:12:16]
  Epoch: [137][1200/1475]   Time 0.133 (0.139)   Data 0.102 (0.106)   Loss 0.5853 (0.3577)   Prec@1 75.000 (84.724)   Prec@5 75.000 (84.724)   [2018-04-01 02:12:43]
  Epoch: [137][1400/1475]   Time 0.129 (0.139)   Data 0.094 (0.106)   Loss 0.3531 (0.3581)   Prec@1 87.500 (84.810)   Prec@5 87.500 (84.810)   [2018-04-01 02:13:11]
  **Train** Prec@1 84.832 Prec@5 84.832 Error@1 15.168
  **VAL** Prec@1 87.761 Prec@5 87.761 Error@1 12.239

==>>[2018-04-01 02:13:39] [Epoch=138/250] [Need: 06:57:53] [learning_rate=0.0020] [Best : Accuracy=88.07, Error=11.93]

==>>Epoch=[138/250]], [2018-04-01 02:13:39], LR=[0.002], Batch=[32] [Model=SimpleNet]
  Epoch: [138][000/1475]   Time 0.144 (0.144)   Data 0.113 (0.113)   Loss 0.5219 (0.5219)   Prec@1 78.125 (78.125)   Prec@5 78.125 (78.125)   [2018-04-01 02:13:39]
  Epoch: [138][200/1475]   Time 0.123 (0.138)   Data 0.092 (0.105)   Loss 0.2759 (0.3418)   Prec@1 84.375 (85.463)   Prec@5 84.375 (85.463)   [2018-04-01 02:14:07]
  Epoch: [138][400/1475]   Time 0.125 (0.138)   Data 0.094 (0.105)   Loss 0.2595 (0.3489)   Prec@1 87.500 (85.170)   Prec@5 87.500 (85.170)   [2018-04-01 02:14:35]
  Epoch: [138][600/1475]   Time 0.148 (0.138)   Data 0.114 (0.105)   Loss 0.6180 (0.3507)   Prec@1 71.875 (84.921)   Prec@5 71.875 (84.921)   [2018-04-01 02:15:02]
  Epoch: [138][800/1475]   Time 0.128 (0.138)   Data 0.108 (0.105)   Loss 0.6068 (0.3499)   Prec@1 81.250 (85.089)   Prec@5 81.250 (85.089)   [2018-04-01 02:15:30]
  Epoch: [138][1000/1475]   Time 0.125 (0.138)   Data 0.094 (0.105)   Loss 0.4724 (0.3530)   Prec@1 78.125 (85.024)   Prec@5 78.125 (85.024)   [2018-04-01 02:15:58]
  Epoch: [138][1200/1475]   Time 0.128 (0.138)   Data 0.097 (0.105)   Loss 0.4466 (0.3525)   Prec@1 81.250 (85.049)   Prec@5 81.250 (85.049)   [2018-04-01 02:16:25]
  Epoch: [138][1400/1475]   Time 0.145 (0.138)   Data 0.098 (0.105)   Loss 0.3148 (0.3531)   Prec@1 90.625 (85.011)   Prec@5 90.625 (85.011)   [2018-04-01 02:16:53]
  **Train** Prec@1 84.968 Prec@5 84.968 Error@1 15.032
  **VAL** Prec@1 87.785 Prec@5 87.785 Error@1 12.215

==>>[2018-04-01 02:17:21] [Epoch=139/250] [Need: 06:54:08] [learning_rate=0.0020] [Best : Accuracy=88.07, Error=11.93]

==>>Epoch=[139/250]], [2018-04-01 02:17:21], LR=[0.002], Batch=[32] [Model=SimpleNet]
  Epoch: [139][000/1475]   Time 0.145 (0.145)   Data 0.115 (0.115)   Loss 0.2619 (0.2619)   Prec@1 90.625 (90.625)   Prec@5 90.625 (90.625)   [2018-04-01 02:17:22]
  Epoch: [139][200/1475]   Time 0.133 (0.139)   Data 0.094 (0.105)   Loss 0.4855 (0.3620)   Prec@1 81.250 (84.562)   Prec@5 81.250 (84.562)   [2018-04-01 02:17:49]
  Epoch: [139][400/1475]   Time 0.141 (0.139)   Data 0.094 (0.105)   Loss 0.3611 (0.3581)   Prec@1 78.125 (84.640)   Prec@5 78.125 (84.640)   [2018-04-01 02:18:17]
  Epoch: [139][600/1475]   Time 0.143 (0.139)   Data 0.109 (0.105)   Loss 0.2227 (0.3560)   Prec@1 93.750 (84.713)   Prec@5 93.750 (84.713)   [2018-04-01 02:18:45]
  Epoch: [139][800/1475]   Time 0.144 (0.139)   Data 0.113 (0.105)   Loss 0.2359 (0.3542)   Prec@1 87.500 (84.839)   Prec@5 87.500 (84.839)   [2018-04-01 02:19:12]
  Epoch: [139][1000/1475]   Time 0.142 (0.139)   Data 0.094 (0.105)   Loss 0.5625 (0.3533)   Prec@1 68.750 (84.924)   Prec@5 68.750 (84.924)   [2018-04-01 02:19:40]
  Epoch: [139][1200/1475]   Time 0.141 (0.139)   Data 0.109 (0.105)   Loss 0.5250 (0.3517)   Prec@1 78.125 (85.007)   Prec@5 78.125 (85.007)   [2018-04-01 02:20:08]
  Epoch: [139][1400/1475]   Time 0.141 (0.139)   Data 0.109 (0.105)   Loss 0.5066 (0.3535)   Prec@1 81.250 (84.855)   Prec@5 81.250 (84.855)   [2018-04-01 02:20:36]
  **Train** Prec@1 84.839 Prec@5 84.839 Error@1 15.161
  **VAL** Prec@1 87.605 Prec@5 87.605 Error@1 12.395

==>>[2018-04-01 02:21:04] [Epoch=140/250] [Need: 06:50:23] [learning_rate=0.0020] [Best : Accuracy=88.07, Error=11.93]

==>>Epoch=[140/250]], [2018-04-01 02:21:04], LR=[0.002], Batch=[32] [Model=SimpleNet]
  Epoch: [140][000/1475]   Time 0.128 (0.128)   Data 0.097 (0.097)   Loss 0.3814 (0.3814)   Prec@1 84.375 (84.375)   Prec@5 84.375 (84.375)   [2018-04-01 02:21:04]
  Epoch: [140][200/1475]   Time 0.141 (0.138)   Data 0.109 (0.105)   Loss 0.4131 (0.3421)   Prec@1 84.375 (85.961)   Prec@5 84.375 (85.961)   [2018-04-01 02:21:32]
  Epoch: [140][400/1475]   Time 0.146 (0.138)   Data 0.115 (0.105)   Loss 0.4872 (0.3436)   Prec@1 71.875 (85.591)   Prec@5 71.875 (85.591)   [2018-04-01 02:21:59]
  Epoch: [140][600/1475]   Time 0.141 (0.139)   Data 0.109 (0.105)   Loss 0.4938 (0.3534)   Prec@1 75.000 (85.061)   Prec@5 75.000 (85.061)   [2018-04-01 02:22:27]
  Epoch: [140][800/1475]   Time 0.150 (0.139)   Data 0.103 (0.105)   Loss 0.3292 (0.3518)   Prec@1 87.500 (85.155)   Prec@5 87.500 (85.155)   [2018-04-01 02:22:55]
  Epoch: [140][1000/1475]   Time 0.150 (0.139)   Data 0.119 (0.106)   Loss 0.3470 (0.3523)   Prec@1 84.375 (85.140)   Prec@5 84.375 (85.140)   [2018-04-01 02:23:22]
  Epoch: [140][1200/1475]   Time 0.145 (0.139)   Data 0.109 (0.105)   Loss 0.2955 (0.3518)   Prec@1 81.250 (85.143)   Prec@5 81.250 (85.143)   [2018-04-01 02:23:50]
  Epoch: [140][1400/1475]   Time 0.134 (0.139)   Data 0.113 (0.105)   Loss 0.3252 (0.3522)   Prec@1 87.500 (85.133)   Prec@5 87.500 (85.133)   [2018-04-01 02:24:18]
  **Train** Prec@1 85.154 Prec@5 85.154 Error@1 14.846
  **VAL** Prec@1 88.037 Prec@5 88.037 Error@1 11.963

==>>[2018-04-01 02:24:46] [Epoch=141/250] [Need: 06:46:38] [learning_rate=0.0020] [Best : Accuracy=88.07, Error=11.93]

==>>Epoch=[141/250]], [2018-04-01 02:24:46], LR=[0.002], Batch=[32] [Model=SimpleNet]
  Epoch: [141][000/1475]   Time 0.132 (0.132)   Data 0.101 (0.101)   Loss 0.2504 (0.2504)   Prec@1 87.500 (87.500)   Prec@5 87.500 (87.500)   [2018-04-01 02:24:46]
  Epoch: [141][200/1475]   Time 0.144 (0.138)   Data 0.116 (0.106)   Loss 0.2415 (0.3537)   Prec@1 87.500 (85.028)   Prec@5 87.500 (85.028)   [2018-04-01 02:25:14]
  Epoch: [141][400/1475]   Time 0.125 (0.139)   Data 0.094 (0.105)   Loss 0.2755 (0.3621)   Prec@1 90.625 (84.687)   Prec@5 90.625 (84.687)   [2018-04-01 02:25:42]
  Epoch: [141][600/1475]   Time 0.137 (0.139)   Data 0.090 (0.105)   Loss 0.1630 (0.3558)   Prec@1 96.875 (84.926)   Prec@5 96.875 (84.926)   [2018-04-01 02:26:09]
  Epoch: [141][800/1475]   Time 0.141 (0.139)   Data 0.109 (0.105)   Loss 0.2927 (0.3514)   Prec@1 87.500 (85.097)   Prec@5 87.500 (85.097)   [2018-04-01 02:26:37]
  Epoch: [141][1000/1475]   Time 0.141 (0.139)   Data 0.094 (0.105)   Loss 0.3532 (0.3519)   Prec@1 84.375 (85.118)   Prec@5 84.375 (85.118)   [2018-04-01 02:27:05]
  Epoch: [141][1200/1475]   Time 0.141 (0.139)   Data 0.109 (0.105)   Loss 0.3657 (0.3527)   Prec@1 84.375 (85.054)   Prec@5 84.375 (85.054)   [2018-04-01 02:27:32]
  Epoch: [141][1400/1475]   Time 0.141 (0.139)   Data 0.109 (0.105)   Loss 0.2392 (0.3535)   Prec@1 96.875 (85.089)   Prec@5 96.875 (85.089)   [2018-04-01 02:28:00]
  **Train** Prec@1 85.152 Prec@5 85.152 Error@1 14.848
  **VAL** Prec@1 87.965 Prec@5 87.965 Error@1 12.035

==>>[2018-04-01 02:28:28] [Epoch=142/250] [Need: 06:42:53] [learning_rate=0.0020] [Best : Accuracy=88.07, Error=11.93]

==>>Epoch=[142/250]], [2018-04-01 02:28:28], LR=[0.002], Batch=[32] [Model=SimpleNet]
  Epoch: [142][000/1475]   Time 0.128 (0.128)   Data 0.097 (0.097)   Loss 0.3036 (0.3036)   Prec@1 84.375 (84.375)   Prec@5 84.375 (84.375)   [2018-04-01 02:28:28]
  Epoch: [142][200/1475]   Time 0.146 (0.139)   Data 0.099 (0.105)   Loss 0.2891 (0.3648)   Prec@1 90.625 (84.344)   Prec@5 90.625 (84.344)   [2018-04-01 02:28:56]
  Epoch: [142][400/1475]   Time 0.133 (0.139)   Data 0.111 (0.106)   Loss 0.5065 (0.3625)   Prec@1 81.250 (84.578)   Prec@5 81.250 (84.578)   [2018-04-01 02:29:24]
  Epoch: [142][600/1475]   Time 0.141 (0.139)   Data 0.109 (0.105)   Loss 0.4058 (0.3581)   Prec@1 81.250 (84.734)   Prec@5 81.250 (84.734)   [2018-04-01 02:29:52]
  Epoch: [142][800/1475]   Time 0.141 (0.139)   Data 0.109 (0.106)   Loss 0.3301 (0.3541)   Prec@1 84.375 (84.945)   Prec@5 84.375 (84.945)   [2018-04-01 02:30:19]
  Epoch: [142][1000/1475]   Time 0.131 (0.139)   Data 0.100 (0.105)   Loss 0.2279 (0.3538)   Prec@1 90.625 (84.987)   Prec@5 90.625 (84.987)   [2018-04-01 02:30:47]
  Epoch: [142][1200/1475]   Time 0.141 (0.139)   Data 0.109 (0.105)   Loss 0.3646 (0.3533)   Prec@1 87.500 (85.052)   Prec@5 87.500 (85.052)   [2018-04-01 02:31:15]
  Epoch: [142][1400/1475]   Time 0.145 (0.139)   Data 0.114 (0.106)   Loss 0.2762 (0.3529)   Prec@1 87.500 (85.109)   Prec@5 87.500 (85.109)   [2018-04-01 02:31:43]
  **Train** Prec@1 85.089 Prec@5 85.089 Error@1 14.911
  **VAL** Prec@1 87.761 Prec@5 87.761 Error@1 12.239

==>>[2018-04-01 02:32:11] [Epoch=143/250] [Need: 06:39:08] [learning_rate=0.0020] [Best : Accuracy=88.07, Error=11.93]

==>>Epoch=[143/250]], [2018-04-01 02:32:11], LR=[0.002], Batch=[32] [Model=SimpleNet]
  Epoch: [143][000/1475]   Time 0.128 (0.128)   Data 0.097 (0.097)   Loss 0.2460 (0.2460)   Prec@1 87.500 (87.500)   Prec@5 87.500 (87.500)   [2018-04-01 02:32:11]
  Epoch: [143][200/1475]   Time 0.141 (0.139)   Data 0.109 (0.106)   Loss 0.2621 (0.3519)   Prec@1 96.875 (85.012)   Prec@5 96.875 (85.012)   [2018-04-01 02:32:39]
  Epoch: [143][400/1475]   Time 0.127 (0.139)   Data 0.096 (0.106)   Loss 0.3197 (0.3506)   Prec@1 93.750 (85.108)   Prec@5 93.750 (85.108)   [2018-04-01 02:33:07]
  Epoch: [143][600/1475]   Time 0.141 (0.139)   Data 0.109 (0.106)   Loss 0.2586 (0.3535)   Prec@1 90.625 (85.020)   Prec@5 90.625 (85.020)   [2018-04-01 02:33:34]
  Epoch: [143][800/1475]   Time 0.138 (0.139)   Data 0.100 (0.106)   Loss 0.3285 (0.3528)   Prec@1 84.375 (84.968)   Prec@5 84.375 (84.968)   [2018-04-01 02:34:02]
  Epoch: [143][1000/1475]   Time 0.144 (0.139)   Data 0.113 (0.106)   Loss 0.3025 (0.3527)   Prec@1 87.500 (84.912)   Prec@5 87.500 (84.912)   [2018-04-01 02:34:30]
  Epoch: [143][1200/1475]   Time 0.127 (0.139)   Data 0.096 (0.106)   Loss 0.3781 (0.3536)   Prec@1 81.250 (84.882)   Prec@5 81.250 (84.882)   [2018-04-01 02:34:58]
  Epoch: [143][1400/1475]   Time 0.140 (0.139)   Data 0.093 (0.106)   Loss 0.4337 (0.3538)   Prec@1 78.125 (84.886)   Prec@5 78.125 (84.886)   [2018-04-01 02:35:25]
  **Train** Prec@1 84.894 Prec@5 84.894 Error@1 15.106
  **VAL** Prec@1 87.509 Prec@5 87.509 Error@1 12.491

==>>[2018-04-01 02:35:54] [Epoch=144/250] [Need: 06:35:23] [learning_rate=0.0020] [Best : Accuracy=88.07, Error=11.93]

==>>Epoch=[144/250]], [2018-04-01 02:35:54], LR=[0.002], Batch=[32] [Model=SimpleNet]
  Epoch: [144][000/1475]   Time 0.141 (0.141)   Data 0.109 (0.109)   Loss 0.3584 (0.3584)   Prec@1 81.250 (81.250)   Prec@5 81.250 (81.250)   [2018-04-01 02:35:54]
  Epoch: [144][200/1475]   Time 0.143 (0.139)   Data 0.109 (0.106)   Loss 0.3404 (0.3578)   Prec@1 84.375 (84.655)   Prec@5 84.375 (84.655)   [2018-04-01 02:36:22]
  Epoch: [144][400/1475]   Time 0.141 (0.139)   Data 0.094 (0.106)   Loss 0.4219 (0.3578)   Prec@1 78.125 (84.640)   Prec@5 78.125 (84.640)   [2018-04-01 02:36:49]
  Epoch: [144][600/1475]   Time 0.145 (0.139)   Data 0.109 (0.106)   Loss 0.5072 (0.3584)   Prec@1 78.125 (84.614)   Prec@5 78.125 (84.614)   [2018-04-01 02:37:17]
  Epoch: [144][800/1475]   Time 0.129 (0.139)   Data 0.098 (0.106)   Loss 0.3657 (0.3551)   Prec@1 84.375 (84.734)   Prec@5 84.375 (84.734)   [2018-04-01 02:37:45]
  Epoch: [144][1000/1475]   Time 0.141 (0.139)   Data 0.109 (0.106)   Loss 0.3118 (0.3541)   Prec@1 90.625 (84.887)   Prec@5 90.625 (84.887)   [2018-04-01 02:38:12]
  Epoch: [144][1200/1475]   Time 0.132 (0.139)   Data 0.110 (0.106)   Loss 0.2984 (0.3536)   Prec@1 84.375 (84.911)   Prec@5 84.375 (84.911)   [2018-04-01 02:38:40]
  Epoch: [144][1400/1475]   Time 0.141 (0.139)   Data 0.109 (0.106)   Loss 0.5327 (0.3527)   Prec@1 81.250 (85.008)   Prec@5 81.250 (85.008)   [2018-04-01 02:39:08]
  **Train** Prec@1 84.987 Prec@5 84.987 Error@1 15.013
  **VAL** Prec@1 87.221 Prec@5 87.221 Error@1 12.779

==>>[2018-04-01 02:39:36] [Epoch=145/250] [Need: 06:31:39] [learning_rate=0.0020] [Best : Accuracy=88.07, Error=11.93]

==>>Epoch=[145/250]], [2018-04-01 02:39:36], LR=[0.002], Batch=[32] [Model=SimpleNet]
  Epoch: [145][000/1475]   Time 0.141 (0.141)   Data 0.094 (0.094)   Loss 0.3476 (0.3476)   Prec@1 81.250 (81.250)   Prec@5 81.250 (81.250)   [2018-04-01 02:39:36]
  Epoch: [145][200/1475]   Time 0.141 (0.139)   Data 0.109 (0.105)   Loss 0.4572 (0.3482)   Prec@1 75.000 (85.137)   Prec@5 75.000 (85.137)   [2018-04-01 02:40:04]
  Epoch: [145][400/1475]   Time 0.141 (0.139)   Data 0.094 (0.105)   Loss 0.2566 (0.3450)   Prec@1 90.625 (85.427)   Prec@5 90.625 (85.427)   [2018-04-01 02:40:32]
  Epoch: [145][600/1475]   Time 0.148 (0.139)   Data 0.115 (0.105)   Loss 0.4788 (0.3497)   Prec@1 78.125 (85.171)   Prec@5 78.125 (85.171)   [2018-04-01 02:40:59]
  Epoch: [145][800/1475]   Time 0.141 (0.139)   Data 0.109 (0.105)   Loss 0.5364 (0.3486)   Prec@1 75.000 (85.233)   Prec@5 75.000 (85.233)   [2018-04-01 02:41:27]
  Epoch: [145][1000/1475]   Time 0.144 (0.139)   Data 0.113 (0.105)   Loss 0.2699 (0.3486)   Prec@1 90.625 (85.146)   Prec@5 90.625 (85.146)   [2018-04-01 02:41:55]
  Epoch: [145][1200/1475]   Time 0.133 (0.139)   Data 0.108 (0.106)   Loss 0.3071 (0.3515)   Prec@1 90.625 (84.973)   Prec@5 90.625 (84.973)   [2018-04-01 02:42:23]
  Epoch: [145][1400/1475]   Time 0.141 (0.139)   Data 0.109 (0.106)   Loss 0.6322 (0.3520)   Prec@1 75.000 (84.977)   Prec@5 75.000 (84.977)   [2018-04-01 02:42:50]
  **Train** Prec@1 84.993 Prec@5 84.993 Error@1 15.007
  **VAL** Prec@1 87.917 Prec@5 87.917 Error@1 12.083

==>>[2018-04-01 02:43:18] [Epoch=146/250] [Need: 06:27:54] [learning_rate=0.0020] [Best : Accuracy=88.07, Error=11.93]

==>>Epoch=[146/250]], [2018-04-01 02:43:18], LR=[0.002], Batch=[32] [Model=SimpleNet]
  Epoch: [146][000/1475]   Time 0.141 (0.141)   Data 0.094 (0.094)   Loss 0.3293 (0.3293)   Prec@1 84.375 (84.375)   Prec@5 84.375 (84.375)   [2018-04-01 02:43:19]
  Epoch: [146][200/1475]   Time 0.155 (0.139)   Data 0.109 (0.106)   Loss 0.3289 (0.3424)   Prec@1 81.250 (84.966)   Prec@5 81.250 (84.966)   [2018-04-01 02:43:46]
  Epoch: [146][400/1475]   Time 0.143 (0.139)   Data 0.110 (0.106)   Loss 0.3275 (0.3417)   Prec@1 84.375 (85.147)   Prec@5 84.375 (85.147)   [2018-04-01 02:44:14]
  Epoch: [146][600/1475]   Time 0.143 (0.139)   Data 0.096 (0.105)   Loss 0.4320 (0.3446)   Prec@1 81.250 (85.176)   Prec@5 81.250 (85.176)   [2018-04-01 02:44:42]
  Epoch: [146][800/1475]   Time 0.126 (0.139)   Data 0.095 (0.106)   Loss 0.4026 (0.3478)   Prec@1 84.375 (85.105)   Prec@5 84.375 (85.105)   [2018-04-01 02:45:10]
  Epoch: [146][1000/1475]   Time 0.141 (0.139)   Data 0.109 (0.106)   Loss 0.3021 (0.3453)   Prec@1 90.625 (85.346)   Prec@5 90.625 (85.346)   [2018-04-01 02:45:37]
  Epoch: [146][1200/1475]   Time 0.142 (0.139)   Data 0.113 (0.106)   Loss 0.5814 (0.3458)   Prec@1 75.000 (85.361)   Prec@5 75.000 (85.361)   [2018-04-01 02:46:05]
  Epoch: [146][1400/1475]   Time 0.140 (0.139)   Data 0.102 (0.106)   Loss 0.2514 (0.3483)   Prec@1 84.375 (85.211)   Prec@5 84.375 (85.211)   [2018-04-01 02:46:33]
  **Train** Prec@1 85.184 Prec@5 85.184 Error@1 14.816
  **VAL** Prec@1 87.689 Prec@5 87.689 Error@1 12.311

==>>[2018-04-01 02:47:01] [Epoch=147/250] [Need: 06:24:09] [learning_rate=0.0020] [Best : Accuracy=88.07, Error=11.93]

==>>Epoch=[147/250]], [2018-04-01 02:47:01], LR=[0.002], Batch=[32] [Model=SimpleNet]
  Epoch: [147][000/1475]   Time 0.144 (0.144)   Data 0.113 (0.113)   Loss 0.3025 (0.3025)   Prec@1 84.375 (84.375)   Prec@5 84.375 (84.375)   [2018-04-01 02:47:01]
  Epoch: [147][200/1475]   Time 0.125 (0.139)   Data 0.094 (0.105)   Loss 0.2969 (0.3672)   Prec@1 90.625 (84.188)   Prec@5 90.625 (84.188)   [2018-04-01 02:47:29]
  Epoch: [147][400/1475]   Time 0.136 (0.139)   Data 0.104 (0.105)   Loss 0.3174 (0.3609)   Prec@1 90.625 (84.484)   Prec@5 90.625 (84.484)   [2018-04-01 02:47:57]
  Epoch: [147][600/1475]   Time 0.141 (0.139)   Data 0.109 (0.105)   Loss 0.3912 (0.3614)   Prec@1 81.250 (84.677)   Prec@5 81.250 (84.677)   [2018-04-01 02:48:24]
  Epoch: [147][800/1475]   Time 0.137 (0.139)   Data 0.106 (0.105)   Loss 0.3627 (0.3591)   Prec@1 81.250 (84.765)   Prec@5 81.250 (84.765)   [2018-04-01 02:48:52]
  Epoch: [147][1000/1475]   Time 0.144 (0.139)   Data 0.109 (0.105)   Loss 0.3610 (0.3560)   Prec@1 87.500 (84.978)   Prec@5 87.500 (84.978)   [2018-04-01 02:49:20]
  Epoch: [147][1200/1475]   Time 0.128 (0.139)   Data 0.105 (0.105)   Loss 0.1481 (0.3547)   Prec@1 96.875 (85.010)   Prec@5 96.875 (85.010)   [2018-04-01 02:49:48]
  Epoch: [147][1400/1475]   Time 0.129 (0.139)   Data 0.098 (0.105)   Loss 0.3647 (0.3556)   Prec@1 90.625 (84.917)   Prec@5 90.625 (84.917)   [2018-04-01 02:50:15]
  **Train** Prec@1 84.887 Prec@5 84.887 Error@1 15.113
  **VAL** Prec@1 88.025 Prec@5 88.025 Error@1 11.975

==>>[2018-04-01 02:50:43] [Epoch=148/250] [Need: 06:20:24] [learning_rate=0.0020] [Best : Accuracy=88.07, Error=11.93]

==>>Epoch=[148/250]], [2018-04-01 02:50:43], LR=[0.002], Batch=[32] [Model=SimpleNet]
  Epoch: [148][000/1475]   Time 0.125 (0.125)   Data 0.094 (0.094)   Loss 0.3344 (0.3344)   Prec@1 90.625 (90.625)   Prec@5 90.625 (90.625)   [2018-04-01 02:50:44]
  Epoch: [148][200/1475]   Time 0.127 (0.138)   Data 0.096 (0.105)   Loss 0.2671 (0.3346)   Prec@1 90.625 (85.603)   Prec@5 90.625 (85.603)   [2018-04-01 02:51:11]
  Epoch: [148][400/1475]   Time 0.141 (0.138)   Data 0.109 (0.105)   Loss 0.3945 (0.3458)   Prec@1 87.500 (85.349)   Prec@5 87.500 (85.349)   [2018-04-01 02:51:39]
  Epoch: [148][600/1475]   Time 0.141 (0.138)   Data 0.109 (0.105)   Loss 0.4214 (0.3475)   Prec@1 81.250 (85.342)   Prec@5 81.250 (85.342)   [2018-04-01 02:52:07]
  Epoch: [148][800/1475]   Time 0.149 (0.138)   Data 0.109 (0.105)   Loss 0.5134 (0.3491)   Prec@1 75.000 (85.307)   Prec@5 75.000 (85.307)   [2018-04-01 02:52:34]
  Epoch: [148][1000/1475]   Time 0.141 (0.138)   Data 0.109 (0.105)   Loss 0.3118 (0.3484)   Prec@1 90.625 (85.240)   Prec@5 90.625 (85.240)   [2018-04-01 02:53:02]
  Epoch: [148][1200/1475]   Time 0.141 (0.138)   Data 0.109 (0.105)   Loss 0.3705 (0.3515)   Prec@1 84.375 (85.119)   Prec@5 84.375 (85.119)   [2018-04-01 02:53:30]
  Epoch: [148][1400/1475]   Time 0.130 (0.139)   Data 0.098 (0.105)   Loss 0.5101 (0.3530)   Prec@1 84.375 (84.997)   Prec@5 84.375 (84.997)   [2018-04-01 02:53:57]
  **Train** Prec@1 85.010 Prec@5 85.010 Error@1 14.990
  **VAL** Prec@1 87.173 Prec@5 87.173 Error@1 12.827

==>>[2018-04-01 02:54:26] [Epoch=149/250] [Need: 06:16:40] [learning_rate=0.0020] [Best : Accuracy=88.07, Error=11.93]

==>>Epoch=[149/250]], [2018-04-01 02:54:26], LR=[0.002], Batch=[32] [Model=SimpleNet]
  Epoch: [149][000/1475]   Time 0.146 (0.146)   Data 0.114 (0.114)   Loss 0.2146 (0.2146)   Prec@1 96.875 (96.875)   Prec@5 96.875 (96.875)   [2018-04-01 02:54:26]
  Epoch: [149][200/1475]   Time 0.145 (0.139)   Data 0.098 (0.105)   Loss 0.4176 (0.3607)   Prec@1 84.375 (84.686)   Prec@5 84.375 (84.686)   [2018-04-01 02:54:53]
  Epoch: [149][400/1475]   Time 0.145 (0.139)   Data 0.114 (0.106)   Loss 0.3451 (0.3576)   Prec@1 90.625 (84.866)   Prec@5 90.625 (84.866)   [2018-04-01 02:55:21]
  Epoch: [149][600/1475]   Time 0.141 (0.139)   Data 0.109 (0.106)   Loss 0.2971 (0.3497)   Prec@1 87.500 (85.285)   Prec@5 87.500 (85.285)   [2018-04-01 02:55:49]
  Epoch: [149][800/1475]   Time 0.141 (0.139)   Data 0.109 (0.105)   Loss 0.4585 (0.3499)   Prec@1 81.250 (85.424)   Prec@5 81.250 (85.424)   [2018-04-01 02:56:17]
  Epoch: [149][1000/1475]   Time 0.126 (0.139)   Data 0.095 (0.105)   Loss 0.3810 (0.3508)   Prec@1 78.125 (85.318)   Prec@5 78.125 (85.318)   [2018-04-01 02:56:44]
  Epoch: [149][1200/1475]   Time 0.141 (0.139)   Data 0.109 (0.105)   Loss 0.4512 (0.3504)   Prec@1 78.125 (85.286)   Prec@5 78.125 (85.286)   [2018-04-01 02:57:12]
  Epoch: [149][1400/1475]   Time 0.151 (0.139)   Data 0.109 (0.105)   Loss 0.3856 (0.3515)   Prec@1 84.375 (85.165)   Prec@5 84.375 (85.165)   [2018-04-01 02:57:40]
  **Train** Prec@1 85.161 Prec@5 85.161 Error@1 14.839
  **VAL** Prec@1 87.629 Prec@5 87.629 Error@1 12.371

==>>[2018-04-01 02:58:08] [Epoch=150/250] [Need: 06:12:55] [learning_rate=0.0002] [Best : Accuracy=88.07, Error=11.93]

==>>Epoch=[150/250]], [2018-04-01 02:58:08], LR=[0.002], Batch=[32] [Model=SimpleNet]
  Epoch: [150][000/1475]   Time 0.148 (0.148)   Data 0.109 (0.109)   Loss 0.3983 (0.3983)   Prec@1 78.125 (78.125)   Prec@5 78.125 (78.125)   [2018-04-01 02:58:08]
  Epoch: [150][200/1475]   Time 0.137 (0.138)   Data 0.117 (0.106)   Loss 0.1961 (0.3413)   Prec@1 96.875 (85.354)   Prec@5 96.875 (85.354)   [2018-04-01 02:58:36]
  Epoch: [150][400/1475]   Time 0.152 (0.139)   Data 0.118 (0.106)   Loss 0.2458 (0.3417)   Prec@1 87.500 (85.450)   Prec@5 87.500 (85.450)   [2018-04-01 02:59:03]
  Epoch: [150][600/1475]   Time 0.143 (0.139)   Data 0.109 (0.106)   Loss 0.3219 (0.3423)   Prec@1 90.625 (85.425)   Prec@5 90.625 (85.425)   [2018-04-01 02:59:31]
  Epoch: [150][800/1475]   Time 0.127 (0.138)   Data 0.096 (0.105)   Loss 0.2873 (0.3424)   Prec@1 87.500 (85.530)   Prec@5 87.500 (85.530)   [2018-04-01 02:59:59]
  Epoch: [150][1000/1475]   Time 0.141 (0.139)   Data 0.109 (0.105)   Loss 0.3466 (0.3428)   Prec@1 84.375 (85.621)   Prec@5 84.375 (85.621)   [2018-04-01 03:00:26]
  Epoch: [150][1200/1475]   Time 0.148 (0.139)   Data 0.117 (0.105)   Loss 0.1643 (0.3416)   Prec@1 93.750 (85.689)   Prec@5 93.750 (85.689)   [2018-04-01 03:00:54]
  Epoch: [150][1400/1475]   Time 0.161 (0.139)   Data 0.114 (0.105)   Loss 0.3042 (0.3429)   Prec@1 84.375 (85.586)   Prec@5 84.375 (85.586)   [2018-04-01 03:01:22]
  **Train** Prec@1 85.591 Prec@5 85.591 Error@1 14.409
  **VAL** Prec@1 88.037 Prec@5 88.037 Error@1 11.963

==>>[2018-04-01 03:01:50] [Epoch=151/250] [Need: 06:09:10] [learning_rate=0.0002] [Best : Accuracy=88.07, Error=11.93]

==>>Epoch=[151/250]], [2018-04-01 03:01:50], LR=[0.002], Batch=[32] [Model=SimpleNet]
  Epoch: [151][000/1475]   Time 0.125 (0.125)   Data 0.094 (0.094)   Loss 0.2852 (0.2852)   Prec@1 84.375 (84.375)   Prec@5 84.375 (84.375)   [2018-04-01 03:01:50]
  Epoch: [151][200/1475]   Time 0.139 (0.139)   Data 0.096 (0.105)   Loss 0.2707 (0.3446)   Prec@1 87.500 (85.914)   Prec@5 87.500 (85.914)   [2018-04-01 03:02:18]
  Epoch: [151][400/1475]   Time 0.149 (0.139)   Data 0.109 (0.105)   Loss 0.2950 (0.3449)   Prec@1 90.625 (85.435)   Prec@5 90.625 (85.435)   [2018-04-01 03:02:46]
  Epoch: [151][600/1475]   Time 0.137 (0.139)   Data 0.106 (0.105)   Loss 0.3684 (0.3410)   Prec@1 87.500 (85.405)   Prec@5 87.500 (85.405)   [2018-04-01 03:03:13]
  Epoch: [151][800/1475]   Time 0.147 (0.139)   Data 0.114 (0.106)   Loss 0.4609 (0.3381)   Prec@1 81.250 (85.514)   Prec@5 81.250 (85.514)   [2018-04-01 03:03:41]
  Epoch: [151][1000/1475]   Time 0.132 (0.139)   Data 0.101 (0.105)   Loss 0.2510 (0.3379)   Prec@1 90.625 (85.646)   Prec@5 90.625 (85.646)   [2018-04-01 03:04:09]
  Epoch: [151][1200/1475]   Time 0.125 (0.139)   Data 0.094 (0.105)   Loss 0.3022 (0.3383)   Prec@1 84.375 (85.543)   Prec@5 84.375 (85.543)   [2018-04-01 03:04:37]
  Epoch: [151][1400/1475]   Time 0.125 (0.139)   Data 0.094 (0.105)   Loss 0.1760 (0.3373)   Prec@1 96.875 (85.635)   Prec@5 96.875 (85.635)   [2018-04-01 03:05:04]
  **Train** Prec@1 85.585 Prec@5 85.585 Error@1 14.415
  **VAL** Prec@1 88.170 Prec@5 88.170 Error@1 11.830

==>>[2018-04-01 03:05:32] [Epoch=152/250] [Need: 06:05:25] [learning_rate=0.0002] [Best : Accuracy=88.17, Error=11.83]

==>>Epoch=[152/250]], [2018-04-01 03:05:32], LR=[0.002], Batch=[32] [Model=SimpleNet]
  Epoch: [152][000/1475]   Time 0.141 (0.141)   Data 0.109 (0.109)   Loss 0.3155 (0.3155)   Prec@1 78.125 (78.125)   Prec@5 78.125 (78.125)   [2018-04-01 03:05:33]
  Epoch: [152][200/1475]   Time 0.140 (0.139)   Data 0.109 (0.105)   Loss 0.3286 (0.3473)   Prec@1 87.500 (85.323)   Prec@5 87.500 (85.323)   [2018-04-01 03:06:00]
  Epoch: [152][400/1475]   Time 0.141 (0.139)   Data 0.094 (0.105)   Loss 0.3078 (0.3421)   Prec@1 90.625 (85.505)   Prec@5 90.625 (85.505)   [2018-04-01 03:06:28]
  Epoch: [152][600/1475]   Time 0.141 (0.139)   Data 0.109 (0.105)   Loss 0.4896 (0.3426)   Prec@1 81.250 (85.483)   Prec@5 81.250 (85.483)   [2018-04-01 03:06:56]
  Epoch: [152][800/1475]   Time 0.143 (0.139)   Data 0.112 (0.105)   Loss 0.2522 (0.3395)   Prec@1 90.625 (85.604)   Prec@5 90.625 (85.604)   [2018-04-01 03:07:24]
  Epoch: [152][1000/1475]   Time 0.134 (0.139)   Data 0.112 (0.105)   Loss 0.3366 (0.3391)   Prec@1 81.250 (85.593)   Prec@5 81.250 (85.593)   [2018-04-01 03:07:51]
  Epoch: [152][1200/1475]   Time 0.136 (0.139)   Data 0.105 (0.105)   Loss 0.4066 (0.3392)   Prec@1 75.000 (85.582)   Prec@5 75.000 (85.582)   [2018-04-01 03:08:19]
  Epoch: [152][1400/1475]   Time 0.137 (0.139)   Data 0.093 (0.105)   Loss 0.2036 (0.3387)   Prec@1 93.750 (85.582)   Prec@5 93.750 (85.582)   [2018-04-01 03:08:47]
  **Train** Prec@1 85.608 Prec@5 85.608 Error@1 14.392
  **VAL** Prec@1 88.074 Prec@5 88.074 Error@1 11.926

==>>[2018-04-01 03:09:15] [Epoch=153/250] [Need: 06:01:41] [learning_rate=0.0002] [Best : Accuracy=88.17, Error=11.83]

==>>Epoch=[153/250]], [2018-04-01 03:09:15], LR=[0.002], Batch=[32] [Model=SimpleNet]
  Epoch: [153][000/1475]   Time 0.144 (0.144)   Data 0.116 (0.116)   Loss 0.3026 (0.3026)   Prec@1 87.500 (87.500)   Prec@5 87.500 (87.500)   [2018-04-01 03:09:15]
  Epoch: [153][200/1475]   Time 0.141 (0.138)   Data 0.109 (0.105)   Loss 0.2973 (0.3443)   Prec@1 84.375 (85.059)   Prec@5 84.375 (85.059)   [2018-04-01 03:09:43]
  Epoch: [153][400/1475]   Time 0.141 (0.139)   Data 0.109 (0.105)   Loss 0.3372 (0.3419)   Prec@1 84.375 (85.521)   Prec@5 84.375 (85.521)   [2018-04-01 03:10:10]
  Epoch: [153][600/1475]   Time 0.125 (0.139)   Data 0.094 (0.105)   Loss 0.2833 (0.3403)   Prec@1 87.500 (85.644)   Prec@5 87.500 (85.644)   [2018-04-01 03:10:38]
  Epoch: [153][800/1475]   Time 0.141 (0.139)   Data 0.109 (0.105)   Loss 0.4496 (0.3379)   Prec@1 81.250 (85.791)   Prec@5 81.250 (85.791)   [2018-04-01 03:11:06]
  Epoch: [153][1000/1475]   Time 0.145 (0.139)   Data 0.109 (0.105)   Loss 0.3239 (0.3363)   Prec@1 87.500 (85.786)   Prec@5 87.500 (85.786)   [2018-04-01 03:11:33]
  Epoch: [153][1200/1475]   Time 0.129 (0.139)   Data 0.098 (0.105)   Loss 0.2093 (0.3383)   Prec@1 90.625 (85.731)   Prec@5 90.625 (85.731)   [2018-04-01 03:12:01]
  Epoch: [153][1400/1475]   Time 0.136 (0.139)   Data 0.095 (0.105)   Loss 0.3386 (0.3383)   Prec@1 87.500 (85.736)   Prec@5 87.500 (85.736)   [2018-04-01 03:12:29]
  **Train** Prec@1 85.727 Prec@5 85.727 Error@1 14.273
  **VAL** Prec@1 88.001 Prec@5 88.001 Error@1 11.999

==>>[2018-04-01 03:12:57] [Epoch=154/250] [Need: 05:57:56] [learning_rate=0.0002] [Best : Accuracy=88.17, Error=11.83]

==>>Epoch=[154/250]], [2018-04-01 03:12:57], LR=[0.002], Batch=[32] [Model=SimpleNet]
  Epoch: [154][000/1475]   Time 0.141 (0.141)   Data 0.109 (0.109)   Loss 0.3426 (0.3426)   Prec@1 87.500 (87.500)   Prec@5 87.500 (87.500)   [2018-04-01 03:12:57]
  Epoch: [154][200/1475]   Time 0.134 (0.139)   Data 0.103 (0.106)   Loss 0.3028 (0.3524)   Prec@1 90.625 (84.919)   Prec@5 90.625 (84.919)   [2018-04-01 03:13:25]
  Epoch: [154][400/1475]   Time 0.141 (0.139)   Data 0.109 (0.105)   Loss 0.1841 (0.3437)   Prec@1 96.875 (85.341)   Prec@5 96.875 (85.341)   [2018-04-01 03:13:53]
  Epoch: [154][600/1475]   Time 0.137 (0.139)   Data 0.106 (0.105)   Loss 0.2419 (0.3369)   Prec@1 90.625 (85.732)   Prec@5 90.625 (85.732)   [2018-04-01 03:14:20]
  Epoch: [154][800/1475]   Time 0.141 (0.139)   Data 0.109 (0.105)   Loss 0.4771 (0.3321)   Prec@1 75.000 (85.912)   Prec@5 75.000 (85.912)   [2018-04-01 03:14:48]
  Epoch: [154][1000/1475]   Time 0.141 (0.139)   Data 0.109 (0.105)   Loss 0.2337 (0.3333)   Prec@1 93.750 (85.855)   Prec@5 93.750 (85.855)   [2018-04-01 03:15:16]
  Epoch: [154][1200/1475]   Time 0.145 (0.139)   Data 0.109 (0.105)   Loss 0.3196 (0.3356)   Prec@1 90.625 (85.824)   Prec@5 90.625 (85.824)   [2018-04-01 03:15:44]
  Epoch: [154][1400/1475]   Time 0.135 (0.139)   Data 0.106 (0.105)   Loss 0.2957 (0.3367)   Prec@1 87.500 (85.751)   Prec@5 87.500 (85.751)   [2018-04-01 03:16:11]
  **Train** Prec@1 85.778 Prec@5 85.778 Error@1 14.222
  **VAL** Prec@1 88.037 Prec@5 88.037 Error@1 11.963

==>>[2018-04-01 03:16:39] [Epoch=155/250] [Need: 05:54:12] [learning_rate=0.0002] [Best : Accuracy=88.17, Error=11.83]

==>>Epoch=[155/250]], [2018-04-01 03:16:39], LR=[0.002], Batch=[32] [Model=SimpleNet]
  Epoch: [155][000/1475]   Time 0.129 (0.129)   Data 0.097 (0.097)   Loss 0.3928 (0.3928)   Prec@1 84.375 (84.375)   Prec@5 84.375 (84.375)   [2018-04-01 03:16:40]
  Epoch: [155][200/1475]   Time 0.125 (0.139)   Data 0.094 (0.105)   Loss 0.5939 (0.3512)   Prec@1 75.000 (85.246)   Prec@5 75.000 (85.246)   [2018-04-01 03:17:07]
  Epoch: [155][400/1475]   Time 0.141 (0.139)   Data 0.094 (0.105)   Loss 0.6436 (0.3465)   Prec@1 75.000 (85.443)   Prec@5 75.000 (85.443)   [2018-04-01 03:17:35]
  Epoch: [155][600/1475]   Time 0.141 (0.139)   Data 0.109 (0.105)   Loss 0.3163 (0.3445)   Prec@1 90.625 (85.353)   Prec@5 90.625 (85.353)   [2018-04-01 03:18:03]
  Epoch: [155][800/1475]   Time 0.125 (0.139)   Data 0.094 (0.105)   Loss 0.3889 (0.3422)   Prec@1 81.250 (85.553)   Prec@5 81.250 (85.553)   [2018-04-01 03:18:30]
  Epoch: [155][1000/1475]   Time 0.149 (0.139)   Data 0.117 (0.105)   Loss 0.3093 (0.3423)   Prec@1 87.500 (85.539)   Prec@5 87.500 (85.539)   [2018-04-01 03:18:58]
  Epoch: [155][1200/1475]   Time 0.141 (0.139)   Data 0.109 (0.105)   Loss 0.4383 (0.3382)   Prec@1 84.375 (85.746)   Prec@5 84.375 (85.746)   [2018-04-01 03:19:26]
  Epoch: [155][1400/1475]   Time 0.138 (0.139)   Data 0.109 (0.105)   Loss 0.5849 (0.3381)   Prec@1 81.250 (85.791)   Prec@5 81.250 (85.791)   [2018-04-01 03:19:54]
  **Train** Prec@1 85.752 Prec@5 85.752 Error@1 14.248
  **VAL** Prec@1 88.037 Prec@5 88.037 Error@1 11.963

==>>[2018-04-01 03:20:22] [Epoch=156/250] [Need: 05:50:27] [learning_rate=0.0002] [Best : Accuracy=88.17, Error=11.83]

==>>Epoch=[156/250]], [2018-04-01 03:20:22], LR=[0.002], Batch=[32] [Model=SimpleNet]
  Epoch: [156][000/1475]   Time 0.141 (0.141)   Data 0.109 (0.109)   Loss 0.3728 (0.3728)   Prec@1 81.250 (81.250)   Prec@5 81.250 (81.250)   [2018-04-01 03:20:22]
  Epoch: [156][200/1475]   Time 0.141 (0.139)   Data 0.109 (0.105)   Loss 0.3269 (0.3490)   Prec@1 93.750 (85.044)   Prec@5 93.750 (85.044)   [2018-04-01 03:20:50]
  Epoch: [156][400/1475]   Time 0.145 (0.139)   Data 0.114 (0.105)   Loss 0.2019 (0.3441)   Prec@1 90.625 (85.513)   Prec@5 90.625 (85.513)   [2018-04-01 03:21:17]
  Epoch: [156][600/1475]   Time 0.138 (0.139)   Data 0.109 (0.105)   Loss 0.3225 (0.3429)   Prec@1 81.250 (85.451)   Prec@5 81.250 (85.451)   [2018-04-01 03:21:45]
  Epoch: [156][800/1475]   Time 0.125 (0.139)   Data 0.094 (0.105)   Loss 0.2619 (0.3406)   Prec@1 93.750 (85.510)   Prec@5 93.750 (85.510)   [2018-04-01 03:22:13]
  Epoch: [156][1000/1475]   Time 0.129 (0.139)   Data 0.098 (0.105)   Loss 0.3460 (0.3390)   Prec@1 87.500 (85.664)   Prec@5 87.500 (85.664)   [2018-04-01 03:22:40]
  Epoch: [156][1200/1475]   Time 0.139 (0.139)   Data 0.100 (0.105)   Loss 0.4852 (0.3381)   Prec@1 78.125 (85.694)   Prec@5 78.125 (85.694)   [2018-04-01 03:23:08]
  Epoch: [156][1400/1475]   Time 0.141 (0.138)   Data 0.109 (0.105)   Loss 0.3916 (0.3377)   Prec@1 81.250 (85.707)   Prec@5 81.250 (85.707)   [2018-04-01 03:23:36]
  **Train** Prec@1 85.697 Prec@5 85.697 Error@1 14.303
  **VAL** Prec@1 87.929 Prec@5 87.929 Error@1 12.071

==>>[2018-04-01 03:24:04] [Epoch=157/250] [Need: 05:46:43] [learning_rate=0.0002] [Best : Accuracy=88.17, Error=11.83]

==>>Epoch=[157/250]], [2018-04-01 03:24:04], LR=[0.002], Batch=[32] [Model=SimpleNet]
  Epoch: [157][000/1475]   Time 0.141 (0.141)   Data 0.094 (0.094)   Loss 0.3026 (0.3026)   Prec@1 87.500 (87.500)   Prec@5 87.500 (87.500)   [2018-04-01 03:24:04]
  Epoch: [157][200/1475]   Time 0.129 (0.138)   Data 0.098 (0.105)   Loss 0.1952 (0.3426)   Prec@1 93.750 (86.163)   Prec@5 93.750 (86.163)   [2018-04-01 03:24:32]
  Epoch: [157][400/1475]   Time 0.141 (0.138)   Data 0.109 (0.105)   Loss 0.3424 (0.3338)   Prec@1 84.375 (86.331)   Prec@5 84.375 (86.331)   [2018-04-01 03:24:59]
  Epoch: [157][600/1475]   Time 0.127 (0.138)   Data 0.096 (0.105)   Loss 0.3998 (0.3350)   Prec@1 78.125 (86.086)   Prec@5 78.125 (86.086)   [2018-04-01 03:25:27]
  Epoch: [157][800/1475]   Time 0.141 (0.138)   Data 0.109 (0.105)   Loss 0.2531 (0.3348)   Prec@1 87.500 (86.084)   Prec@5 87.500 (86.084)   [2018-04-01 03:25:55]
  Epoch: [157][1000/1475]   Time 0.141 (0.138)   Data 0.094 (0.105)   Loss 0.2326 (0.3364)   Prec@1 87.500 (85.983)   Prec@5 87.500 (85.983)   [2018-04-01 03:26:22]
  Epoch: [157][1200/1475]   Time 0.141 (0.138)   Data 0.109 (0.105)   Loss 0.4510 (0.3372)   Prec@1 81.250 (85.845)   Prec@5 81.250 (85.845)   [2018-04-01 03:26:50]
  Epoch: [157][1400/1475]   Time 0.148 (0.138)   Data 0.116 (0.105)   Loss 0.4778 (0.3386)   Prec@1 84.375 (85.776)   Prec@5 84.375 (85.776)   [2018-04-01 03:27:18]
  **Train** Prec@1 85.746 Prec@5 85.746 Error@1 14.254
  **VAL** Prec@1 88.278 Prec@5 88.278 Error@1 11.722

==>>[2018-04-01 03:27:46] [Epoch=158/250] [Need: 05:42:58] [learning_rate=0.0002] [Best : Accuracy=88.28, Error=11.72]

==>>Epoch=[158/250]], [2018-04-01 03:27:46], LR=[0.002], Batch=[32] [Model=SimpleNet]
  Epoch: [158][000/1475]   Time 0.141 (0.141)   Data 0.109 (0.109)   Loss 0.2286 (0.2286)   Prec@1 90.625 (90.625)   Prec@5 90.625 (90.625)   [2018-04-01 03:27:46]
  Epoch: [158][200/1475]   Time 0.130 (0.139)   Data 0.099 (0.106)   Loss 0.4043 (0.3347)   Prec@1 84.375 (86.023)   Prec@5 84.375 (86.023)   [2018-04-01 03:28:14]
  Epoch: [158][400/1475]   Time 0.141 (0.139)   Data 0.109 (0.106)   Loss 0.2695 (0.3346)   Prec@1 90.625 (85.856)   Prec@5 90.625 (85.856)   [2018-04-01 03:28:41]
  Epoch: [158][600/1475]   Time 0.141 (0.139)   Data 0.097 (0.106)   Loss 0.2761 (0.3404)   Prec@1 84.375 (85.555)   Prec@5 84.375 (85.555)   [2018-04-01 03:29:09]
  Epoch: [158][800/1475]   Time 0.129 (0.139)   Data 0.098 (0.105)   Loss 0.3737 (0.3381)   Prec@1 81.250 (85.651)   Prec@5 81.250 (85.651)   [2018-04-01 03:29:37]
  Epoch: [158][1000/1475]   Time 0.144 (0.139)   Data 0.109 (0.105)   Loss 0.4549 (0.3365)   Prec@1 81.250 (85.646)   Prec@5 81.250 (85.646)   [2018-04-01 03:30:05]
  Epoch: [158][1200/1475]   Time 0.149 (0.139)   Data 0.121 (0.105)   Loss 0.1958 (0.3356)   Prec@1 93.750 (85.668)   Prec@5 93.750 (85.668)   [2018-04-01 03:30:32]
  Epoch: [158][1400/1475]   Time 0.145 (0.139)   Data 0.114 (0.105)   Loss 0.3317 (0.3349)   Prec@1 84.375 (85.780)   Prec@5 84.375 (85.780)   [2018-04-01 03:31:00]
  **Train** Prec@1 85.737 Prec@5 85.737 Error@1 14.263
  **VAL** Prec@1 87.965 Prec@5 87.965 Error@1 12.035

==>>[2018-04-01 03:31:28] [Epoch=159/250] [Need: 05:39:13] [learning_rate=0.0002] [Best : Accuracy=88.28, Error=11.72]

==>>Epoch=[159/250]], [2018-04-01 03:31:28], LR=[0.002], Batch=[32] [Model=SimpleNet]
  Epoch: [159][000/1475]   Time 0.144 (0.144)   Data 0.109 (0.109)   Loss 0.4153 (0.4153)   Prec@1 81.250 (81.250)   Prec@5 81.250 (81.250)   [2018-04-01 03:31:28]
  Epoch: [159][200/1475]   Time 0.138 (0.139)   Data 0.106 (0.105)   Loss 0.1529 (0.3286)   Prec@1 96.875 (86.769)   Prec@5 96.875 (86.769)   [2018-04-01 03:31:56]
  Epoch: [159][400/1475]   Time 0.128 (0.139)   Data 0.094 (0.105)   Loss 0.3516 (0.3300)   Prec@1 84.375 (86.245)   Prec@5 84.375 (86.245)   [2018-04-01 03:32:24]
  Epoch: [159][600/1475]   Time 0.140 (0.139)   Data 0.107 (0.105)   Loss 0.2422 (0.3320)   Prec@1 81.250 (86.075)   Prec@5 81.250 (86.075)   [2018-04-01 03:32:52]
  Epoch: [159][800/1475]   Time 0.141 (0.139)   Data 0.109 (0.105)   Loss 0.3428 (0.3341)   Prec@1 87.500 (85.811)   Prec@5 87.500 (85.811)   [2018-04-01 03:33:19]
  Epoch: [159][1000/1475]   Time 0.139 (0.139)   Data 0.121 (0.105)   Loss 0.2156 (0.3353)   Prec@1 96.875 (85.833)   Prec@5 96.875 (85.833)   [2018-04-01 03:33:47]
  Epoch: [159][1200/1475]   Time 0.137 (0.139)   Data 0.113 (0.105)   Loss 0.4644 (0.3367)   Prec@1 81.250 (85.780)   Prec@5 81.250 (85.780)   [2018-04-01 03:34:15]
  Epoch: [159][1400/1475]   Time 0.145 (0.139)   Data 0.098 (0.105)   Loss 0.3033 (0.3362)   Prec@1 87.500 (85.805)   Prec@5 87.500 (85.805)   [2018-04-01 03:34:43]
  **Train** Prec@1 85.828 Prec@5 85.828 Error@1 14.172
  **VAL** Prec@1 88.434 Prec@5 88.434 Error@1 11.566

==>>[2018-04-01 03:35:11] [Epoch=160/250] [Need: 05:35:29] [learning_rate=0.0002] [Best : Accuracy=88.43, Error=11.57]

==>>Epoch=[160/250]], [2018-04-01 03:35:11], LR=[0.002], Batch=[32] [Model=SimpleNet]
  Epoch: [160][000/1475]   Time 0.141 (0.141)   Data 0.109 (0.109)   Loss 0.3318 (0.3318)   Prec@1 84.375 (84.375)   Prec@5 84.375 (84.375)   [2018-04-01 03:35:11]
  Epoch: [160][200/1475]   Time 0.125 (0.139)   Data 0.094 (0.105)   Loss 0.5326 (0.3375)   Prec@1 81.250 (85.479)   Prec@5 81.250 (85.479)   [2018-04-01 03:35:39]
  Epoch: [160][400/1475]   Time 0.141 (0.138)   Data 0.094 (0.105)   Loss 0.3468 (0.3360)   Prec@1 90.625 (85.599)   Prec@5 90.625 (85.599)   [2018-04-01 03:36:06]
  Epoch: [160][600/1475]   Time 0.125 (0.138)   Data 0.094 (0.105)   Loss 0.2721 (0.3382)   Prec@1 81.250 (85.685)   Prec@5 81.250 (85.685)   [2018-04-01 03:36:34]
  Epoch: [160][800/1475]   Time 0.142 (0.138)   Data 0.111 (0.105)   Loss 0.2639 (0.3396)   Prec@1 87.500 (85.682)   Prec@5 87.500 (85.682)   [2018-04-01 03:37:02]
  Epoch: [160][1000/1475]   Time 0.136 (0.138)   Data 0.109 (0.105)   Loss 0.3545 (0.3367)   Prec@1 84.375 (85.783)   Prec@5 84.375 (85.783)   [2018-04-01 03:37:29]
  Epoch: [160][1200/1475]   Time 0.140 (0.138)   Data 0.103 (0.105)   Loss 0.3370 (0.3367)   Prec@1 90.625 (85.824)   Prec@5 90.625 (85.824)   [2018-04-01 03:37:57]
  Epoch: [160][1400/1475]   Time 0.131 (0.138)   Data 0.100 (0.105)   Loss 0.2601 (0.3357)   Prec@1 90.625 (85.803)   Prec@5 90.625 (85.803)   [2018-04-01 03:38:25]
  **Train** Prec@1 85.722 Prec@5 85.722 Error@1 14.278
  **VAL** Prec@1 88.278 Prec@5 88.278 Error@1 11.722

==>>[2018-04-01 03:38:53] [Epoch=161/250] [Need: 05:31:45] [learning_rate=0.0002] [Best : Accuracy=88.43, Error=11.57]

==>>Epoch=[161/250]], [2018-04-01 03:38:53], LR=[0.002], Batch=[32] [Model=SimpleNet]
  Epoch: [161][000/1475]   Time 0.148 (0.148)   Data 0.101 (0.101)   Loss 0.2981 (0.2981)   Prec@1 90.625 (90.625)   Prec@5 90.625 (90.625)   [2018-04-01 03:38:53]
  Epoch: [161][200/1475]   Time 0.141 (0.139)   Data 0.109 (0.105)   Loss 0.2879 (0.3458)   Prec@1 90.625 (85.463)   Prec@5 90.625 (85.463)   [2018-04-01 03:39:21]
  Epoch: [161][400/1475]   Time 0.141 (0.139)   Data 0.109 (0.105)   Loss 0.2514 (0.3424)   Prec@1 90.625 (85.622)   Prec@5 90.625 (85.622)   [2018-04-01 03:39:49]
  Epoch: [161][600/1475]   Time 0.125 (0.139)   Data 0.094 (0.105)   Loss 0.3546 (0.3400)   Prec@1 78.125 (85.649)   Prec@5 78.125 (85.649)   [2018-04-01 03:40:16]
  Epoch: [161][800/1475]   Time 0.125 (0.139)   Data 0.093 (0.105)   Loss 0.4505 (0.3363)   Prec@1 84.375 (85.744)   Prec@5 84.375 (85.744)   [2018-04-01 03:40:44]
  Epoch: [161][1000/1475]   Time 0.138 (0.139)   Data 0.107 (0.105)   Loss 0.3150 (0.3367)   Prec@1 84.375 (85.683)   Prec@5 84.375 (85.683)   [2018-04-01 03:41:12]
  Epoch: [161][1200/1475]   Time 0.141 (0.139)   Data 0.109 (0.105)   Loss 0.2385 (0.3354)   Prec@1 93.750 (85.744)   Prec@5 93.750 (85.744)   [2018-04-01 03:41:40]
  Epoch: [161][1400/1475]   Time 0.141 (0.139)   Data 0.109 (0.105)   Loss 0.3675 (0.3365)   Prec@1 87.500 (85.671)   Prec@5 87.500 (85.671)   [2018-04-01 03:42:07]
  **Train** Prec@1 85.614 Prec@5 85.614 Error@1 14.386
  **VAL** Prec@1 88.025 Prec@5 88.025 Error@1 11.975

==>>[2018-04-01 03:42:36] [Epoch=162/250] [Need: 05:28:00] [learning_rate=0.0002] [Best : Accuracy=88.43, Error=11.57]

==>>Epoch=[162/250]], [2018-04-01 03:42:36], LR=[0.002], Batch=[32] [Model=SimpleNet]
  Epoch: [162][000/1475]   Time 0.128 (0.128)   Data 0.097 (0.097)   Loss 0.3751 (0.3751)   Prec@1 87.500 (87.500)   Prec@5 87.500 (87.500)   [2018-04-01 03:42:36]
  Epoch: [162][200/1475]   Time 0.136 (0.139)   Data 0.105 (0.106)   Loss 0.3414 (0.3334)   Prec@1 84.375 (85.650)   Prec@5 84.375 (85.650)   [2018-04-01 03:43:03]
  Epoch: [162][400/1475]   Time 0.133 (0.139)   Data 0.102 (0.105)   Loss 0.3071 (0.3348)   Prec@1 84.375 (85.692)   Prec@5 84.375 (85.692)   [2018-04-01 03:43:31]
  Epoch: [162][600/1475]   Time 0.156 (0.139)   Data 0.125 (0.105)   Loss 0.3217 (0.3376)   Prec@1 87.500 (85.587)   Prec@5 87.500 (85.587)   [2018-04-01 03:43:59]
  Epoch: [162][800/1475]   Time 0.136 (0.139)   Data 0.105 (0.105)   Loss 0.2674 (0.3390)   Prec@1 90.625 (85.542)   Prec@5 90.625 (85.542)   [2018-04-01 03:44:27]
  Epoch: [162][1000/1475]   Time 0.141 (0.139)   Data 0.110 (0.105)   Loss 0.2817 (0.3365)   Prec@1 90.625 (85.702)   Prec@5 90.625 (85.702)   [2018-04-01 03:44:54]
  Epoch: [162][1200/1475]   Time 0.125 (0.138)   Data 0.094 (0.105)   Loss 0.2202 (0.3371)   Prec@1 84.375 (85.671)   Prec@5 84.375 (85.671)   [2018-04-01 03:45:22]
  Epoch: [162][1400/1475]   Time 0.143 (0.138)   Data 0.096 (0.105)   Loss 0.3144 (0.3357)   Prec@1 81.250 (85.794)   Prec@5 81.250 (85.794)   [2018-04-01 03:45:50]
  **Train** Prec@1 85.809 Prec@5 85.809 Error@1 14.191
  **VAL** Prec@1 87.857 Prec@5 87.857 Error@1 12.143

==>>[2018-04-01 03:46:18] [Epoch=163/250] [Need: 05:24:16] [learning_rate=0.0002] [Best : Accuracy=88.43, Error=11.57]

==>>Epoch=[163/250]], [2018-04-01 03:46:18], LR=[0.002], Batch=[32] [Model=SimpleNet]
  Epoch: [163][000/1475]   Time 0.133 (0.133)   Data 0.094 (0.094)   Loss 0.3012 (0.3012)   Prec@1 87.500 (87.500)   Prec@5 87.500 (87.500)   [2018-04-01 03:46:18]
  Epoch: [163][200/1475]   Time 0.144 (0.139)   Data 0.113 (0.105)   Loss 0.2987 (0.3389)   Prec@1 90.625 (85.417)   Prec@5 90.625 (85.417)   [2018-04-01 03:46:46]
  Epoch: [163][400/1475]   Time 0.141 (0.139)   Data 0.109 (0.105)   Loss 0.2628 (0.3350)   Prec@1 84.375 (85.591)   Prec@5 84.375 (85.591)   [2018-04-01 03:47:14]
  Epoch: [163][600/1475]   Time 0.125 (0.139)   Data 0.094 (0.105)   Loss 0.5804 (0.3352)   Prec@1 75.000 (85.613)   Prec@5 75.000 (85.613)   [2018-04-01 03:47:41]
  Epoch: [163][800/1475]   Time 0.141 (0.139)   Data 0.109 (0.105)   Loss 0.3323 (0.3379)   Prec@1 87.500 (85.627)   Prec@5 87.500 (85.627)   [2018-04-01 03:48:09]
  Epoch: [163][1000/1475]   Time 0.141 (0.139)   Data 0.109 (0.105)   Loss 0.3701 (0.3364)   Prec@1 84.375 (85.730)   Prec@5 84.375 (85.730)   [2018-04-01 03:48:37]
  Epoch: [163][1200/1475]   Time 0.133 (0.139)   Data 0.097 (0.105)   Loss 0.2992 (0.3358)   Prec@1 84.375 (85.728)   Prec@5 84.375 (85.728)   [2018-04-01 03:49:05]
  Epoch: [163][1400/1475]   Time 0.144 (0.139)   Data 0.113 (0.105)   Loss 0.2419 (0.3363)   Prec@1 90.625 (85.716)   Prec@5 90.625 (85.716)   [2018-04-01 03:49:32]
  **Train** Prec@1 85.703 Prec@5 85.703 Error@1 14.297
  **VAL** Prec@1 88.074 Prec@5 88.074 Error@1 11.926

==>>[2018-04-01 03:50:01] [Epoch=164/250] [Need: 05:20:32] [learning_rate=0.0002] [Best : Accuracy=88.43, Error=11.57]

==>>Epoch=[164/250]], [2018-04-01 03:50:01], LR=[0.002], Batch=[32] [Model=SimpleNet]
  Epoch: [164][000/1475]   Time 0.141 (0.141)   Data 0.094 (0.094)   Loss 0.3560 (0.3560)   Prec@1 87.500 (87.500)   Prec@5 87.500 (87.500)   [2018-04-01 03:50:01]
  Epoch: [164][200/1475]   Time 0.141 (0.139)   Data 0.109 (0.105)   Loss 0.5013 (0.3279)   Prec@1 81.250 (86.070)   Prec@5 81.250 (86.070)   [2018-04-01 03:50:29]
  Epoch: [164][400/1475]   Time 0.130 (0.138)   Data 0.099 (0.105)   Loss 0.3727 (0.3274)   Prec@1 90.625 (86.066)   Prec@5 90.625 (86.066)   [2018-04-01 03:50:56]
  Epoch: [164][600/1475]   Time 0.141 (0.138)   Data 0.109 (0.105)   Loss 0.3079 (0.3302)   Prec@1 87.500 (86.049)   Prec@5 87.500 (86.049)   [2018-04-01 03:51:24]
  Epoch: [164][800/1475]   Time 0.141 (0.139)   Data 0.094 (0.106)   Loss 0.3689 (0.3328)   Prec@1 81.250 (85.893)   Prec@5 81.250 (85.893)   [2018-04-01 03:51:52]
  Epoch: [164][1000/1475]   Time 0.126 (0.139)   Data 0.095 (0.106)   Loss 0.4538 (0.3331)   Prec@1 81.250 (85.905)   Prec@5 81.250 (85.905)   [2018-04-01 03:52:19]
  Epoch: [164][1200/1475]   Time 0.142 (0.139)   Data 0.109 (0.106)   Loss 0.2987 (0.3341)   Prec@1 90.625 (85.910)   Prec@5 90.625 (85.910)   [2018-04-01 03:52:47]
  Epoch: [164][1400/1475]   Time 0.145 (0.139)   Data 0.109 (0.106)   Loss 0.4788 (0.3339)   Prec@1 81.250 (85.881)   Prec@5 81.250 (85.881)   [2018-04-01 03:53:15]
  **Train** Prec@1 85.839 Prec@5 85.839 Error@1 14.161
  **VAL** Prec@1 88.037 Prec@5 88.037 Error@1 11.963

==>>[2018-04-01 03:53:43] [Epoch=165/250] [Need: 05:16:48] [learning_rate=0.0002] [Best : Accuracy=88.43, Error=11.57]

==>>Epoch=[165/250]], [2018-04-01 03:53:43], LR=[0.002], Batch=[32] [Model=SimpleNet]
  Epoch: [165][000/1475]   Time 0.132 (0.132)   Data 0.101 (0.101)   Loss 0.1638 (0.1638)   Prec@1 93.750 (93.750)   Prec@5 93.750 (93.750)   [2018-04-01 03:53:43]
  Epoch: [165][200/1475]   Time 0.141 (0.139)   Data 0.109 (0.105)   Loss 0.3367 (0.3367)   Prec@1 87.500 (85.525)   Prec@5 87.500 (85.525)   [2018-04-01 03:54:11]
  Epoch: [165][400/1475]   Time 0.125 (0.139)   Data 0.094 (0.105)   Loss 0.1666 (0.3364)   Prec@1 93.750 (85.521)   Prec@5 93.750 (85.521)   [2018-04-01 03:54:39]
  Epoch: [165][600/1475]   Time 0.141 (0.139)   Data 0.109 (0.105)   Loss 0.2341 (0.3364)   Prec@1 90.625 (85.576)   Prec@5 90.625 (85.576)   [2018-04-01 03:55:06]
  Epoch: [165][800/1475]   Time 0.124 (0.139)   Data 0.105 (0.105)   Loss 0.1793 (0.3365)   Prec@1 93.750 (85.510)   Prec@5 93.750 (85.510)   [2018-04-01 03:55:34]
  Epoch: [165][1000/1475]   Time 0.138 (0.139)   Data 0.113 (0.105)   Loss 0.3993 (0.3350)   Prec@1 81.250 (85.618)   Prec@5 81.250 (85.618)   [2018-04-01 03:56:02]
  Epoch: [165][1200/1475]   Time 0.141 (0.139)   Data 0.109 (0.105)   Loss 0.3836 (0.3361)   Prec@1 84.375 (85.515)   Prec@5 84.375 (85.515)   [2018-04-01 03:56:30]
  Epoch: [165][1400/1475]   Time 0.144 (0.139)   Data 0.121 (0.105)   Loss 0.3369 (0.3354)   Prec@1 84.375 (85.666)   Prec@5 84.375 (85.666)   [2018-04-01 03:56:57]
  **Train** Prec@1 85.706 Prec@5 85.706 Error@1 14.294
  **VAL** Prec@1 88.110 Prec@5 88.110 Error@1 11.890

==>>[2018-04-01 03:57:26] [Epoch=166/250] [Need: 05:13:03] [learning_rate=0.0002] [Best : Accuracy=88.43, Error=11.57]

==>>Epoch=[166/250]], [2018-04-01 03:57:26], LR=[0.002], Batch=[32] [Model=SimpleNet]
  Epoch: [166][000/1475]   Time 0.141 (0.141)   Data 0.094 (0.094)   Loss 0.3142 (0.3142)   Prec@1 90.625 (90.625)   Prec@5 90.625 (90.625)   [2018-04-01 03:57:26]
  Epoch: [166][200/1475]   Time 0.147 (0.139)   Data 0.109 (0.105)   Loss 0.2867 (0.3317)   Prec@1 87.500 (86.007)   Prec@5 87.500 (86.007)   [2018-04-01 03:57:53]
  Epoch: [166][400/1475]   Time 0.147 (0.139)   Data 0.113 (0.105)   Loss 0.3014 (0.3354)   Prec@1 87.500 (85.762)   Prec@5 87.500 (85.762)   [2018-04-01 03:58:21]
  Epoch: [166][600/1475]   Time 0.141 (0.138)   Data 0.094 (0.105)   Loss 0.1965 (0.3344)   Prec@1 93.750 (85.888)   Prec@5 93.750 (85.888)   [2018-04-01 03:58:49]
  Epoch: [166][800/1475]   Time 0.148 (0.139)   Data 0.109 (0.105)   Loss 0.5219 (0.3344)   Prec@1 81.250 (85.830)   Prec@5 81.250 (85.830)   [2018-04-01 03:59:17]
  Epoch: [166][1000/1475]   Time 0.151 (0.139)   Data 0.117 (0.105)   Loss 0.2814 (0.3348)   Prec@1 87.500 (85.861)   Prec@5 87.500 (85.861)   [2018-04-01 03:59:44]
  Epoch: [166][1200/1475]   Time 0.128 (0.139)   Data 0.108 (0.105)   Loss 0.4375 (0.3364)   Prec@1 81.250 (85.788)   Prec@5 81.250 (85.788)   [2018-04-01 04:00:12]
  Epoch: [166][1400/1475]   Time 0.141 (0.139)   Data 0.109 (0.105)   Loss 0.2936 (0.3363)   Prec@1 84.375 (85.684)   Prec@5 84.375 (85.684)   [2018-04-01 04:00:40]
  **Train** Prec@1 85.646 Prec@5 85.646 Error@1 14.354
  **VAL** Prec@1 88.098 Prec@5 88.098 Error@1 11.902

==>>[2018-04-01 04:01:08] [Epoch=167/250] [Need: 05:09:19] [learning_rate=0.0002] [Best : Accuracy=88.43, Error=11.57]

==>>Epoch=[167/250]], [2018-04-01 04:01:08], LR=[0.002], Batch=[32] [Model=SimpleNet]
  Epoch: [167][000/1475]   Time 0.141 (0.141)   Data 0.109 (0.109)   Loss 0.4191 (0.4191)   Prec@1 78.125 (78.125)   Prec@5 78.125 (78.125)   [2018-04-01 04:01:08]
  Epoch: [167][200/1475]   Time 0.136 (0.139)   Data 0.105 (0.106)   Loss 0.2791 (0.3358)   Prec@1 84.375 (85.634)   Prec@5 84.375 (85.634)   [2018-04-01 04:01:36]
  Epoch: [167][400/1475]   Time 0.141 (0.139)   Data 0.109 (0.106)   Loss 0.3919 (0.3392)   Prec@1 84.375 (85.419)   Prec@5 84.375 (85.419)   [2018-04-01 04:02:04]
  Epoch: [167][600/1475]   Time 0.145 (0.139)   Data 0.109 (0.106)   Loss 0.3037 (0.3400)   Prec@1 81.250 (85.368)   Prec@5 81.250 (85.368)   [2018-04-01 04:02:31]
  Epoch: [167][800/1475]   Time 0.145 (0.139)   Data 0.114 (0.106)   Loss 0.2443 (0.3397)   Prec@1 90.625 (85.401)   Prec@5 90.625 (85.401)   [2018-04-01 04:02:59]
  Epoch: [167][1000/1475]   Time 0.131 (0.139)   Data 0.100 (0.106)   Loss 0.3125 (0.3392)   Prec@1 87.500 (85.486)   Prec@5 87.500 (85.486)   [2018-04-01 04:03:27]
  Epoch: [167][1200/1475]   Time 0.126 (0.139)   Data 0.095 (0.106)   Loss 0.2122 (0.3403)   Prec@1 93.750 (85.439)   Prec@5 93.750 (85.439)   [2018-04-01 04:03:54]
  Epoch: [167][1400/1475]   Time 0.130 (0.139)   Data 0.099 (0.106)   Loss 0.4046 (0.3391)   Prec@1 84.375 (85.488)   Prec@5 84.375 (85.488)   [2018-04-01 04:04:22]
  **Train** Prec@1 85.464 Prec@5 85.464 Error@1 14.536
  **VAL** Prec@1 88.278 Prec@5 88.278 Error@1 11.722

==>>[2018-04-01 04:04:50] [Epoch=168/250] [Need: 05:05:35] [learning_rate=0.0002] [Best : Accuracy=88.43, Error=11.57]

==>>Epoch=[168/250]], [2018-04-01 04:04:50], LR=[0.002], Batch=[32] [Model=SimpleNet]
  Epoch: [168][000/1475]   Time 0.141 (0.141)   Data 0.109 (0.109)   Loss 0.2850 (0.2850)   Prec@1 87.500 (87.500)   Prec@5 87.500 (87.500)   [2018-04-01 04:04:51]
  Epoch: [168][200/1475]   Time 0.141 (0.139)   Data 0.109 (0.105)   Loss 0.3794 (0.3459)   Prec@1 81.250 (85.121)   Prec@5 81.250 (85.121)   [2018-04-01 04:05:18]
  Epoch: [168][400/1475]   Time 0.125 (0.139)   Data 0.094 (0.105)   Loss 0.4696 (0.3439)   Prec@1 78.125 (85.263)   Prec@5 78.125 (85.263)   [2018-04-01 04:05:46]
  Epoch: [168][600/1475]   Time 0.129 (0.139)   Data 0.098 (0.105)   Loss 0.3853 (0.3438)   Prec@1 87.500 (85.347)   Prec@5 87.500 (85.347)   [2018-04-01 04:06:14]
  Epoch: [168][800/1475]   Time 0.129 (0.139)   Data 0.098 (0.105)   Loss 0.2272 (0.3372)   Prec@1 90.625 (85.698)   Prec@5 90.625 (85.698)   [2018-04-01 04:06:41]
  Epoch: [168][1000/1475]   Time 0.129 (0.139)   Data 0.098 (0.105)   Loss 0.4993 (0.3380)   Prec@1 71.875 (85.711)   Prec@5 71.875 (85.711)   [2018-04-01 04:07:09]
  Epoch: [168][1200/1475]   Time 0.141 (0.139)   Data 0.109 (0.105)   Loss 0.2053 (0.3366)   Prec@1 90.625 (85.832)   Prec@5 90.625 (85.832)   [2018-04-01 04:07:37]
  Epoch: [168][1400/1475]   Time 0.125 (0.139)   Data 0.094 (0.105)   Loss 0.4754 (0.3375)   Prec@1 78.125 (85.753)   Prec@5 78.125 (85.753)   [2018-04-01 04:08:05]
  **Train** Prec@1 85.763 Prec@5 85.763 Error@1 14.237
  **VAL** Prec@1 88.122 Prec@5 88.122 Error@1 11.878

==>>[2018-04-01 04:08:33] [Epoch=169/250] [Need: 05:01:51] [learning_rate=0.0002] [Best : Accuracy=88.43, Error=11.57]

==>>Epoch=[169/250]], [2018-04-01 04:08:33], LR=[0.002], Batch=[32] [Model=SimpleNet]
  Epoch: [169][000/1475]   Time 0.141 (0.141)   Data 0.109 (0.109)   Loss 0.2987 (0.2987)   Prec@1 81.250 (81.250)   Prec@5 81.250 (81.250)   [2018-04-01 04:08:33]
  Epoch: [169][200/1475]   Time 0.141 (0.139)   Data 0.109 (0.106)   Loss 0.4813 (0.3413)   Prec@1 84.375 (85.634)   Prec@5 84.375 (85.634)   [2018-04-01 04:09:01]
  Epoch: [169][400/1475]   Time 0.145 (0.139)   Data 0.114 (0.106)   Loss 0.4152 (0.3415)   Prec@1 81.250 (85.419)   Prec@5 81.250 (85.419)   [2018-04-01 04:09:28]
  Epoch: [169][600/1475]   Time 0.153 (0.139)   Data 0.113 (0.106)   Loss 0.4427 (0.3381)   Prec@1 78.125 (85.644)   Prec@5 78.125 (85.644)   [2018-04-01 04:09:56]
  Epoch: [169][800/1475]   Time 0.141 (0.139)   Data 0.109 (0.106)   Loss 0.2194 (0.3353)   Prec@1 93.750 (85.799)   Prec@5 93.750 (85.799)   [2018-04-01 04:10:24]
  Epoch: [169][1000/1475]   Time 0.141 (0.139)   Data 0.109 (0.106)   Loss 0.2810 (0.3364)   Prec@1 84.375 (85.739)   Prec@5 84.375 (85.739)   [2018-04-01 04:10:52]
  Epoch: [169][1200/1475]   Time 0.141 (0.139)   Data 0.094 (0.106)   Loss 0.2191 (0.3379)   Prec@1 90.625 (85.751)   Prec@5 90.625 (85.751)   [2018-04-01 04:11:19]
  Epoch: [169][1400/1475]   Time 0.141 (0.139)   Data 0.109 (0.106)   Loss 0.3009 (0.3361)   Prec@1 87.500 (85.798)   Prec@5 87.500 (85.798)   [2018-04-01 04:11:47]
  **Train** Prec@1 85.795 Prec@5 85.795 Error@1 14.205
  **VAL** Prec@1 88.266 Prec@5 88.266 Error@1 11.734

==>>[2018-04-01 04:12:15] [Epoch=170/250] [Need: 04:58:07] [learning_rate=0.0002] [Best : Accuracy=88.43, Error=11.57]

==>>Epoch=[170/250]], [2018-04-01 04:12:15], LR=[0.002], Batch=[32] [Model=SimpleNet]
  Epoch: [170][000/1475]   Time 0.141 (0.141)   Data 0.109 (0.109)   Loss 0.3367 (0.3367)   Prec@1 84.375 (84.375)   Prec@5 84.375 (84.375)   [2018-04-01 04:12:15]
  Epoch: [170][200/1475]   Time 0.137 (0.139)   Data 0.106 (0.106)   Loss 0.5230 (0.3239)   Prec@1 71.875 (86.738)   Prec@5 71.875 (86.738)   [2018-04-01 04:12:43]
  Epoch: [170][400/1475]   Time 0.125 (0.139)   Data 0.094 (0.106)   Loss 0.3351 (0.3357)   Prec@1 84.375 (86.074)   Prec@5 84.375 (86.074)   [2018-04-01 04:13:11]
  Epoch: [170][600/1475]   Time 0.141 (0.139)   Data 0.109 (0.105)   Loss 0.5083 (0.3311)   Prec@1 75.000 (86.252)   Prec@5 75.000 (86.252)   [2018-04-01 04:13:39]
  Epoch: [170][800/1475]   Time 0.132 (0.139)   Data 0.101 (0.105)   Loss 0.6158 (0.3331)   Prec@1 65.625 (86.107)   Prec@5 65.625 (86.107)   [2018-04-01 04:14:06]
  Epoch: [170][1000/1475]   Time 0.137 (0.139)   Data 0.102 (0.106)   Loss 0.3201 (0.3341)   Prec@1 84.375 (85.973)   Prec@5 84.375 (85.973)   [2018-04-01 04:14:34]
  Epoch: [170][1200/1475]   Time 0.128 (0.139)   Data 0.097 (0.106)   Loss 0.3717 (0.3324)   Prec@1 81.250 (86.043)   Prec@5 81.250 (86.043)   [2018-04-01 04:15:02]
  Epoch: [170][1400/1475]   Time 0.141 (0.139)   Data 0.109 (0.105)   Loss 0.3010 (0.3335)   Prec@1 87.500 (85.925)   Prec@5 87.500 (85.925)   [2018-04-01 04:15:29]
  **Train** Prec@1 85.907 Prec@5 85.907 Error@1 14.093
  **VAL** Prec@1 88.242 Prec@5 88.242 Error@1 11.758

==>>[2018-04-01 04:15:58] [Epoch=171/250] [Need: 04:54:23] [learning_rate=0.0002] [Best : Accuracy=88.43, Error=11.57]

==>>Epoch=[171/250]], [2018-04-01 04:15:58], LR=[0.002], Batch=[32] [Model=SimpleNet]
  Epoch: [171][000/1475]   Time 0.127 (0.127)   Data 0.096 (0.096)   Loss 0.2015 (0.2015)   Prec@1 93.750 (93.750)   Prec@5 93.750 (93.750)   [2018-04-01 04:15:58]
  Epoch: [171][200/1475]   Time 0.141 (0.138)   Data 0.109 (0.106)   Loss 0.3461 (0.3322)   Prec@1 84.375 (86.178)   Prec@5 84.375 (86.178)   [2018-04-01 04:16:26]
  Epoch: [171][400/1475]   Time 0.141 (0.138)   Data 0.094 (0.106)   Loss 0.2888 (0.3359)   Prec@1 93.750 (86.113)   Prec@5 93.750 (86.113)   [2018-04-01 04:16:53]
  Epoch: [171][600/1475]   Time 0.125 (0.139)   Data 0.094 (0.105)   Loss 0.5018 (0.3375)   Prec@1 84.375 (85.966)   Prec@5 84.375 (85.966)   [2018-04-01 04:17:21]
  Epoch: [171][800/1475]   Time 0.141 (0.139)   Data 0.109 (0.105)   Loss 0.2180 (0.3384)   Prec@1 93.750 (85.877)   Prec@5 93.750 (85.877)   [2018-04-01 04:17:49]
  Epoch: [171][1000/1475]   Time 0.134 (0.139)   Data 0.103 (0.106)   Loss 0.4915 (0.3366)   Prec@1 87.500 (85.917)   Prec@5 87.500 (85.917)   [2018-04-01 04:18:16]
  Epoch: [171][1200/1475]   Time 0.145 (0.139)   Data 0.109 (0.105)   Loss 0.3769 (0.3368)   Prec@1 84.375 (85.879)   Prec@5 84.375 (85.879)   [2018-04-01 04:18:44]
  Epoch: [171][1400/1475]   Time 0.145 (0.139)   Data 0.109 (0.105)   Loss 0.3831 (0.3365)   Prec@1 78.125 (85.932)   Prec@5 78.125 (85.932)   [2018-04-01 04:19:12]
  **Train** Prec@1 85.905 Prec@5 85.905 Error@1 14.095
  **VAL** Prec@1 88.001 Prec@5 88.001 Error@1 11.999

==>>[2018-04-01 04:19:40] [Epoch=172/250] [Need: 04:50:39] [learning_rate=0.0002] [Best : Accuracy=88.43, Error=11.57]

==>>Epoch=[172/250]], [2018-04-01 04:19:40], LR=[0.002], Batch=[32] [Model=SimpleNet]
  Epoch: [172][000/1475]   Time 0.141 (0.141)   Data 0.109 (0.109)   Loss 0.3576 (0.3576)   Prec@1 87.500 (87.500)   Prec@5 87.500 (87.500)   [2018-04-01 04:19:40]
  Epoch: [172][200/1475]   Time 0.144 (0.139)   Data 0.113 (0.105)   Loss 0.3033 (0.3347)   Prec@1 87.500 (86.474)   Prec@5 87.500 (86.474)   [2018-04-01 04:20:08]
  Epoch: [172][400/1475]   Time 0.141 (0.139)   Data 0.109 (0.105)   Loss 0.2808 (0.3331)   Prec@1 90.625 (86.401)   Prec@5 90.625 (86.401)   [2018-04-01 04:20:36]
  Epoch: [172][600/1475]   Time 0.141 (0.139)   Data 0.109 (0.105)   Loss 0.2473 (0.3308)   Prec@1 87.500 (86.340)   Prec@5 87.500 (86.340)   [2018-04-01 04:21:03]
  Epoch: [172][800/1475]   Time 0.150 (0.139)   Data 0.116 (0.105)   Loss 0.5017 (0.3343)   Prec@1 75.000 (86.045)   Prec@5 75.000 (86.045)   [2018-04-01 04:21:31]
  Epoch: [172][1000/1475]   Time 0.141 (0.139)   Data 0.109 (0.105)   Loss 0.5180 (0.3358)   Prec@1 75.000 (85.936)   Prec@5 75.000 (85.936)   [2018-04-01 04:21:59]
  Epoch: [172][1200/1475]   Time 0.142 (0.139)   Data 0.095 (0.105)   Loss 0.3652 (0.3326)   Prec@1 84.375 (86.058)   Prec@5 84.375 (86.058)   [2018-04-01 04:22:27]
  Epoch: [172][1400/1475]   Time 0.140 (0.139)   Data 0.093 (0.105)   Loss 0.3430 (0.3334)   Prec@1 84.375 (86.019)   Prec@5 84.375 (86.019)   [2018-04-01 04:22:54]
  **Train** Prec@1 85.990 Prec@5 85.990 Error@1 14.010
  **VAL** Prec@1 88.098 Prec@5 88.098 Error@1 11.902

==>>[2018-04-01 04:23:23] [Epoch=173/250] [Need: 04:46:54] [learning_rate=0.0002] [Best : Accuracy=88.43, Error=11.57]

==>>Epoch=[173/250]], [2018-04-01 04:23:23], LR=[0.002], Batch=[32] [Model=SimpleNet]
  Epoch: [173][000/1475]   Time 0.145 (0.145)   Data 0.114 (0.114)   Loss 0.3142 (0.3142)   Prec@1 84.375 (84.375)   Prec@5 84.375 (84.375)   [2018-04-01 04:23:23]
  Epoch: [173][200/1475]   Time 0.149 (0.138)   Data 0.116 (0.106)   Loss 0.2919 (0.3322)   Prec@1 87.500 (85.976)   Prec@5 87.500 (85.976)   [2018-04-01 04:23:50]
  Epoch: [173][400/1475]   Time 0.144 (0.138)   Data 0.113 (0.106)   Loss 0.2101 (0.3349)   Prec@1 93.750 (85.895)   Prec@5 93.750 (85.895)   [2018-04-01 04:24:18]
  Epoch: [173][600/1475]   Time 0.131 (0.138)   Data 0.100 (0.106)   Loss 0.2274 (0.3353)   Prec@1 93.750 (85.883)   Prec@5 93.750 (85.883)   [2018-04-01 04:24:46]
  Epoch: [173][800/1475]   Time 0.144 (0.138)   Data 0.109 (0.106)   Loss 0.1850 (0.3352)   Prec@1 93.750 (85.795)   Prec@5 93.750 (85.795)   [2018-04-01 04:25:14]
  Epoch: [173][1000/1475]   Time 0.141 (0.138)   Data 0.094 (0.106)   Loss 0.2145 (0.3356)   Prec@1 93.750 (85.802)   Prec@5 93.750 (85.802)   [2018-04-01 04:25:41]
  Epoch: [173][1200/1475]   Time 0.125 (0.138)   Data 0.094 (0.105)   Loss 0.2363 (0.3352)   Prec@1 90.625 (85.887)   Prec@5 90.625 (85.887)   [2018-04-01 04:26:09]
  Epoch: [173][1400/1475]   Time 0.141 (0.138)   Data 0.109 (0.105)   Loss 0.4202 (0.3333)   Prec@1 84.375 (85.988)   Prec@5 84.375 (85.988)   [2018-04-01 04:26:37]
  **Train** Prec@1 85.992 Prec@5 85.992 Error@1 14.008
  **VAL** Prec@1 88.434 Prec@5 88.434 Error@1 11.566

==>>[2018-04-01 04:27:05] [Epoch=174/250] [Need: 04:43:10] [learning_rate=0.0002] [Best : Accuracy=88.43, Error=11.57]

==>>Epoch=[174/250]], [2018-04-01 04:27:05], LR=[0.002], Batch=[32] [Model=SimpleNet]
  Epoch: [174][000/1475]   Time 0.141 (0.141)   Data 0.109 (0.109)   Loss 0.3032 (0.3032)   Prec@1 84.375 (84.375)   Prec@5 84.375 (84.375)   [2018-04-01 04:27:05]
  Epoch: [174][200/1475]   Time 0.141 (0.139)   Data 0.110 (0.105)   Loss 0.3165 (0.3322)   Prec@1 84.375 (85.712)   Prec@5 84.375 (85.712)   [2018-04-01 04:27:33]
  Epoch: [174][400/1475]   Time 0.144 (0.139)   Data 0.113 (0.105)   Loss 0.2948 (0.3415)   Prec@1 87.500 (85.295)   Prec@5 87.500 (85.295)   [2018-04-01 04:28:00]
  Epoch: [174][600/1475]   Time 0.140 (0.139)   Data 0.092 (0.105)   Loss 0.2202 (0.3377)   Prec@1 93.750 (85.519)   Prec@5 93.750 (85.519)   [2018-04-01 04:28:28]
  Epoch: [174][800/1475]   Time 0.129 (0.139)   Data 0.098 (0.105)   Loss 0.2768 (0.3362)   Prec@1 93.750 (85.596)   Prec@5 93.750 (85.596)   [2018-04-01 04:28:56]
  Epoch: [174][1000/1475]   Time 0.149 (0.139)   Data 0.109 (0.105)   Loss 0.2405 (0.3370)   Prec@1 87.500 (85.589)   Prec@5 87.500 (85.589)   [2018-04-01 04:29:24]
  Epoch: [174][1200/1475]   Time 0.131 (0.139)   Data 0.100 (0.105)   Loss 0.2485 (0.3346)   Prec@1 84.375 (85.694)   Prec@5 84.375 (85.694)   [2018-04-01 04:29:51]
  Epoch: [174][1400/1475]   Time 0.135 (0.139)   Data 0.104 (0.105)   Loss 0.3099 (0.3333)   Prec@1 87.500 (85.798)   Prec@5 87.500 (85.798)   [2018-04-01 04:30:19]
  **Train** Prec@1 85.801 Prec@5 85.801 Error@1 14.199
  **VAL** Prec@1 87.965 Prec@5 87.965 Error@1 12.035

==>>[2018-04-01 04:30:47] [Epoch=175/250] [Need: 04:39:26] [learning_rate=0.0002] [Best : Accuracy=88.43, Error=11.57]

==>>Epoch=[175/250]], [2018-04-01 04:30:47], LR=[0.002], Batch=[32] [Model=SimpleNet]
  Epoch: [175][000/1475]   Time 0.128 (0.128)   Data 0.097 (0.097)   Loss 0.2528 (0.2528)   Prec@1 93.750 (93.750)   Prec@5 93.750 (93.750)   [2018-04-01 04:30:48]
  Epoch: [175][200/1475]   Time 0.129 (0.139)   Data 0.098 (0.105)   Loss 0.6349 (0.3409)   Prec@1 65.625 (85.494)   Prec@5 65.625 (85.494)   [2018-04-01 04:31:15]
  Epoch: [175][400/1475]   Time 0.125 (0.139)   Data 0.094 (0.105)   Loss 0.3549 (0.3378)   Prec@1 84.375 (85.762)   Prec@5 84.375 (85.762)   [2018-04-01 04:31:43]
  Epoch: [175][600/1475]   Time 0.141 (0.139)   Data 0.094 (0.105)   Loss 0.5417 (0.3361)   Prec@1 78.125 (85.899)   Prec@5 78.125 (85.899)   [2018-04-01 04:32:11]
  Epoch: [175][800/1475]   Time 0.139 (0.139)   Data 0.112 (0.105)   Loss 0.2533 (0.3366)   Prec@1 93.750 (85.865)   Prec@5 93.750 (85.865)   [2018-04-01 04:32:38]
  Epoch: [175][1000/1475]   Time 0.141 (0.139)   Data 0.109 (0.105)   Loss 0.5991 (0.3370)   Prec@1 78.125 (85.849)   Prec@5 78.125 (85.849)   [2018-04-01 04:33:06]
  Epoch: [175][1200/1475]   Time 0.143 (0.139)   Data 0.102 (0.106)   Loss 0.2944 (0.3371)   Prec@1 81.250 (85.793)   Prec@5 81.250 (85.793)   [2018-04-01 04:33:34]
  Epoch: [175][1400/1475]   Time 0.135 (0.139)   Data 0.104 (0.106)   Loss 0.4562 (0.3350)   Prec@1 78.125 (85.916)   Prec@5 78.125 (85.916)   [2018-04-01 04:34:01]
  **Train** Prec@1 85.926 Prec@5 85.926 Error@1 14.074
  **VAL** Prec@1 88.074 Prec@5 88.074 Error@1 11.926

==>>[2018-04-01 04:34:30] [Epoch=176/250] [Need: 04:35:42] [learning_rate=0.0002] [Best : Accuracy=88.43, Error=11.57]

==>>Epoch=[176/250]], [2018-04-01 04:34:30], LR=[0.002], Batch=[32] [Model=SimpleNet]
  Epoch: [176][000/1475]   Time 0.125 (0.125)   Data 0.094 (0.094)   Loss 0.5294 (0.5294)   Prec@1 78.125 (78.125)   Prec@5 78.125 (78.125)   [2018-04-01 04:34:30]
  Epoch: [176][200/1475]   Time 0.141 (0.139)   Data 0.109 (0.105)   Loss 0.3828 (0.3283)   Prec@1 78.125 (85.976)   Prec@5 78.125 (85.976)   [2018-04-01 04:34:58]
  Epoch: [176][400/1475]   Time 0.145 (0.139)   Data 0.114 (0.106)   Loss 0.4616 (0.3343)   Prec@1 81.250 (85.786)   Prec@5 81.250 (85.786)   [2018-04-01 04:35:25]
  Epoch: [176][600/1475]   Time 0.125 (0.139)   Data 0.094 (0.106)   Loss 0.4397 (0.3336)   Prec@1 78.125 (85.784)   Prec@5 78.125 (85.784)   [2018-04-01 04:35:53]
  Epoch: [176][800/1475]   Time 0.141 (0.139)   Data 0.094 (0.106)   Loss 0.4690 (0.3316)   Prec@1 71.875 (85.900)   Prec@5 71.875 (85.900)   [2018-04-01 04:36:21]
  Epoch: [176][1000/1475]   Time 0.141 (0.139)   Data 0.109 (0.106)   Loss 0.2412 (0.3314)   Prec@1 96.875 (85.927)   Prec@5 96.875 (85.927)   [2018-04-01 04:36:49]
  Epoch: [176][1200/1475]   Time 0.125 (0.139)   Data 0.094 (0.106)   Loss 0.2543 (0.3315)   Prec@1 90.625 (85.861)   Prec@5 90.625 (85.861)   [2018-04-01 04:37:16]
  Epoch: [176][1400/1475]   Time 0.125 (0.139)   Data 0.094 (0.106)   Loss 0.4038 (0.3336)   Prec@1 84.375 (85.798)   Prec@5 84.375 (85.798)   [2018-04-01 04:37:44]
  **Train** Prec@1 85.767 Prec@5 85.767 Error@1 14.233
  **VAL** Prec@1 88.013 Prec@5 88.013 Error@1 11.987

==>>[2018-04-01 04:38:12] [Epoch=177/250] [Need: 04:31:58] [learning_rate=0.0002] [Best : Accuracy=88.43, Error=11.57]

==>>Epoch=[177/250]], [2018-04-01 04:38:12], LR=[0.002], Batch=[32] [Model=SimpleNet]
  Epoch: [177][000/1475]   Time 0.137 (0.137)   Data 0.097 (0.097)   Loss 0.2601 (0.2601)   Prec@1 87.500 (87.500)   Prec@5 87.500 (87.500)   [2018-04-01 04:38:12]
  Epoch: [177][200/1475]   Time 0.127 (0.139)   Data 0.090 (0.105)   Loss 0.3452 (0.3334)   Prec@1 93.750 (85.868)   Prec@5 93.750 (85.868)   [2018-04-01 04:38:40]
  Epoch: [177][400/1475]   Time 0.141 (0.139)   Data 0.109 (0.105)   Loss 0.3624 (0.3343)   Prec@1 87.500 (86.035)   Prec@5 87.500 (86.035)   [2018-04-01 04:39:08]
  Epoch: [177][600/1475]   Time 0.141 (0.139)   Data 0.109 (0.105)   Loss 0.5141 (0.3345)   Prec@1 84.375 (85.961)   Prec@5 84.375 (85.961)   [2018-04-01 04:39:36]
  Epoch: [177][800/1475]   Time 0.142 (0.139)   Data 0.110 (0.105)   Loss 0.3169 (0.3337)   Prec@1 90.625 (85.951)   Prec@5 90.625 (85.951)   [2018-04-01 04:40:03]
  Epoch: [177][1000/1475]   Time 0.152 (0.139)   Data 0.109 (0.105)   Loss 0.2492 (0.3329)   Prec@1 93.750 (85.986)   Prec@5 93.750 (85.986)   [2018-04-01 04:40:31]
  Epoch: [177][1200/1475]   Time 0.141 (0.139)   Data 0.109 (0.105)   Loss 0.2236 (0.3343)   Prec@1 90.625 (85.879)   Prec@5 90.625 (85.879)   [2018-04-01 04:40:59]
  Epoch: [177][1400/1475]   Time 0.134 (0.139)   Data 0.102 (0.105)   Loss 0.1997 (0.3358)   Prec@1 96.875 (85.854)   Prec@5 96.875 (85.854)   [2018-04-01 04:41:26]
  **Train** Prec@1 85.898 Prec@5 85.898 Error@1 14.102
  **VAL** Prec@1 88.242 Prec@5 88.242 Error@1 11.758

==>>[2018-04-01 04:41:55] [Epoch=178/250] [Need: 04:28:14] [learning_rate=0.0002] [Best : Accuracy=88.43, Error=11.57]

==>>Epoch=[178/250]], [2018-04-01 04:41:55], LR=[0.002], Batch=[32] [Model=SimpleNet]
  Epoch: [178][000/1475]   Time 0.133 (0.133)   Data 0.114 (0.114)   Loss 0.3803 (0.3803)   Prec@1 84.375 (84.375)   Prec@5 84.375 (84.375)   [2018-04-01 04:41:55]
  Epoch: [178][200/1475]   Time 0.131 (0.139)   Data 0.099 (0.106)   Loss 0.2906 (0.3242)   Prec@1 84.375 (86.474)   Prec@5 84.375 (86.474)   [2018-04-01 04:42:23]
  Epoch: [178][400/1475]   Time 0.141 (0.139)   Data 0.109 (0.106)   Loss 0.4226 (0.3309)   Prec@1 81.250 (86.066)   Prec@5 81.250 (86.066)   [2018-04-01 04:42:50]
  Epoch: [178][600/1475]   Time 0.149 (0.139)   Data 0.102 (0.106)   Loss 0.2323 (0.3334)   Prec@1 93.750 (85.873)   Prec@5 93.750 (85.873)   [2018-04-01 04:43:18]
  Epoch: [178][800/1475]   Time 0.125 (0.139)   Data 0.094 (0.106)   Loss 0.3368 (0.3306)   Prec@1 87.500 (86.060)   Prec@5 87.500 (86.060)   [2018-04-01 04:43:46]
  Epoch: [178][1000/1475]   Time 0.141 (0.139)   Data 0.109 (0.106)   Loss 0.3442 (0.3310)   Prec@1 84.375 (86.073)   Prec@5 84.375 (86.073)   [2018-04-01 04:44:14]
  Epoch: [178][1200/1475]   Time 0.145 (0.139)   Data 0.118 (0.106)   Loss 0.1761 (0.3306)   Prec@1 93.750 (86.056)   Prec@5 93.750 (86.056)   [2018-04-01 04:44:41]
  Epoch: [178][1400/1475]   Time 0.149 (0.139)   Data 0.118 (0.106)   Loss 0.4120 (0.3325)   Prec@1 84.375 (85.950)   Prec@5 84.375 (85.950)   [2018-04-01 04:45:09]
  **Train** Prec@1 85.903 Prec@5 85.903 Error@1 14.097
  **VAL** Prec@1 88.158 Prec@5 88.158 Error@1 11.842

==>>[2018-04-01 04:45:37] [Epoch=179/250] [Need: 04:24:30] [learning_rate=0.0002] [Best : Accuracy=88.43, Error=11.57]

==>>Epoch=[179/250]], [2018-04-01 04:45:37], LR=[0.002], Batch=[32] [Model=SimpleNet]
  Epoch: [179][000/1475]   Time 0.141 (0.141)   Data 0.094 (0.094)   Loss 0.1849 (0.1849)   Prec@1 96.875 (96.875)   Prec@5 96.875 (96.875)   [2018-04-01 04:45:38]
  Epoch: [179][200/1475]   Time 0.144 (0.139)   Data 0.114 (0.106)   Loss 0.2102 (0.3295)   Prec@1 96.875 (86.007)   Prec@5 96.875 (86.007)   [2018-04-01 04:46:05]
  Epoch: [179][400/1475]   Time 0.141 (0.139)   Data 0.109 (0.106)   Loss 0.3131 (0.3340)   Prec@1 81.250 (85.832)   Prec@5 81.250 (85.832)   [2018-04-01 04:46:33]
  Epoch: [179][600/1475]   Time 0.144 (0.139)   Data 0.113 (0.106)   Loss 0.2883 (0.3376)   Prec@1 84.375 (85.779)   Prec@5 84.375 (85.779)   [2018-04-01 04:47:01]
  Epoch: [179][800/1475]   Time 0.141 (0.139)   Data 0.109 (0.106)   Loss 0.2571 (0.3397)   Prec@1 87.500 (85.631)   Prec@5 87.500 (85.631)   [2018-04-01 04:47:29]
  Epoch: [179][1000/1475]   Time 0.141 (0.139)   Data 0.109 (0.106)   Loss 0.3100 (0.3406)   Prec@1 81.250 (85.627)   Prec@5 81.250 (85.627)   [2018-04-01 04:47:56]
  Epoch: [179][1200/1475]   Time 0.145 (0.139)   Data 0.114 (0.106)   Loss 0.2638 (0.3392)   Prec@1 93.750 (85.710)   Prec@5 93.750 (85.710)   [2018-04-01 04:48:24]
  Epoch: [179][1400/1475]   Time 0.125 (0.139)   Data 0.094 (0.106)   Loss 0.5206 (0.3387)   Prec@1 78.125 (85.646)   Prec@5 78.125 (85.646)   [2018-04-01 04:48:52]
  **Train** Prec@1 85.674 Prec@5 85.674 Error@1 14.326
  **VAL** Prec@1 88.037 Prec@5 88.037 Error@1 11.963

==>>[2018-04-01 04:49:20] [Epoch=180/250] [Need: 04:20:47] [learning_rate=0.0002] [Best : Accuracy=88.43, Error=11.57]

==>>Epoch=[180/250]], [2018-04-01 04:49:20], LR=[0.002], Batch=[32] [Model=SimpleNet]
  Epoch: [180][000/1475]   Time 0.149 (0.149)   Data 0.115 (0.115)   Loss 0.3191 (0.3191)   Prec@1 90.625 (90.625)   Prec@5 90.625 (90.625)   [2018-04-01 04:49:20]
  Epoch: [180][200/1475]   Time 0.134 (0.139)   Data 0.105 (0.106)   Loss 0.3809 (0.3287)   Prec@1 87.500 (86.194)   Prec@5 87.500 (86.194)   [2018-04-01 04:49:48]
  Epoch: [180][400/1475]   Time 0.129 (0.139)   Data 0.098 (0.105)   Loss 0.4872 (0.3339)   Prec@1 81.250 (85.770)   Prec@5 81.250 (85.770)   [2018-04-01 04:50:16]
  Epoch: [180][600/1475]   Time 0.141 (0.139)   Data 0.109 (0.106)   Loss 0.4061 (0.3398)   Prec@1 81.250 (85.363)   Prec@5 81.250 (85.363)   [2018-04-01 04:50:44]
  Epoch: [180][800/1475]   Time 0.141 (0.139)   Data 0.109 (0.106)   Loss 0.2886 (0.3406)   Prec@1 87.500 (85.397)   Prec@5 87.500 (85.397)   [2018-04-01 04:51:11]
  Epoch: [180][1000/1475]   Time 0.141 (0.139)   Data 0.109 (0.106)   Loss 0.2023 (0.3382)   Prec@1 96.875 (85.543)   Prec@5 96.875 (85.543)   [2018-04-01 04:51:39]
  Epoch: [180][1200/1475]   Time 0.141 (0.139)   Data 0.109 (0.106)   Loss 0.3606 (0.3386)   Prec@1 87.500 (85.575)   Prec@5 87.500 (85.575)   [2018-04-01 04:52:07]
  Epoch: [180][1400/1475]   Time 0.154 (0.139)   Data 0.120 (0.106)   Loss 0.3475 (0.3366)   Prec@1 81.250 (85.626)   Prec@5 81.250 (85.626)   [2018-04-01 04:52:35]
  **Train** Prec@1 85.625 Prec@5 85.625 Error@1 14.375
  **VAL** Prec@1 88.037 Prec@5 88.037 Error@1 11.963

==>>[2018-04-01 04:53:03] [Epoch=181/250] [Need: 04:17:03] [learning_rate=0.0002] [Best : Accuracy=88.43, Error=11.57]

==>>Epoch=[181/250]], [2018-04-01 04:53:03], LR=[0.002], Batch=[32] [Model=SimpleNet]
  Epoch: [181][000/1475]   Time 0.141 (0.141)   Data 0.109 (0.109)   Loss 0.2523 (0.2523)   Prec@1 90.625 (90.625)   Prec@5 90.625 (90.625)   [2018-04-01 04:53:03]
  Epoch: [181][200/1475]   Time 0.141 (0.139)   Data 0.109 (0.105)   Loss 0.2797 (0.3423)   Prec@1 90.625 (85.354)   Prec@5 90.625 (85.354)   [2018-04-01 04:53:31]
  Epoch: [181][400/1475]   Time 0.141 (0.139)   Data 0.094 (0.105)   Loss 0.2559 (0.3380)   Prec@1 87.500 (85.513)   Prec@5 87.500 (85.513)   [2018-04-01 04:53:58]
  Epoch: [181][600/1475]   Time 0.145 (0.139)   Data 0.113 (0.105)   Loss 0.4633 (0.3338)   Prec@1 78.125 (85.774)   Prec@5 78.125 (85.774)   [2018-04-01 04:54:26]
  Epoch: [181][800/1475]   Time 0.126 (0.139)   Data 0.094 (0.106)   Loss 0.3418 (0.3338)   Prec@1 81.250 (85.725)   Prec@5 81.250 (85.725)   [2018-04-01 04:54:54]
  Epoch: [181][1000/1475]   Time 0.147 (0.139)   Data 0.109 (0.106)   Loss 0.3726 (0.3331)   Prec@1 81.250 (85.789)   Prec@5 81.250 (85.789)   [2018-04-01 04:55:22]
  Epoch: [181][1200/1475]   Time 0.141 (0.139)   Data 0.109 (0.106)   Loss 0.2705 (0.3345)   Prec@1 87.500 (85.806)   Prec@5 87.500 (85.806)   [2018-04-01 04:55:49]
  Epoch: [181][1400/1475]   Time 0.129 (0.139)   Data 0.098 (0.106)   Loss 0.1890 (0.3347)   Prec@1 96.875 (85.771)   Prec@5 96.875 (85.771)   [2018-04-01 04:56:17]
  **Train** Prec@1 85.809 Prec@5 85.809 Error@1 14.191
  **VAL** Prec@1 87.977 Prec@5 87.977 Error@1 12.023

==>>[2018-04-01 04:56:45] [Epoch=182/250] [Need: 04:13:19] [learning_rate=0.0002] [Best : Accuracy=88.43, Error=11.57]

==>>Epoch=[182/250]], [2018-04-01 04:56:45], LR=[0.002], Batch=[32] [Model=SimpleNet]
  Epoch: [182][000/1475]   Time 0.142 (0.142)   Data 0.111 (0.111)   Loss 0.5324 (0.5324)   Prec@1 81.250 (81.250)   Prec@5 81.250 (81.250)   [2018-04-01 04:56:46]
  Epoch: [182][200/1475]   Time 0.141 (0.139)   Data 0.109 (0.105)   Loss 0.4698 (0.3426)   Prec@1 78.125 (85.836)   Prec@5 78.125 (85.836)   [2018-04-01 04:57:13]
  Epoch: [182][400/1475]   Time 0.142 (0.139)   Data 0.111 (0.105)   Loss 0.3212 (0.3368)   Prec@1 84.375 (86.066)   Prec@5 84.375 (86.066)   [2018-04-01 04:57:41]
  Epoch: [182][600/1475]   Time 0.141 (0.139)   Data 0.109 (0.105)   Loss 0.4241 (0.3344)   Prec@1 81.250 (86.164)   Prec@5 81.250 (86.164)   [2018-04-01 04:58:09]
  Epoch: [182][800/1475]   Time 0.141 (0.139)   Data 0.109 (0.105)   Loss 0.3766 (0.3351)   Prec@1 78.125 (85.978)   Prec@5 78.125 (85.978)   [2018-04-01 04:58:36]
  Epoch: [182][1000/1475]   Time 0.155 (0.139)   Data 0.121 (0.105)   Loss 0.2760 (0.3361)   Prec@1 87.500 (85.917)   Prec@5 87.500 (85.917)   [2018-04-01 04:59:04]
  Epoch: [182][1200/1475]   Time 0.141 (0.139)   Data 0.109 (0.105)   Loss 0.1525 (0.3352)   Prec@1 93.750 (85.999)   Prec@5 93.750 (85.999)   [2018-04-01 04:59:32]
  Epoch: [182][1400/1475]   Time 0.133 (0.139)   Data 0.102 (0.105)   Loss 0.2256 (0.3345)   Prec@1 90.625 (85.961)   Prec@5 90.625 (85.961)   [2018-04-01 05:00:00]
  **Train** Prec@1 85.975 Prec@5 85.975 Error@1 14.025
  **VAL** Prec@1 87.977 Prec@5 87.977 Error@1 12.023

==>>[2018-04-01 05:00:28] [Epoch=183/250] [Need: 04:09:35] [learning_rate=0.0002] [Best : Accuracy=88.43, Error=11.57]

==>>Epoch=[183/250]], [2018-04-01 05:00:28], LR=[0.002], Batch=[32] [Model=SimpleNet]
  Epoch: [183][000/1475]   Time 0.141 (0.141)   Data 0.109 (0.109)   Loss 0.4096 (0.4096)   Prec@1 78.125 (78.125)   Prec@5 78.125 (78.125)   [2018-04-01 05:00:28]
  Epoch: [183][200/1475]   Time 0.141 (0.139)   Data 0.109 (0.106)   Loss 0.2414 (0.3282)   Prec@1 90.625 (85.930)   Prec@5 90.625 (85.930)   [2018-04-01 05:00:56]
  Epoch: [183][400/1475]   Time 0.141 (0.139)   Data 0.109 (0.106)   Loss 0.3399 (0.3273)   Prec@1 87.500 (86.199)   Prec@5 87.500 (86.199)   [2018-04-01 05:01:24]
  Epoch: [183][600/1475]   Time 0.129 (0.139)   Data 0.094 (0.106)   Loss 0.4246 (0.3319)   Prec@1 81.250 (85.976)   Prec@5 81.250 (85.976)   [2018-04-01 05:01:51]
  Epoch: [183][800/1475]   Time 0.145 (0.139)   Data 0.114 (0.106)   Loss 0.1887 (0.3333)   Prec@1 93.750 (85.924)   Prec@5 93.750 (85.924)   [2018-04-01 05:02:19]
  Epoch: [183][1000/1475]   Time 0.139 (0.139)   Data 0.108 (0.106)   Loss 0.1979 (0.3340)   Prec@1 93.750 (85.911)   Prec@5 93.750 (85.911)   [2018-04-01 05:02:47]
  Epoch: [183][1200/1475]   Time 0.141 (0.139)   Data 0.094 (0.106)   Loss 0.4072 (0.3335)   Prec@1 78.125 (85.939)   Prec@5 78.125 (85.939)   [2018-04-01 05:03:15]
  Epoch: [183][1400/1475]   Time 0.141 (0.139)   Data 0.094 (0.106)   Loss 0.3030 (0.3337)   Prec@1 87.500 (85.894)   Prec@5 87.500 (85.894)   [2018-04-01 05:03:42]
  **Train** Prec@1 85.905 Prec@5 85.905 Error@1 14.095
  **VAL** Prec@1 87.857 Prec@5 87.857 Error@1 12.143

==>>[2018-04-01 05:04:11] [Epoch=184/250] [Need: 04:05:51] [learning_rate=0.0002] [Best : Accuracy=88.43, Error=11.57]

==>>Epoch=[184/250]], [2018-04-01 05:04:11], LR=[0.002], Batch=[32] [Model=SimpleNet]
  Epoch: [184][000/1475]   Time 0.144 (0.144)   Data 0.116 (0.116)   Loss 0.1929 (0.1929)   Prec@1 93.750 (93.750)   Prec@5 93.750 (93.750)   [2018-04-01 05:04:11]
  Epoch: [184][200/1475]   Time 0.134 (0.139)   Data 0.103 (0.106)   Loss 0.1959 (0.3362)   Prec@1 96.875 (85.681)   Prec@5 96.875 (85.681)   [2018-04-01 05:04:38]
  Epoch: [184][400/1475]   Time 0.145 (0.139)   Data 0.114 (0.105)   Loss 0.2891 (0.3340)   Prec@1 87.500 (85.747)   Prec@5 87.500 (85.747)   [2018-04-01 05:05:06]
  Epoch: [184][600/1475]   Time 0.124 (0.139)   Data 0.093 (0.105)   Loss 0.3913 (0.3305)   Prec@1 78.125 (86.044)   Prec@5 78.125 (86.044)   [2018-04-01 05:05:34]
  Epoch: [184][800/1475]   Time 0.149 (0.139)   Data 0.102 (0.105)   Loss 0.6076 (0.3332)   Prec@1 78.125 (85.900)   Prec@5 78.125 (85.900)   [2018-04-01 05:06:01]
  Epoch: [184][1000/1475]   Time 0.145 (0.139)   Data 0.114 (0.106)   Loss 0.2591 (0.3325)   Prec@1 90.625 (85.874)   Prec@5 90.625 (85.874)   [2018-04-01 05:06:29]
  Epoch: [184][1200/1475]   Time 0.145 (0.139)   Data 0.114 (0.106)   Loss 0.3624 (0.3368)   Prec@1 87.500 (85.679)   Prec@5 87.500 (85.679)   [2018-04-01 05:06:57]
  Epoch: [184][1400/1475]   Time 0.125 (0.139)   Data 0.094 (0.105)   Loss 0.3100 (0.3366)   Prec@1 81.250 (85.655)   Prec@5 81.250 (85.655)   [2018-04-01 05:07:25]
  **Train** Prec@1 85.680 Prec@5 85.680 Error@1 14.320
  **VAL** Prec@1 88.134 Prec@5 88.134 Error@1 11.866

==>>[2018-04-01 05:07:53] [Epoch=185/250] [Need: 04:02:07] [learning_rate=0.0002] [Best : Accuracy=88.43, Error=11.57]

==>>Epoch=[185/250]], [2018-04-01 05:07:53], LR=[0.002], Batch=[32] [Model=SimpleNet]
  Epoch: [185][000/1475]   Time 0.149 (0.149)   Data 0.113 (0.113)   Loss 0.2907 (0.2907)   Prec@1 87.500 (87.500)   Prec@5 87.500 (87.500)   [2018-04-01 05:07:53]
  Epoch: [185][200/1475]   Time 0.137 (0.139)   Data 0.106 (0.106)   Loss 0.3844 (0.3302)   Prec@1 81.250 (85.619)   Prec@5 81.250 (85.619)   [2018-04-01 05:08:21]
  Epoch: [185][400/1475]   Time 0.144 (0.139)   Data 0.113 (0.106)   Loss 0.4287 (0.3308)   Prec@1 81.250 (85.637)   Prec@5 81.250 (85.637)   [2018-04-01 05:08:49]
  Epoch: [185][600/1475]   Time 0.126 (0.139)   Data 0.095 (0.106)   Loss 0.3533 (0.3372)   Prec@1 87.500 (85.467)   Prec@5 87.500 (85.467)   [2018-04-01 05:09:16]
  Epoch: [185][800/1475]   Time 0.150 (0.139)   Data 0.119 (0.106)   Loss 0.1935 (0.3352)   Prec@1 93.750 (85.604)   Prec@5 93.750 (85.604)   [2018-04-01 05:09:44]
  Epoch: [185][1000/1475]   Time 0.125 (0.139)   Data 0.094 (0.106)   Loss 0.2318 (0.3356)   Prec@1 93.750 (85.661)   Prec@5 93.750 (85.661)   [2018-04-01 05:10:12]
  Epoch: [185][1200/1475]   Time 0.141 (0.139)   Data 0.109 (0.106)   Loss 0.1719 (0.3349)   Prec@1 87.500 (85.712)   Prec@5 87.500 (85.712)   [2018-04-01 05:10:39]
  Epoch: [185][1400/1475]   Time 0.124 (0.139)   Data 0.093 (0.106)   Loss 0.5072 (0.3356)   Prec@1 65.625 (85.689)   Prec@5 65.625 (85.689)   [2018-04-01 05:11:07]
  **Train** Prec@1 85.742 Prec@5 85.742 Error@1 14.258
  **VAL** Prec@1 88.134 Prec@5 88.134 Error@1 11.866

==>>[2018-04-01 05:11:35] [Epoch=186/250] [Need: 03:58:23] [learning_rate=0.0002] [Best : Accuracy=88.43, Error=11.57]

==>>Epoch=[186/250]], [2018-04-01 05:11:35], LR=[0.002], Batch=[32] [Model=SimpleNet]
  Epoch: [186][000/1475]   Time 0.125 (0.125)   Data 0.094 (0.094)   Loss 0.4676 (0.4676)   Prec@1 78.125 (78.125)   Prec@5 78.125 (78.125)   [2018-04-01 05:11:36]
  Epoch: [186][200/1475]   Time 0.136 (0.139)   Data 0.105 (0.106)   Loss 0.4711 (0.3396)   Prec@1 81.250 (85.961)   Prec@5 81.250 (85.961)   [2018-04-01 05:12:03]
  Epoch: [186][400/1475]   Time 0.141 (0.139)   Data 0.109 (0.105)   Loss 0.2562 (0.3326)   Prec@1 90.625 (86.276)   Prec@5 90.625 (86.276)   [2018-04-01 05:12:31]
  Epoch: [186][600/1475]   Time 0.141 (0.138)   Data 0.109 (0.105)   Loss 0.2734 (0.3333)   Prec@1 87.500 (86.169)   Prec@5 87.500 (86.169)   [2018-04-01 05:12:59]
  Epoch: [186][800/1475]   Time 0.141 (0.138)   Data 0.109 (0.105)   Loss 0.3448 (0.3345)   Prec@1 87.500 (86.146)   Prec@5 87.500 (86.146)   [2018-04-01 05:13:26]
  Epoch: [186][1000/1475]   Time 0.156 (0.138)   Data 0.109 (0.105)   Loss 0.4358 (0.3363)   Prec@1 71.875 (85.930)   Prec@5 71.875 (85.930)   [2018-04-01 05:13:54]
  Epoch: [186][1200/1475]   Time 0.142 (0.138)   Data 0.096 (0.105)   Loss 0.1422 (0.3364)   Prec@1 96.875 (85.884)   Prec@5 96.875 (85.884)   [2018-04-01 05:14:22]
  Epoch: [186][1400/1475]   Time 0.138 (0.138)   Data 0.091 (0.105)   Loss 0.3536 (0.3364)   Prec@1 84.375 (85.854)   Prec@5 84.375 (85.854)   [2018-04-01 05:14:49]
  **Train** Prec@1 85.833 Prec@5 85.833 Error@1 14.167
  **VAL** Prec@1 88.061 Prec@5 88.061 Error@1 11.939

==>>[2018-04-01 05:15:18] [Epoch=187/250] [Need: 03:54:39] [learning_rate=0.0002] [Best : Accuracy=88.43, Error=11.57]

==>>Epoch=[187/250]], [2018-04-01 05:15:18], LR=[0.002], Batch=[32] [Model=SimpleNet]
  Epoch: [187][000/1475]   Time 0.125 (0.125)   Data 0.094 (0.094)   Loss 0.3565 (0.3565)   Prec@1 81.250 (81.250)   Prec@5 81.250 (81.250)   [2018-04-01 05:15:18]
  Epoch: [187][200/1475]   Time 0.150 (0.139)   Data 0.109 (0.106)   Loss 0.3934 (0.3293)   Prec@1 90.625 (86.256)   Prec@5 90.625 (86.256)   [2018-04-01 05:15:46]
  Epoch: [187][400/1475]   Time 0.141 (0.139)   Data 0.114 (0.105)   Loss 0.2903 (0.3339)   Prec@1 84.375 (86.074)   Prec@5 84.375 (86.074)   [2018-04-01 05:16:13]
  Epoch: [187][600/1475]   Time 0.138 (0.139)   Data 0.105 (0.105)   Loss 0.2496 (0.3331)   Prec@1 90.625 (86.044)   Prec@5 90.625 (86.044)   [2018-04-01 05:16:41]
  Epoch: [187][800/1475]   Time 0.128 (0.139)   Data 0.097 (0.105)   Loss 0.2193 (0.3346)   Prec@1 93.750 (85.920)   Prec@5 93.750 (85.920)   [2018-04-01 05:17:09]
  Epoch: [187][1000/1475]   Time 0.133 (0.139)   Data 0.101 (0.105)   Loss 0.2919 (0.3358)   Prec@1 81.250 (85.886)   Prec@5 81.250 (85.886)   [2018-04-01 05:17:36]
  Epoch: [187][1200/1475]   Time 0.131 (0.139)   Data 0.099 (0.105)   Loss 0.3618 (0.3363)   Prec@1 81.250 (85.817)   Prec@5 81.250 (85.817)   [2018-04-01 05:18:04]
  Epoch: [187][1400/1475]   Time 0.136 (0.139)   Data 0.105 (0.105)   Loss 0.2495 (0.3364)   Prec@1 90.625 (85.827)   Prec@5 90.625 (85.827)   [2018-04-01 05:18:32]
  **Train** Prec@1 85.752 Prec@5 85.752 Error@1 14.248
  **VAL** Prec@1 88.146 Prec@5 88.146 Error@1 11.854

==>>[2018-04-01 05:19:00] [Epoch=188/250] [Need: 03:50:56] [learning_rate=0.0002] [Best : Accuracy=88.43, Error=11.57]

==>>Epoch=[188/250]], [2018-04-01 05:19:00], LR=[0.002], Batch=[32] [Model=SimpleNet]
  Epoch: [188][000/1475]   Time 0.145 (0.145)   Data 0.114 (0.114)   Loss 0.2714 (0.2714)   Prec@1 90.625 (90.625)   Prec@5 90.625 (90.625)   [2018-04-01 05:19:00]
  Epoch: [188][200/1475]   Time 0.141 (0.139)   Data 0.109 (0.105)   Loss 0.3327 (0.3352)   Prec@1 90.625 (85.665)   Prec@5 90.625 (85.665)   [2018-04-01 05:19:28]
  Epoch: [188][400/1475]   Time 0.141 (0.139)   Data 0.094 (0.105)   Loss 0.3678 (0.3311)   Prec@1 84.375 (86.050)   Prec@5 84.375 (86.050)   [2018-04-01 05:19:56]
  Epoch: [188][600/1475]   Time 0.129 (0.139)   Data 0.097 (0.105)   Loss 0.4637 (0.3320)   Prec@1 78.125 (85.966)   Prec@5 78.125 (85.966)   [2018-04-01 05:20:23]
  Epoch: [188][800/1475]   Time 0.141 (0.139)   Data 0.109 (0.105)   Loss 0.2552 (0.3333)   Prec@1 90.625 (85.846)   Prec@5 90.625 (85.846)   [2018-04-01 05:20:51]
  Epoch: [188][1000/1475]   Time 0.141 (0.139)   Data 0.110 (0.105)   Loss 0.2637 (0.3310)   Prec@1 93.750 (85.992)   Prec@5 93.750 (85.992)   [2018-04-01 05:21:19]
  Epoch: [188][1200/1475]   Time 0.141 (0.139)   Data 0.109 (0.105)   Loss 0.2850 (0.3319)   Prec@1 84.375 (85.941)   Prec@5 84.375 (85.941)   [2018-04-01 05:21:47]
  Epoch: [188][1400/1475]   Time 0.141 (0.139)   Data 0.109 (0.105)   Loss 0.2568 (0.3340)   Prec@1 81.250 (85.809)   Prec@5 81.250 (85.809)   [2018-04-01 05:22:14]
  **Train** Prec@1 85.795 Prec@5 85.795 Error@1 14.205
  **VAL** Prec@1 88.410 Prec@5 88.410 Error@1 11.590

==>>[2018-04-01 05:22:42] [Epoch=189/250] [Need: 03:47:12] [learning_rate=0.0002] [Best : Accuracy=88.43, Error=11.57]

==>>Epoch=[189/250]], [2018-04-01 05:22:42], LR=[0.002], Batch=[32] [Model=SimpleNet]
  Epoch: [189][000/1475]   Time 0.141 (0.141)   Data 0.109 (0.109)   Loss 0.2041 (0.2041)   Prec@1 93.750 (93.750)   Prec@5 93.750 (93.750)   [2018-04-01 05:22:43]
  Epoch: [189][200/1475]   Time 0.144 (0.139)   Data 0.109 (0.105)   Loss 0.5772 (0.3365)   Prec@1 71.875 (85.774)   Prec@5 71.875 (85.774)   [2018-04-01 05:23:10]
  Epoch: [189][400/1475]   Time 0.144 (0.138)   Data 0.113 (0.105)   Loss 0.2793 (0.3381)   Prec@1 84.375 (85.653)   Prec@5 84.375 (85.653)   [2018-04-01 05:23:38]
  Epoch: [189][600/1475]   Time 0.141 (0.138)   Data 0.094 (0.105)   Loss 0.2997 (0.3378)   Prec@1 84.375 (85.675)   Prec@5 84.375 (85.675)   [2018-04-01 05:24:06]
  Epoch: [189][800/1475]   Time 0.144 (0.138)   Data 0.111 (0.105)   Loss 0.1567 (0.3365)   Prec@1 96.875 (85.768)   Prec@5 96.875 (85.768)   [2018-04-01 05:24:33]
  Epoch: [189][1000/1475]   Time 0.129 (0.138)   Data 0.098 (0.105)   Loss 0.5802 (0.3330)   Prec@1 75.000 (85.939)   Prec@5 75.000 (85.939)   [2018-04-01 05:25:01]
  Epoch: [189][1200/1475]   Time 0.149 (0.138)   Data 0.113 (0.105)   Loss 0.3590 (0.3355)   Prec@1 90.625 (85.908)   Prec@5 90.625 (85.908)   [2018-04-01 05:25:29]
  Epoch: [189][1400/1475]   Time 0.137 (0.138)   Data 0.090 (0.105)   Loss 0.2338 (0.3340)   Prec@1 87.500 (85.979)   Prec@5 87.500 (85.979)   [2018-04-01 05:25:56]
  **Train** Prec@1 85.994 Prec@5 85.994 Error@1 14.006
  **VAL** Prec@1 88.146 Prec@5 88.146 Error@1 11.854

==>>[2018-04-01 05:26:25] [Epoch=190/250] [Need: 03:43:28] [learning_rate=0.0002] [Best : Accuracy=88.43, Error=11.57]

==>>Epoch=[190/250]], [2018-04-01 05:26:25], LR=[0.002], Batch=[32] [Model=SimpleNet]
  Epoch: [190][000/1475]   Time 0.144 (0.144)   Data 0.113 (0.113)   Loss 0.4947 (0.4947)   Prec@1 81.250 (81.250)   Prec@5 81.250 (81.250)   [2018-04-01 05:26:25]
  Epoch: [190][200/1475]   Time 0.141 (0.139)   Data 0.094 (0.105)   Loss 0.5673 (0.3310)   Prec@1 78.125 (86.194)   Prec@5 78.125 (86.194)   [2018-04-01 05:26:52]
  Epoch: [190][400/1475]   Time 0.139 (0.139)   Data 0.092 (0.106)   Loss 0.1822 (0.3317)   Prec@1 96.875 (86.160)   Prec@5 96.875 (86.160)   [2018-04-01 05:27:20]
  Epoch: [190][600/1475]   Time 0.144 (0.139)   Data 0.113 (0.106)   Loss 0.4584 (0.3325)   Prec@1 81.250 (85.935)   Prec@5 81.250 (85.935)   [2018-04-01 05:27:48]
  Epoch: [190][800/1475]   Time 0.125 (0.139)   Data 0.094 (0.106)   Loss 0.5466 (0.3328)   Prec@1 78.125 (85.943)   Prec@5 78.125 (85.943)   [2018-04-01 05:28:16]
  Epoch: [190][1000/1475]   Time 0.136 (0.139)   Data 0.109 (0.106)   Loss 0.3585 (0.3351)   Prec@1 84.375 (85.795)   Prec@5 84.375 (85.795)   [2018-04-01 05:28:43]
  Epoch: [190][1200/1475]   Time 0.141 (0.139)   Data 0.109 (0.106)   Loss 0.2728 (0.3360)   Prec@1 93.750 (85.759)   Prec@5 93.750 (85.759)   [2018-04-01 05:29:11]
  Epoch: [190][1400/1475]   Time 0.145 (0.139)   Data 0.109 (0.106)   Loss 0.3332 (0.3341)   Prec@1 84.375 (85.861)   Prec@5 84.375 (85.861)   [2018-04-01 05:29:39]
  **Train** Prec@1 85.845 Prec@5 85.845 Error@1 14.155
  **VAL** Prec@1 88.158 Prec@5 88.158 Error@1 11.842

==>>[2018-04-01 05:30:07] [Epoch=191/250] [Need: 03:39:44] [learning_rate=0.0002] [Best : Accuracy=88.43, Error=11.57]

==>>Epoch=[191/250]], [2018-04-01 05:30:07], LR=[0.002], Batch=[32] [Model=SimpleNet]
  Epoch: [191][000/1475]   Time 0.141 (0.141)   Data 0.109 (0.109)   Loss 0.3001 (0.3001)   Prec@1 87.500 (87.500)   Prec@5 87.500 (87.500)   [2018-04-01 05:30:07]
  Epoch: [191][200/1475]   Time 0.132 (0.139)   Data 0.101 (0.106)   Loss 0.2873 (0.3408)   Prec@1 87.500 (85.510)   Prec@5 87.500 (85.510)   [2018-04-01 05:30:35]
  Epoch: [191][400/1475]   Time 0.128 (0.139)   Data 0.097 (0.106)   Loss 0.0966 (0.3362)   Prec@1 100.000 (85.910)   Prec@5 100.000 (85.910)   [2018-04-01 05:31:03]
  Epoch: [191][600/1475]   Time 0.126 (0.139)   Data 0.095 (0.106)   Loss 0.3244 (0.3339)   Prec@1 81.250 (86.153)   Prec@5 81.250 (86.153)   [2018-04-01 05:31:30]
  Epoch: [191][800/1475]   Time 0.139 (0.139)   Data 0.106 (0.106)   Loss 0.5210 (0.3328)   Prec@1 81.250 (86.115)   Prec@5 81.250 (86.115)   [2018-04-01 05:31:58]
  Epoch: [191][1000/1475]   Time 0.141 (0.139)   Data 0.109 (0.106)   Loss 0.4237 (0.3338)   Prec@1 81.250 (86.098)   Prec@5 81.250 (86.098)   [2018-04-01 05:32:26]
  Epoch: [191][1200/1475]   Time 0.144 (0.139)   Data 0.097 (0.106)   Loss 0.3094 (0.3337)   Prec@1 90.625 (86.103)   Prec@5 90.625 (86.103)   [2018-04-01 05:32:54]
  Epoch: [191][1400/1475]   Time 0.145 (0.139)   Data 0.109 (0.106)   Loss 0.4405 (0.3338)   Prec@1 84.375 (86.068)   Prec@5 84.375 (86.068)   [2018-04-01 05:33:21]
  **Train** Prec@1 86.070 Prec@5 86.070 Error@1 13.930
  **VAL** Prec@1 88.074 Prec@5 88.074 Error@1 11.926

==>>[2018-04-01 05:33:49] [Epoch=192/250] [Need: 03:36:00] [learning_rate=0.0002] [Best : Accuracy=88.43, Error=11.57]

==>>Epoch=[192/250]], [2018-04-01 05:33:49], LR=[0.002], Batch=[32] [Model=SimpleNet]
  Epoch: [192][000/1475]   Time 0.133 (0.133)   Data 0.102 (0.102)   Loss 0.2140 (0.2140)   Prec@1 87.500 (87.500)   Prec@5 87.500 (87.500)   [2018-04-01 05:33:50]
  Epoch: [192][200/1475]   Time 0.141 (0.139)   Data 0.109 (0.105)   Loss 0.5202 (0.3310)   Prec@1 71.875 (85.774)   Prec@5 71.875 (85.774)   [2018-04-01 05:34:17]
  Epoch: [192][400/1475]   Time 0.131 (0.139)   Data 0.094 (0.105)   Loss 0.6745 (0.3329)   Prec@1 68.750 (85.887)   Prec@5 68.750 (85.887)   [2018-04-01 05:34:45]
  Epoch: [192][600/1475]   Time 0.135 (0.139)   Data 0.108 (0.105)   Loss 0.4375 (0.3336)   Prec@1 81.250 (85.940)   Prec@5 81.250 (85.940)   [2018-04-01 05:35:13]
  Epoch: [192][800/1475]   Time 0.141 (0.139)   Data 0.109 (0.105)   Loss 0.3051 (0.3338)   Prec@1 87.500 (85.889)   Prec@5 87.500 (85.889)   [2018-04-01 05:35:41]
  Epoch: [192][1000/1475]   Time 0.141 (0.139)   Data 0.109 (0.105)   Loss 0.2893 (0.3327)   Prec@1 84.375 (85.845)   Prec@5 84.375 (85.845)   [2018-04-01 05:36:08]
  Epoch: [192][1200/1475]   Time 0.141 (0.139)   Data 0.109 (0.105)   Loss 0.2115 (0.3333)   Prec@1 90.625 (85.798)   Prec@5 90.625 (85.798)   [2018-04-01 05:36:36]
  Epoch: [192][1400/1475]   Time 0.144 (0.139)   Data 0.112 (0.105)   Loss 0.4840 (0.3349)   Prec@1 75.000 (85.704)   Prec@5 75.000 (85.704)   [2018-04-01 05:37:04]
  **Train** Prec@1 85.758 Prec@5 85.758 Error@1 14.242
  **VAL** Prec@1 88.170 Prec@5 88.170 Error@1 11.830

==>>[2018-04-01 05:37:32] [Epoch=193/250] [Need: 03:32:17] [learning_rate=0.0002] [Best : Accuracy=88.43, Error=11.57]

==>>Epoch=[193/250]], [2018-04-01 05:37:32], LR=[0.002], Batch=[32] [Model=SimpleNet]
  Epoch: [193][000/1475]   Time 0.144 (0.144)   Data 0.113 (0.113)   Loss 0.3988 (0.3988)   Prec@1 84.375 (84.375)   Prec@5 84.375 (84.375)   [2018-04-01 05:37:32]
  Epoch: [193][200/1475]   Time 0.141 (0.139)   Data 0.109 (0.106)   Loss 0.3826 (0.3307)   Prec@1 84.375 (86.194)   Prec@5 84.375 (86.194)   [2018-04-01 05:38:00]
  Epoch: [193][400/1475]   Time 0.145 (0.139)   Data 0.098 (0.106)   Loss 0.2696 (0.3348)   Prec@1 87.500 (85.840)   Prec@5 87.500 (85.840)   [2018-04-01 05:38:28]
  Epoch: [193][600/1475]   Time 0.125 (0.139)   Data 0.094 (0.106)   Loss 0.5404 (0.3391)   Prec@1 71.875 (85.836)   Prec@5 71.875 (85.836)   [2018-04-01 05:38:55]
  Epoch: [193][800/1475]   Time 0.128 (0.139)   Data 0.097 (0.105)   Loss 0.2652 (0.3378)   Prec@1 87.500 (85.881)   Prec@5 87.500 (85.881)   [2018-04-01 05:39:23]
  Epoch: [193][1000/1475]   Time 0.125 (0.139)   Data 0.094 (0.105)   Loss 0.3670 (0.3367)   Prec@1 84.375 (85.877)   Prec@5 84.375 (85.877)   [2018-04-01 05:39:51]
  Epoch: [193][1200/1475]   Time 0.141 (0.139)   Data 0.109 (0.105)   Loss 0.3664 (0.3356)   Prec@1 84.375 (85.848)   Prec@5 84.375 (85.848)   [2018-04-01 05:40:18]
  Epoch: [193][1400/1475]   Time 0.129 (0.139)   Data 0.094 (0.105)   Loss 0.3277 (0.3353)   Prec@1 78.125 (85.845)   Prec@5 78.125 (85.845)   [2018-04-01 05:40:46]
  **Train** Prec@1 85.896 Prec@5 85.896 Error@1 14.104
  **VAL** Prec@1 88.013 Prec@5 88.013 Error@1 11.987

==>>[2018-04-01 05:41:14] [Epoch=194/250] [Need: 03:28:33] [learning_rate=0.0002] [Best : Accuracy=88.43, Error=11.57]

==>>Epoch=[194/250]], [2018-04-01 05:41:14], LR=[0.002], Batch=[32] [Model=SimpleNet]
  Epoch: [194][000/1475]   Time 0.124 (0.124)   Data 0.093 (0.093)   Loss 0.2733 (0.2733)   Prec@1 90.625 (90.625)   Prec@5 90.625 (90.625)   [2018-04-01 05:41:14]
  Epoch: [194][200/1475]   Time 0.129 (0.139)   Data 0.094 (0.105)   Loss 0.4469 (0.3308)   Prec@1 87.500 (86.334)   Prec@5 87.500 (86.334)   [2018-04-01 05:41:42]
  Epoch: [194][400/1475]   Time 0.141 (0.139)   Data 0.109 (0.105)   Loss 0.4427 (0.3294)   Prec@1 81.250 (86.409)   Prec@5 81.250 (86.409)   [2018-04-01 05:42:10]
  Epoch: [194][600/1475]   Time 0.141 (0.139)   Data 0.094 (0.105)   Loss 0.2611 (0.3309)   Prec@1 84.375 (86.247)   Prec@5 84.375 (86.247)   [2018-04-01 05:42:38]
  Epoch: [194][800/1475]   Time 0.141 (0.139)   Data 0.109 (0.106)   Loss 0.2757 (0.3320)   Prec@1 87.500 (86.092)   Prec@5 87.500 (86.092)   [2018-04-01 05:43:05]
  Epoch: [194][1000/1475]   Time 0.141 (0.139)   Data 0.109 (0.105)   Loss 0.3455 (0.3339)   Prec@1 87.500 (85.983)   Prec@5 87.500 (85.983)   [2018-04-01 05:43:33]
  Epoch: [194][1200/1475]   Time 0.141 (0.139)   Data 0.109 (0.106)   Loss 0.2254 (0.3335)   Prec@1 90.625 (86.009)   Prec@5 90.625 (86.009)   [2018-04-01 05:44:01]
  Epoch: [194][1400/1475]   Time 0.141 (0.139)   Data 0.109 (0.106)   Loss 0.3029 (0.3336)   Prec@1 84.375 (85.950)   Prec@5 84.375 (85.950)   [2018-04-01 05:44:28]
  **Train** Prec@1 85.843 Prec@5 85.843 Error@1 14.157
  **VAL** Prec@1 88.134 Prec@5 88.134 Error@1 11.866

==>>[2018-04-01 05:44:57] [Epoch=195/250] [Need: 03:24:49] [learning_rate=0.0002] [Best : Accuracy=88.43, Error=11.57]

==>>Epoch=[195/250]], [2018-04-01 05:44:57], LR=[0.002], Batch=[32] [Model=SimpleNet]
  Epoch: [195][000/1475]   Time 0.140 (0.140)   Data 0.109 (0.109)   Loss 0.3514 (0.3514)   Prec@1 87.500 (87.500)   Prec@5 87.500 (87.500)   [2018-04-01 05:44:57]
  Epoch: [195][200/1475]   Time 0.145 (0.139)   Data 0.114 (0.106)   Loss 0.3465 (0.3361)   Prec@1 81.250 (85.992)   Prec@5 81.250 (85.992)   [2018-04-01 05:45:24]
  Epoch: [195][400/1475]   Time 0.144 (0.139)   Data 0.113 (0.105)   Loss 0.5121 (0.3318)   Prec@1 75.000 (85.934)   Prec@5 75.000 (85.934)   [2018-04-01 05:45:52]
  Epoch: [195][600/1475]   Time 0.146 (0.139)   Data 0.115 (0.106)   Loss 0.4679 (0.3301)   Prec@1 84.375 (86.034)   Prec@5 84.375 (86.034)   [2018-04-01 05:46:20]
  Epoch: [195][800/1475]   Time 0.145 (0.139)   Data 0.109 (0.106)   Loss 0.3260 (0.3331)   Prec@1 84.375 (85.830)   Prec@5 84.375 (85.830)   [2018-04-01 05:46:48]
  Epoch: [195][1000/1475]   Time 0.138 (0.139)   Data 0.105 (0.106)   Loss 0.3779 (0.3325)   Prec@1 87.500 (85.902)   Prec@5 87.500 (85.902)   [2018-04-01 05:47:16]
  Epoch: [195][1200/1475]   Time 0.126 (0.139)   Data 0.106 (0.106)   Loss 0.5034 (0.3328)   Prec@1 75.000 (85.814)   Prec@5 75.000 (85.814)   [2018-04-01 05:47:43]
  Epoch: [195][1400/1475]   Time 0.140 (0.139)   Data 0.106 (0.106)   Loss 0.2409 (0.3336)   Prec@1 90.625 (85.709)   Prec@5 90.625 (85.709)   [2018-04-01 05:48:11]
  **Train** Prec@1 85.695 Prec@5 85.695 Error@1 14.305
  **VAL** Prec@1 88.134 Prec@5 88.134 Error@1 11.866

==>>[2018-04-01 05:48:39] [Epoch=196/250] [Need: 03:21:05] [learning_rate=0.0002] [Best : Accuracy=88.43, Error=11.57]

==>>Epoch=[196/250]], [2018-04-01 05:48:39], LR=[0.002], Batch=[32] [Model=SimpleNet]
  Epoch: [196][000/1475]   Time 0.141 (0.141)   Data 0.109 (0.109)   Loss 0.2760 (0.2760)   Prec@1 87.500 (87.500)   Prec@5 87.500 (87.500)   [2018-04-01 05:48:39]
  Epoch: [196][200/1475]   Time 0.133 (0.139)   Data 0.106 (0.105)   Loss 0.5320 (0.3355)   Prec@1 75.000 (85.728)   Prec@5 75.000 (85.728)   [2018-04-01 05:49:07]
  Epoch: [196][400/1475]   Time 0.141 (0.139)   Data 0.109 (0.105)   Loss 0.3814 (0.3353)   Prec@1 84.375 (85.700)   Prec@5 84.375 (85.700)   [2018-04-01 05:49:35]
  Epoch: [196][600/1475]   Time 0.128 (0.139)   Data 0.097 (0.106)   Loss 0.2830 (0.3343)   Prec@1 90.625 (85.867)   Prec@5 90.625 (85.867)   [2018-04-01 05:50:03]
  Epoch: [196][800/1475]   Time 0.148 (0.139)   Data 0.113 (0.106)   Loss 0.3044 (0.3352)   Prec@1 84.375 (85.846)   Prec@5 84.375 (85.846)   [2018-04-01 05:50:30]
  Epoch: [196][1000/1475]   Time 0.151 (0.139)   Data 0.106 (0.106)   Loss 0.2505 (0.3342)   Prec@1 90.625 (85.861)   Prec@5 90.625 (85.861)   [2018-04-01 05:50:58]
  Epoch: [196][1200/1475]   Time 0.141 (0.139)   Data 0.109 (0.106)   Loss 0.4789 (0.3347)   Prec@1 71.875 (85.819)   Prec@5 71.875 (85.819)   [2018-04-01 05:51:26]
  Epoch: [196][1400/1475]   Time 0.141 (0.139)   Data 0.109 (0.106)   Loss 0.4327 (0.3352)   Prec@1 81.250 (85.843)   Prec@5 81.250 (85.843)   [2018-04-01 05:51:54]
  **Train** Prec@1 85.826 Prec@5 85.826 Error@1 14.174
  **VAL** Prec@1 88.074 Prec@5 88.074 Error@1 11.926

==>>[2018-04-01 05:52:22] [Epoch=197/250] [Need: 03:17:22] [learning_rate=0.0002] [Best : Accuracy=88.43, Error=11.57]

==>>Epoch=[197/250]], [2018-04-01 05:52:22], LR=[0.002], Batch=[32] [Model=SimpleNet]
  Epoch: [197][000/1475]   Time 0.130 (0.130)   Data 0.099 (0.099)   Loss 0.1648 (0.1648)   Prec@1 100.000 (100.000)   Prec@5 100.000 (100.000)   [2018-04-01 05:52:22]
  Epoch: [197][200/1475]   Time 0.141 (0.139)   Data 0.109 (0.106)   Loss 0.3569 (0.3457)   Prec@1 87.500 (84.950)   Prec@5 87.500 (84.950)   [2018-04-01 05:52:50]
  Epoch: [197][400/1475]   Time 0.141 (0.139)   Data 0.109 (0.105)   Loss 0.6167 (0.3420)   Prec@1 71.875 (85.240)   Prec@5 71.875 (85.240)   [2018-04-01 05:53:17]
  Epoch: [197][600/1475]   Time 0.146 (0.139)   Data 0.114 (0.105)   Loss 0.4192 (0.3405)   Prec@1 81.250 (85.410)   Prec@5 81.250 (85.410)   [2018-04-01 05:53:45]
  Epoch: [197][800/1475]   Time 0.141 (0.139)   Data 0.094 (0.106)   Loss 0.3194 (0.3372)   Prec@1 81.250 (85.514)   Prec@5 81.250 (85.514)   [2018-04-01 05:54:13]
  Epoch: [197][1000/1475]   Time 0.141 (0.139)   Data 0.109 (0.105)   Loss 0.2838 (0.3359)   Prec@1 90.625 (85.649)   Prec@5 90.625 (85.649)   [2018-04-01 05:54:41]
  Epoch: [197][1200/1475]   Time 0.141 (0.139)   Data 0.109 (0.105)   Loss 0.4200 (0.3351)   Prec@1 81.250 (85.720)   Prec@5 81.250 (85.720)   [2018-04-01 05:55:08]
  Epoch: [197][1400/1475]   Time 0.144 (0.139)   Data 0.113 (0.105)   Loss 0.1370 (0.3344)   Prec@1 96.875 (85.834)   Prec@5 96.875 (85.834)   [2018-04-01 05:55:36]
  **Train** Prec@1 85.860 Prec@5 85.860 Error@1 14.140
  **VAL** Prec@1 88.146 Prec@5 88.146 Error@1 11.854

==>>[2018-04-01 05:56:04] [Epoch=198/250] [Need: 03:13:38] [learning_rate=0.0002] [Best : Accuracy=88.43, Error=11.57]

==>>Epoch=[198/250]], [2018-04-01 05:56:04], LR=[0.002], Batch=[32] [Model=SimpleNet]
  Epoch: [198][000/1475]   Time 0.141 (0.141)   Data 0.109 (0.109)   Loss 0.2837 (0.2837)   Prec@1 90.625 (90.625)   Prec@5 90.625 (90.625)   [2018-04-01 05:56:04]
  Epoch: [198][200/1475]   Time 0.123 (0.139)   Data 0.103 (0.105)   Loss 0.2268 (0.3380)   Prec@1 87.500 (85.774)   Prec@5 87.500 (85.774)   [2018-04-01 05:56:32]
  Epoch: [198][400/1475]   Time 0.141 (0.139)   Data 0.109 (0.105)   Loss 0.2552 (0.3364)   Prec@1 87.500 (85.879)   Prec@5 87.500 (85.879)   [2018-04-01 05:57:00]
  Epoch: [198][600/1475]   Time 0.139 (0.139)   Data 0.107 (0.105)   Loss 0.2185 (0.3333)   Prec@1 93.750 (86.028)   Prec@5 93.750 (86.028)   [2018-04-01 05:57:28]
  Epoch: [198][800/1475]   Time 0.148 (0.139)   Data 0.112 (0.105)   Loss 0.3161 (0.3332)   Prec@1 78.125 (86.064)   Prec@5 78.125 (86.064)   [2018-04-01 05:57:55]
  Epoch: [198][1000/1475]   Time 0.139 (0.139)   Data 0.099 (0.105)   Loss 0.3163 (0.3353)   Prec@1 84.375 (85.986)   Prec@5 84.375 (85.986)   [2018-04-01 05:58:23]
  Epoch: [198][1200/1475]   Time 0.143 (0.139)   Data 0.112 (0.105)   Loss 0.3166 (0.3356)   Prec@1 87.500 (85.983)   Prec@5 87.500 (85.983)   [2018-04-01 05:58:51]
  Epoch: [198][1400/1475]   Time 0.141 (0.139)   Data 0.094 (0.105)   Loss 0.3579 (0.3345)   Prec@1 87.500 (85.977)   Prec@5 87.500 (85.977)   [2018-04-01 05:59:19]
  **Train** Prec@1 85.975 Prec@5 85.975 Error@1 14.025
  **VAL** Prec@1 88.350 Prec@5 88.350 Error@1 11.650

==>>[2018-04-01 05:59:47] [Epoch=199/250] [Need: 03:09:54] [learning_rate=0.0002] [Best : Accuracy=88.43, Error=11.57]

==>>Epoch=[199/250]], [2018-04-01 05:59:47], LR=[0.002], Batch=[32] [Model=SimpleNet]
  Epoch: [199][000/1475]   Time 0.141 (0.141)   Data 0.109 (0.109)   Loss 0.3241 (0.3241)   Prec@1 90.625 (90.625)   Prec@5 90.625 (90.625)   [2018-04-01 05:59:47]
  Epoch: [199][200/1475]   Time 0.141 (0.139)   Data 0.109 (0.105)   Loss 0.3499 (0.3401)   Prec@1 84.375 (85.603)   Prec@5 84.375 (85.603)   [2018-04-01 06:00:15]
  Epoch: [199][400/1475]   Time 0.153 (0.138)   Data 0.120 (0.105)   Loss 0.3646 (0.3345)   Prec@1 84.375 (85.825)   Prec@5 84.375 (85.825)   [2018-04-01 06:00:42]
  Epoch: [199][600/1475]   Time 0.137 (0.138)   Data 0.106 (0.106)   Loss 0.2590 (0.3373)   Prec@1 87.500 (85.550)   Prec@5 87.500 (85.550)   [2018-04-01 06:01:10]
  Epoch: [199][800/1475]   Time 0.141 (0.139)   Data 0.094 (0.105)   Loss 0.4383 (0.3360)   Prec@1 75.000 (85.643)   Prec@5 75.000 (85.643)   [2018-04-01 06:01:38]
  Epoch: [199][1000/1475]   Time 0.149 (0.139)   Data 0.114 (0.105)   Loss 0.4256 (0.3361)   Prec@1 81.250 (85.671)   Prec@5 81.250 (85.671)   [2018-04-01 06:02:05]
  Epoch: [199][1200/1475]   Time 0.141 (0.139)   Data 0.108 (0.105)   Loss 0.2703 (0.3357)   Prec@1 93.750 (85.702)   Prec@5 93.750 (85.702)   [2018-04-01 06:02:33]
  Epoch: [199][1400/1475]   Time 0.125 (0.139)   Data 0.094 (0.105)   Loss 0.2999 (0.3363)   Prec@1 84.375 (85.700)   Prec@5 84.375 (85.700)   [2018-04-01 06:03:01]
  **Train** Prec@1 85.758 Prec@5 85.758 Error@1 14.242
  **VAL** Prec@1 88.146 Prec@5 88.146 Error@1 11.854

==>>[2018-04-01 06:03:29] [Epoch=200/250] [Need: 03:06:11] [learning_rate=0.0002] [Best : Accuracy=88.43, Error=11.57]

==>>Epoch=[200/250]], [2018-04-01 06:03:29], LR=[0.002], Batch=[32] [Model=SimpleNet]
  Epoch: [200][000/1475]   Time 0.128 (0.128)   Data 0.097 (0.097)   Loss 0.4115 (0.4115)   Prec@1 84.375 (84.375)   Prec@5 84.375 (84.375)   [2018-04-01 06:03:29]
  Epoch: [200][200/1475]   Time 0.141 (0.138)   Data 0.109 (0.105)   Loss 0.3938 (0.3447)   Prec@1 84.375 (85.199)   Prec@5 84.375 (85.199)   [2018-04-01 06:03:57]
  Epoch: [200][400/1475]   Time 0.125 (0.139)   Data 0.094 (0.105)   Loss 0.4680 (0.3428)   Prec@1 84.375 (85.279)   Prec@5 84.375 (85.279)   [2018-04-01 06:04:25]
  Epoch: [200][600/1475]   Time 0.141 (0.139)   Data 0.109 (0.105)   Loss 0.5719 (0.3351)   Prec@1 68.750 (85.779)   Prec@5 68.750 (85.779)   [2018-04-01 06:04:52]
  Epoch: [200][800/1475]   Time 0.141 (0.139)   Data 0.109 (0.105)   Loss 0.4435 (0.3368)   Prec@1 81.250 (85.776)   Prec@5 81.250 (85.776)   [2018-04-01 06:05:20]
  Epoch: [200][1000/1475]   Time 0.144 (0.139)   Data 0.113 (0.105)   Loss 0.3213 (0.3357)   Prec@1 87.500 (85.892)   Prec@5 87.500 (85.892)   [2018-04-01 06:05:48]
  Epoch: [200][1200/1475]   Time 0.133 (0.139)   Data 0.102 (0.105)   Loss 0.2803 (0.3338)   Prec@1 87.500 (85.889)   Prec@5 87.500 (85.889)   [2018-04-01 06:06:15]
  Epoch: [200][1400/1475]   Time 0.132 (0.139)   Data 0.105 (0.105)   Loss 0.1836 (0.3326)   Prec@1 93.750 (85.923)   Prec@5 93.750 (85.923)   [2018-04-01 06:06:43]
  **Train** Prec@1 85.964 Prec@5 85.964 Error@1 14.036
  **VAL** Prec@1 88.086 Prec@5 88.086 Error@1 11.914

==>>[2018-04-01 06:07:11] [Epoch=201/250] [Need: 03:02:27] [learning_rate=0.0002] [Best : Accuracy=88.43, Error=11.57]

==>>Epoch=[201/250]], [2018-04-01 06:07:11], LR=[0.002], Batch=[32] [Model=SimpleNet]
  Epoch: [201][000/1475]   Time 0.141 (0.141)   Data 0.109 (0.109)   Loss 0.2910 (0.2910)   Prec@1 87.500 (87.500)   Prec@5 87.500 (87.500)   [2018-04-01 06:07:11]
  Epoch: [201][200/1475]   Time 0.146 (0.139)   Data 0.099 (0.106)   Loss 0.1806 (0.3427)   Prec@1 93.750 (85.230)   Prec@5 93.750 (85.230)   [2018-04-01 06:07:39]
  Epoch: [201][400/1475]   Time 0.134 (0.139)   Data 0.094 (0.106)   Loss 0.2031 (0.3367)   Prec@1 93.750 (85.396)   Prec@5 93.750 (85.396)   [2018-04-01 06:08:07]
  Epoch: [201][600/1475]   Time 0.141 (0.139)   Data 0.109 (0.105)   Loss 0.2188 (0.3328)   Prec@1 90.625 (85.607)   Prec@5 90.625 (85.607)   [2018-04-01 06:08:35]
  Epoch: [201][800/1475]   Time 0.141 (0.139)   Data 0.109 (0.105)   Loss 0.2901 (0.3318)   Prec@1 87.500 (85.819)   Prec@5 87.500 (85.819)   [2018-04-01 06:09:02]
  Epoch: [201][1000/1475]   Time 0.151 (0.139)   Data 0.115 (0.105)   Loss 0.3406 (0.3316)   Prec@1 84.375 (85.877)   Prec@5 84.375 (85.877)   [2018-04-01 06:09:30]
  Epoch: [201][1200/1475]   Time 0.137 (0.139)   Data 0.106 (0.105)   Loss 0.5154 (0.3337)   Prec@1 81.250 (85.811)   Prec@5 81.250 (85.811)   [2018-04-01 06:09:58]
  Epoch: [201][1400/1475]   Time 0.141 (0.139)   Data 0.109 (0.105)   Loss 0.2882 (0.3335)   Prec@1 93.750 (85.852)   Prec@5 93.750 (85.852)   [2018-04-01 06:10:26]
  **Train** Prec@1 85.839 Prec@5 85.839 Error@1 14.161
  **VAL** Prec@1 88.098 Prec@5 88.098 Error@1 11.902

==>>[2018-04-01 06:10:54] [Epoch=202/250] [Need: 02:58:43] [learning_rate=0.0002] [Best : Accuracy=88.43, Error=11.57]

==>>Epoch=[202/250]], [2018-04-01 06:10:54], LR=[0.002], Batch=[32] [Model=SimpleNet]
  Epoch: [202][000/1475]   Time 0.141 (0.141)   Data 0.109 (0.109)   Loss 0.2820 (0.2820)   Prec@1 87.500 (87.500)   Prec@5 87.500 (87.500)   [2018-04-01 06:10:54]
  Epoch: [202][200/1475]   Time 0.141 (0.139)   Data 0.109 (0.105)   Loss 0.5027 (0.3365)   Prec@1 84.375 (85.976)   Prec@5 84.375 (85.976)   [2018-04-01 06:11:22]
  Epoch: [202][400/1475]   Time 0.139 (0.139)   Data 0.092 (0.106)   Loss 0.3037 (0.3359)   Prec@1 87.500 (85.754)   Prec@5 87.500 (85.754)   [2018-04-01 06:11:49]
  Epoch: [202][600/1475]   Time 0.141 (0.139)   Data 0.109 (0.105)   Loss 0.3421 (0.3296)   Prec@1 84.375 (86.091)   Prec@5 84.375 (86.091)   [2018-04-01 06:12:17]
  Epoch: [202][800/1475]   Time 0.141 (0.139)   Data 0.109 (0.105)   Loss 0.3978 (0.3314)   Prec@1 81.250 (86.017)   Prec@5 81.250 (86.017)   [2018-04-01 06:12:45]
  Epoch: [202][1000/1475]   Time 0.135 (0.138)   Data 0.103 (0.105)   Loss 0.4456 (0.3334)   Prec@1 75.000 (85.942)   Prec@5 75.000 (85.942)   [2018-04-01 06:13:12]
  Epoch: [202][1200/1475]   Time 0.126 (0.138)   Data 0.095 (0.105)   Loss 0.2366 (0.3355)   Prec@1 90.625 (85.871)   Prec@5 90.625 (85.871)   [2018-04-01 06:13:40]
  Epoch: [202][1400/1475]   Time 0.136 (0.138)   Data 0.105 (0.105)   Loss 0.3948 (0.3346)   Prec@1 81.250 (85.914)   Prec@5 81.250 (85.914)   [2018-04-01 06:14:08]
  **Train** Prec@1 85.917 Prec@5 85.917 Error@1 14.083
  **VAL** Prec@1 88.206 Prec@5 88.206 Error@1 11.794

==>>[2018-04-01 06:14:36] [Epoch=203/250] [Need: 02:54:59] [learning_rate=0.0002] [Best : Accuracy=88.43, Error=11.57]

==>>Epoch=[203/250]], [2018-04-01 06:14:36], LR=[0.002], Batch=[32] [Model=SimpleNet]
  Epoch: [203][000/1475]   Time 0.125 (0.125)   Data 0.094 (0.094)   Loss 0.1128 (0.1128)   Prec@1 96.875 (96.875)   Prec@5 96.875 (96.875)   [2018-04-01 06:14:36]
  Epoch: [203][200/1475]   Time 0.141 (0.139)   Data 0.094 (0.105)   Loss 0.5864 (0.3357)   Prec@1 75.000 (85.821)   Prec@5 75.000 (85.821)   [2018-04-01 06:15:04]
  Epoch: [203][400/1475]   Time 0.141 (0.139)   Data 0.109 (0.105)   Loss 0.2326 (0.3386)   Prec@1 87.500 (85.536)   Prec@5 87.500 (85.536)   [2018-04-01 06:15:31]
  Epoch: [203][600/1475]   Time 0.146 (0.139)   Data 0.115 (0.106)   Loss 0.2692 (0.3378)   Prec@1 87.500 (85.753)   Prec@5 87.500 (85.753)   [2018-04-01 06:15:59]
  Epoch: [203][800/1475]   Time 0.129 (0.139)   Data 0.094 (0.106)   Loss 0.1408 (0.3379)   Prec@1 96.875 (85.701)   Prec@5 96.875 (85.701)   [2018-04-01 06:16:27]
  Epoch: [203][1000/1475]   Time 0.148 (0.139)   Data 0.117 (0.105)   Loss 0.1887 (0.3380)   Prec@1 96.875 (85.689)   Prec@5 96.875 (85.689)   [2018-04-01 06:16:55]
  Epoch: [203][1200/1475]   Time 0.149 (0.139)   Data 0.114 (0.106)   Loss 0.3523 (0.3372)   Prec@1 84.375 (85.666)   Prec@5 84.375 (85.666)   [2018-04-01 06:17:22]
  Epoch: [203][1400/1475]   Time 0.137 (0.139)   Data 0.094 (0.105)   Loss 0.4483 (0.3365)   Prec@1 78.125 (85.713)   Prec@5 78.125 (85.713)   [2018-04-01 06:17:50]
  **Train** Prec@1 85.737 Prec@5 85.737 Error@1 14.263
  **VAL** Prec@1 87.941 Prec@5 87.941 Error@1 12.059

==>>[2018-04-01 06:18:18] [Epoch=204/250] [Need: 02:51:16] [learning_rate=0.0002] [Best : Accuracy=88.43, Error=11.57]

==>>Epoch=[204/250]], [2018-04-01 06:18:18], LR=[0.002], Batch=[32] [Model=SimpleNet]
  Epoch: [204][000/1475]   Time 0.141 (0.141)   Data 0.109 (0.109)   Loss 0.2498 (0.2498)   Prec@1 93.750 (93.750)   Prec@5 93.750 (93.750)   [2018-04-01 06:18:18]
  Epoch: [204][200/1475]   Time 0.141 (0.139)   Data 0.109 (0.106)   Loss 0.3863 (0.3333)   Prec@1 84.375 (86.054)   Prec@5 84.375 (86.054)   [2018-04-01 06:18:46]
  Epoch: [204][400/1475]   Time 0.141 (0.139)   Data 0.094 (0.105)   Loss 0.3664 (0.3343)   Prec@1 84.375 (85.934)   Prec@5 84.375 (85.934)   [2018-04-01 06:19:14]
  Epoch: [204][600/1475]   Time 0.126 (0.139)   Data 0.095 (0.106)   Loss 0.3505 (0.3362)   Prec@1 84.375 (85.789)   Prec@5 84.375 (85.789)   [2018-04-01 06:19:41]
  Epoch: [204][800/1475]   Time 0.135 (0.139)   Data 0.099 (0.105)   Loss 0.3863 (0.3354)   Prec@1 84.375 (85.897)   Prec@5 84.375 (85.897)   [2018-04-01 06:20:09]
  Epoch: [204][1000/1475]   Time 0.129 (0.138)   Data 0.094 (0.105)   Loss 0.2683 (0.3328)   Prec@1 90.625 (86.058)   Prec@5 90.625 (86.058)   [2018-04-01 06:20:37]
  Epoch: [204][1200/1475]   Time 0.141 (0.138)   Data 0.109 (0.105)   Loss 0.4047 (0.3330)   Prec@1 84.375 (86.006)   Prec@5 84.375 (86.006)   [2018-04-01 06:21:05]
  Epoch: [204][1400/1475]   Time 0.127 (0.138)   Data 0.096 (0.105)   Loss 0.5131 (0.3330)   Prec@1 78.125 (86.032)   Prec@5 78.125 (86.032)   [2018-04-01 06:21:32]
  **Train** Prec@1 86.009 Prec@5 86.009 Error@1 13.991
  **VAL** Prec@1 88.098 Prec@5 88.098 Error@1 11.902

==>>[2018-04-01 06:22:00] [Epoch=205/250] [Need: 02:47:32] [learning_rate=0.0002] [Best : Accuracy=88.43, Error=11.57]

==>>Epoch=[205/250]], [2018-04-01 06:22:00], LR=[0.002], Batch=[32] [Model=SimpleNet]
  Epoch: [205][000/1475]   Time 0.149 (0.149)   Data 0.102 (0.102)   Loss 0.3635 (0.3635)   Prec@1 87.500 (87.500)   Prec@5 87.500 (87.500)   [2018-04-01 06:22:01]
  Epoch: [205][200/1475]   Time 0.151 (0.139)   Data 0.114 (0.105)   Loss 0.2575 (0.3360)   Prec@1 90.625 (85.448)   Prec@5 90.625 (85.448)   [2018-04-01 06:22:28]
  Epoch: [205][400/1475]   Time 0.132 (0.139)   Data 0.109 (0.106)   Loss 0.5490 (0.3371)   Prec@1 78.125 (85.692)   Prec@5 78.125 (85.692)   [2018-04-01 06:22:56]
  Epoch: [205][600/1475]   Time 0.145 (0.139)   Data 0.112 (0.106)   Loss 0.3111 (0.3326)   Prec@1 84.375 (85.930)   Prec@5 84.375 (85.930)   [2018-04-01 06:23:24]
  Epoch: [205][800/1475]   Time 0.125 (0.139)   Data 0.094 (0.106)   Loss 0.2336 (0.3316)   Prec@1 90.625 (85.975)   Prec@5 90.625 (85.975)   [2018-04-01 06:23:52]
  Epoch: [205][1000/1475]   Time 0.141 (0.139)   Data 0.109 (0.106)   Loss 0.3774 (0.3353)   Prec@1 81.250 (85.824)   Prec@5 81.250 (85.824)   [2018-04-01 06:24:19]
  Epoch: [205][1200/1475]   Time 0.141 (0.139)   Data 0.094 (0.106)   Loss 0.4155 (0.3346)   Prec@1 75.000 (85.835)   Prec@5 75.000 (85.835)   [2018-04-01 06:24:47]
  Epoch: [205][1400/1475]   Time 0.141 (0.139)   Data 0.109 (0.106)   Loss 0.3921 (0.3359)   Prec@1 87.500 (85.738)   Prec@5 87.500 (85.738)   [2018-04-01 06:25:15]
  **Train** Prec@1 85.790 Prec@5 85.790 Error@1 14.210
  **VAL** Prec@1 88.134 Prec@5 88.134 Error@1 11.866

==>>[2018-04-01 06:25:43] [Epoch=206/250] [Need: 02:43:49] [learning_rate=0.0002] [Best : Accuracy=88.43, Error=11.57]

==>>Epoch=[206/250]], [2018-04-01 06:25:43], LR=[0.002], Batch=[32] [Model=SimpleNet]
  Epoch: [206][000/1475]   Time 0.157 (0.157)   Data 0.126 (0.126)   Loss 0.2195 (0.2195)   Prec@1 96.875 (96.875)   Prec@5 96.875 (96.875)   [2018-04-01 06:25:43]
  Epoch: [206][200/1475]   Time 0.141 (0.139)   Data 0.109 (0.106)   Loss 0.3850 (0.3394)   Prec@1 84.375 (85.759)   Prec@5 84.375 (85.759)   [2018-04-01 06:26:11]
  Epoch: [206][400/1475]   Time 0.141 (0.139)   Data 0.109 (0.106)   Loss 0.3225 (0.3342)   Prec@1 87.500 (85.957)   Prec@5 87.500 (85.957)   [2018-04-01 06:26:38]
  Epoch: [206][600/1475]   Time 0.131 (0.139)   Data 0.099 (0.106)   Loss 0.4301 (0.3292)   Prec@1 84.375 (86.179)   Prec@5 84.375 (86.179)   [2018-04-01 06:27:06]
  Epoch: [206][800/1475]   Time 0.144 (0.139)   Data 0.113 (0.105)   Loss 0.2897 (0.3307)   Prec@1 87.500 (86.060)   Prec@5 87.500 (86.060)   [2018-04-01 06:27:34]
  Epoch: [206][1000/1475]   Time 0.141 (0.139)   Data 0.109 (0.105)   Loss 0.3241 (0.3305)   Prec@1 87.500 (86.026)   Prec@5 87.500 (86.026)   [2018-04-01 06:28:02]
  Epoch: [206][1200/1475]   Time 0.141 (0.139)   Data 0.109 (0.105)   Loss 0.1854 (0.3326)   Prec@1 93.750 (85.949)   Prec@5 93.750 (85.949)   [2018-04-01 06:28:29]
  Epoch: [206][1400/1475]   Time 0.141 (0.139)   Data 0.109 (0.105)   Loss 0.2097 (0.3339)   Prec@1 90.625 (85.943)   Prec@5 90.625 (85.943)   [2018-04-01 06:28:57]
  **Train** Prec@1 85.964 Prec@5 85.964 Error@1 14.036
  **VAL** Prec@1 88.254 Prec@5 88.254 Error@1 11.746

==>>[2018-04-01 06:29:25] [Epoch=207/250] [Need: 02:40:05] [learning_rate=0.0002] [Best : Accuracy=88.43, Error=11.57]

==>>Epoch=[207/250]], [2018-04-01 06:29:25], LR=[0.002], Batch=[32] [Model=SimpleNet]
  Epoch: [207][000/1475]   Time 0.141 (0.141)   Data 0.109 (0.109)   Loss 0.2727 (0.2727)   Prec@1 87.500 (87.500)   Prec@5 87.500 (87.500)   [2018-04-01 06:29:25]
  Epoch: [207][200/1475]   Time 0.145 (0.139)   Data 0.109 (0.105)   Loss 0.2467 (0.3298)   Prec@1 93.750 (86.007)   Prec@5 93.750 (86.007)   [2018-04-01 06:29:53]
  Epoch: [207][400/1475]   Time 0.141 (0.139)   Data 0.109 (0.105)   Loss 0.1970 (0.3314)   Prec@1 90.625 (85.973)   Prec@5 90.625 (85.973)   [2018-04-01 06:30:21]
  Epoch: [207][600/1475]   Time 0.144 (0.139)   Data 0.112 (0.105)   Loss 0.1946 (0.3335)   Prec@1 93.750 (85.945)   Prec@5 93.750 (85.945)   [2018-04-01 06:30:48]
  Epoch: [207][800/1475]   Time 0.141 (0.139)   Data 0.109 (0.105)   Loss 0.2444 (0.3343)   Prec@1 93.750 (85.943)   Prec@5 93.750 (85.943)   [2018-04-01 06:31:16]
  Epoch: [207][1000/1475]   Time 0.141 (0.139)   Data 0.109 (0.105)   Loss 0.2461 (0.3335)   Prec@1 93.750 (85.967)   Prec@5 93.750 (85.967)   [2018-04-01 06:31:44]
  Epoch: [207][1200/1475]   Time 0.141 (0.139)   Data 0.109 (0.105)   Loss 0.3774 (0.3329)   Prec@1 87.500 (85.954)   Prec@5 87.500 (85.954)   [2018-04-01 06:32:12]
  Epoch: [207][1400/1475]   Time 0.125 (0.139)   Data 0.094 (0.105)   Loss 0.2688 (0.3335)   Prec@1 87.500 (85.961)   Prec@5 87.500 (85.961)   [2018-04-01 06:32:39]
  **Train** Prec@1 85.956 Prec@5 85.956 Error@1 14.044
  **VAL** Prec@1 88.290 Prec@5 88.290 Error@1 11.710

==>>[2018-04-01 06:33:08] [Epoch=208/250] [Need: 02:36:21] [learning_rate=0.0002] [Best : Accuracy=88.43, Error=11.57]

==>>Epoch=[208/250]], [2018-04-01 06:33:08], LR=[0.002], Batch=[32] [Model=SimpleNet]
  Epoch: [208][000/1475]   Time 0.141 (0.141)   Data 0.109 (0.109)   Loss 0.2006 (0.2006)   Prec@1 90.625 (90.625)   Prec@5 90.625 (90.625)   [2018-04-01 06:33:08]
  Epoch: [208][200/1475]   Time 0.143 (0.139)   Data 0.110 (0.105)   Loss 0.3353 (0.3330)   Prec@1 84.375 (85.914)   Prec@5 84.375 (85.914)   [2018-04-01 06:33:35]
  Epoch: [208][400/1475]   Time 0.142 (0.139)   Data 0.109 (0.106)   Loss 0.4426 (0.3373)   Prec@1 84.375 (85.762)   Prec@5 84.375 (85.762)   [2018-04-01 06:34:03]
  Epoch: [208][600/1475]   Time 0.141 (0.139)   Data 0.109 (0.105)   Loss 0.3871 (0.3346)   Prec@1 78.125 (85.899)   Prec@5 78.125 (85.899)   [2018-04-01 06:34:31]
  Epoch: [208][800/1475]   Time 0.129 (0.139)   Data 0.098 (0.105)   Loss 0.5432 (0.3363)   Prec@1 71.875 (85.701)   Prec@5 71.875 (85.701)   [2018-04-01 06:34:59]
  Epoch: [208][1000/1475]   Time 0.141 (0.139)   Data 0.109 (0.105)   Loss 0.5965 (0.3370)   Prec@1 71.875 (85.574)   Prec@5 71.875 (85.574)   [2018-04-01 06:35:26]
  Epoch: [208][1200/1475]   Time 0.148 (0.139)   Data 0.117 (0.105)   Loss 0.2281 (0.3363)   Prec@1 90.625 (85.650)   Prec@5 90.625 (85.650)   [2018-04-01 06:35:54]
  Epoch: [208][1400/1475]   Time 0.141 (0.139)   Data 0.109 (0.105)   Loss 0.5237 (0.3352)   Prec@1 84.375 (85.700)   Prec@5 84.375 (85.700)   [2018-04-01 06:36:22]
  **Train** Prec@1 85.729 Prec@5 85.729 Error@1 14.271
  **VAL** Prec@1 88.446 Prec@5 88.446 Error@1 11.554

==>>[2018-04-01 06:36:50] [Epoch=209/250] [Need: 02:32:38] [learning_rate=0.0002] [Best : Accuracy=88.45, Error=11.55]

==>>Epoch=[209/250]], [2018-04-01 06:36:50], LR=[0.002], Batch=[32] [Model=SimpleNet]
  Epoch: [209][000/1475]   Time 0.144 (0.144)   Data 0.113 (0.113)   Loss 0.2576 (0.2576)   Prec@1 84.375 (84.375)   Prec@5 84.375 (84.375)   [2018-04-01 06:36:50]
  Epoch: [209][200/1475]   Time 0.148 (0.139)   Data 0.117 (0.105)   Loss 0.2926 (0.3359)   Prec@1 81.250 (85.525)   Prec@5 81.250 (85.525)   [2018-04-01 06:37:18]
  Epoch: [209][400/1475]   Time 0.141 (0.139)   Data 0.094 (0.105)   Loss 0.2703 (0.3332)   Prec@1 87.500 (85.731)   Prec@5 87.500 (85.731)   [2018-04-01 06:37:46]
  Epoch: [209][600/1475]   Time 0.139 (0.139)   Data 0.093 (0.106)   Loss 0.3529 (0.3364)   Prec@1 84.375 (85.696)   Prec@5 84.375 (85.696)   [2018-04-01 06:38:13]
  Epoch: [209][800/1475]   Time 0.125 (0.139)   Data 0.094 (0.105)   Loss 0.3856 (0.3361)   Prec@1 84.375 (85.748)   Prec@5 84.375 (85.748)   [2018-04-01 06:38:41]
  Epoch: [209][1000/1475]   Time 0.129 (0.139)   Data 0.098 (0.105)   Loss 0.2531 (0.3374)   Prec@1 90.625 (85.699)   Prec@5 90.625 (85.699)   [2018-04-01 06:39:09]
  Epoch: [209][1200/1475]   Time 0.137 (0.139)   Data 0.106 (0.105)   Loss 0.2496 (0.3355)   Prec@1 84.375 (85.777)   Prec@5 84.375 (85.777)   [2018-04-01 06:39:37]
  Epoch: [209][1400/1475]   Time 0.125 (0.139)   Data 0.094 (0.105)   Loss 0.3610 (0.3367)   Prec@1 84.375 (85.740)   Prec@5 84.375 (85.740)   [2018-04-01 06:40:04]
  **Train** Prec@1 85.790 Prec@5 85.790 Error@1 14.210
  **VAL** Prec@1 88.098 Prec@5 88.098 Error@1 11.902

==>>[2018-04-01 06:40:32] [Epoch=210/250] [Need: 02:28:54] [learning_rate=0.0002] [Best : Accuracy=88.45, Error=11.55]

==>>Epoch=[210/250]], [2018-04-01 06:40:32], LR=[0.002], Batch=[32] [Model=SimpleNet]
  Epoch: [210][000/1475]   Time 0.141 (0.141)   Data 0.094 (0.094)   Loss 0.4072 (0.4072)   Prec@1 84.375 (84.375)   Prec@5 84.375 (84.375)   [2018-04-01 06:40:33]
  Epoch: [210][200/1475]   Time 0.156 (0.139)   Data 0.109 (0.105)   Loss 0.4642 (0.3485)   Prec@1 78.125 (85.215)   Prec@5 78.125 (85.215)   [2018-04-01 06:41:00]
  Epoch: [210][400/1475]   Time 0.146 (0.139)   Data 0.109 (0.106)   Loss 0.3304 (0.3427)   Prec@1 78.125 (85.521)   Prec@5 78.125 (85.521)   [2018-04-01 06:41:28]
  Epoch: [210][600/1475]   Time 0.125 (0.139)   Data 0.094 (0.106)   Loss 0.4203 (0.3410)   Prec@1 78.125 (85.607)   Prec@5 78.125 (85.607)   [2018-04-01 06:41:56]
  Epoch: [210][800/1475]   Time 0.144 (0.139)   Data 0.097 (0.106)   Loss 0.3509 (0.3378)   Prec@1 84.375 (85.717)   Prec@5 84.375 (85.717)   [2018-04-01 06:42:24]
  Epoch: [210][1000/1475]   Time 0.141 (0.139)   Data 0.109 (0.106)   Loss 0.3956 (0.3364)   Prec@1 81.250 (85.736)   Prec@5 81.250 (85.736)   [2018-04-01 06:42:51]
  Epoch: [210][1200/1475]   Time 0.139 (0.139)   Data 0.108 (0.106)   Loss 0.3750 (0.3340)   Prec@1 84.375 (85.887)   Prec@5 84.375 (85.887)   [2018-04-01 06:43:19]
  Epoch: [210][1400/1475]   Time 0.141 (0.139)   Data 0.109 (0.106)   Loss 0.3664 (0.3325)   Prec@1 84.375 (85.921)   Prec@5 84.375 (85.921)   [2018-04-01 06:43:47]
  **Train** Prec@1 85.881 Prec@5 85.881 Error@1 14.119
  **VAL** Prec@1 88.230 Prec@5 88.230 Error@1 11.770

==>>[2018-04-01 06:44:15] [Epoch=211/250] [Need: 02:25:11] [learning_rate=0.0002] [Best : Accuracy=88.45, Error=11.55]

==>>Epoch=[211/250]], [2018-04-01 06:44:15], LR=[0.002], Batch=[32] [Model=SimpleNet]
  Epoch: [211][000/1475]   Time 0.138 (0.138)   Data 0.104 (0.104)   Loss 0.4612 (0.4612)   Prec@1 75.000 (75.000)   Prec@5 75.000 (75.000)   [2018-04-01 06:44:15]
  Epoch: [211][200/1475]   Time 0.128 (0.139)   Data 0.097 (0.106)   Loss 0.2941 (0.3252)   Prec@1 84.375 (86.039)   Prec@5 84.375 (86.039)   [2018-04-01 06:44:43]
  Epoch: [211][400/1475]   Time 0.132 (0.139)   Data 0.101 (0.105)   Loss 0.2327 (0.3256)   Prec@1 87.500 (86.253)   Prec@5 87.500 (86.253)   [2018-04-01 06:45:11]
  Epoch: [211][600/1475]   Time 0.150 (0.139)   Data 0.117 (0.106)   Loss 0.3396 (0.3325)   Prec@1 78.125 (86.096)   Prec@5 78.125 (86.096)   [2018-04-01 06:45:38]
  Epoch: [211][800/1475]   Time 0.133 (0.139)   Data 0.102 (0.105)   Loss 0.3247 (0.3311)   Prec@1 90.625 (86.189)   Prec@5 90.625 (86.189)   [2018-04-01 06:46:06]
  Epoch: [211][1000/1475]   Time 0.137 (0.139)   Data 0.113 (0.106)   Loss 0.3527 (0.3314)   Prec@1 84.375 (86.036)   Prec@5 84.375 (86.036)   [2018-04-01 06:46:34]
  Epoch: [211][1200/1475]   Time 0.125 (0.139)   Data 0.094 (0.106)   Loss 0.2957 (0.3310)   Prec@1 87.500 (86.027)   Prec@5 87.500 (86.027)   [2018-04-01 06:47:02]
  Epoch: [211][1400/1475]   Time 0.144 (0.139)   Data 0.113 (0.106)   Loss 0.3946 (0.3325)   Prec@1 84.375 (85.988)   Prec@5 84.375 (85.988)   [2018-04-01 06:47:30]
  **Train** Prec@1 85.907 Prec@5 85.907 Error@1 14.093
  **VAL** Prec@1 88.422 Prec@5 88.422 Error@1 11.578

==>>[2018-04-01 06:47:58] [Epoch=212/250] [Need: 02:21:27] [learning_rate=0.0002] [Best : Accuracy=88.45, Error=11.55]

==>>Epoch=[212/250]], [2018-04-01 06:47:58], LR=[0.002], Batch=[32] [Model=SimpleNet]
  Epoch: [212][000/1475]   Time 0.141 (0.141)   Data 0.109 (0.109)   Loss 0.5007 (0.5007)   Prec@1 75.000 (75.000)   Prec@5 75.000 (75.000)   [2018-04-01 06:47:58]
  Epoch: [212][200/1475]   Time 0.132 (0.139)   Data 0.101 (0.105)   Loss 0.3059 (0.3301)   Prec@1 90.625 (85.899)   Prec@5 90.625 (85.899)   [2018-04-01 06:48:26]
  Epoch: [212][400/1475]   Time 0.130 (0.139)   Data 0.098 (0.105)   Loss 0.3342 (0.3314)   Prec@1 87.500 (85.801)   Prec@5 87.500 (85.801)   [2018-04-01 06:48:53]
  Epoch: [212][600/1475]   Time 0.143 (0.139)   Data 0.109 (0.105)   Loss 0.2303 (0.3311)   Prec@1 87.500 (85.883)   Prec@5 87.500 (85.883)   [2018-04-01 06:49:21]
  Epoch: [212][800/1475]   Time 0.141 (0.139)   Data 0.109 (0.105)   Loss 0.1616 (0.3355)   Prec@1 96.875 (85.659)   Prec@5 96.875 (85.659)   [2018-04-01 06:49:49]
  Epoch: [212][1000/1475]   Time 0.140 (0.139)   Data 0.107 (0.105)   Loss 0.3910 (0.3338)   Prec@1 81.250 (85.780)   Prec@5 81.250 (85.780)   [2018-04-01 06:50:17]
  Epoch: [212][1200/1475]   Time 0.144 (0.139)   Data 0.113 (0.105)   Loss 0.2602 (0.3347)   Prec@1 87.500 (85.733)   Prec@5 87.500 (85.733)   [2018-04-01 06:50:44]
  Epoch: [212][1400/1475]   Time 0.141 (0.139)   Data 0.110 (0.105)   Loss 0.3318 (0.3349)   Prec@1 87.500 (85.749)   Prec@5 87.500 (85.749)   [2018-04-01 06:51:12]
  **Train** Prec@1 85.735 Prec@5 85.735 Error@1 14.265
  **VAL** Prec@1 88.230 Prec@5 88.230 Error@1 11.770

==>>[2018-04-01 06:51:40] [Epoch=213/250] [Need: 02:17:44] [learning_rate=0.0002] [Best : Accuracy=88.45, Error=11.55]

==>>Epoch=[213/250]], [2018-04-01 06:51:40], LR=[0.002], Batch=[32] [Model=SimpleNet]
  Epoch: [213][000/1475]   Time 0.141 (0.141)   Data 0.094 (0.094)   Loss 0.1713 (0.1713)   Prec@1 93.750 (93.750)   Prec@5 93.750 (93.750)   [2018-04-01 06:51:40]
  Epoch: [213][200/1475]   Time 0.145 (0.139)   Data 0.114 (0.106)   Loss 0.3674 (0.3285)   Prec@1 84.375 (86.023)   Prec@5 84.375 (86.023)   [2018-04-01 06:52:08]
  Epoch: [213][400/1475]   Time 0.141 (0.139)   Data 0.109 (0.105)   Loss 0.3504 (0.3279)   Prec@1 87.500 (86.214)   Prec@5 87.500 (86.214)   [2018-04-01 06:52:36]
  Epoch: [213][600/1475]   Time 0.125 (0.139)   Data 0.094 (0.105)   Loss 0.2943 (0.3297)   Prec@1 90.625 (86.210)   Prec@5 90.625 (86.210)   [2018-04-01 06:53:04]
  Epoch: [213][800/1475]   Time 0.124 (0.139)   Data 0.093 (0.105)   Loss 0.3859 (0.3306)   Prec@1 87.500 (86.166)   Prec@5 87.500 (86.166)   [2018-04-01 06:53:31]
  Epoch: [213][1000/1475]   Time 0.149 (0.139)   Data 0.109 (0.105)   Loss 0.3032 (0.3330)   Prec@1 87.500 (85.995)   Prec@5 87.500 (85.995)   [2018-04-01 06:53:59]
  Epoch: [213][1200/1475]   Time 0.141 (0.139)   Data 0.094 (0.105)   Loss 0.5380 (0.3349)   Prec@1 75.000 (85.884)   Prec@5 75.000 (85.884)   [2018-04-01 06:54:27]
  Epoch: [213][1400/1475]   Time 0.141 (0.139)   Data 0.109 (0.105)   Loss 0.2442 (0.3341)   Prec@1 90.625 (85.945)   Prec@5 90.625 (85.945)   [2018-04-01 06:54:54]
  **Train** Prec@1 85.975 Prec@5 85.975 Error@1 14.025
  **VAL** Prec@1 88.086 Prec@5 88.086 Error@1 11.914

==>>[2018-04-01 06:55:22] [Epoch=214/250] [Need: 02:14:00] [learning_rate=0.0002] [Best : Accuracy=88.45, Error=11.55]

==>>Epoch=[214/250]], [2018-04-01 06:55:22], LR=[0.002], Batch=[32] [Model=SimpleNet]
  Epoch: [214][000/1475]   Time 0.141 (0.141)   Data 0.109 (0.109)   Loss 0.5767 (0.5767)   Prec@1 75.000 (75.000)   Prec@5 75.000 (75.000)   [2018-04-01 06:55:23]
  Epoch: [214][200/1475]   Time 0.145 (0.139)   Data 0.111 (0.105)   Loss 0.4304 (0.3287)   Prec@1 71.875 (85.930)   Prec@5 71.875 (85.930)   [2018-04-01 06:55:50]
  Epoch: [214][400/1475]   Time 0.141 (0.139)   Data 0.109 (0.105)   Loss 0.4135 (0.3368)   Prec@1 78.125 (85.754)   Prec@5 78.125 (85.754)   [2018-04-01 06:56:18]
  Epoch: [214][600/1475]   Time 0.125 (0.139)   Data 0.094 (0.105)   Loss 0.2649 (0.3344)   Prec@1 90.625 (85.935)   Prec@5 90.625 (85.935)   [2018-04-01 06:56:46]
  Epoch: [214][800/1475]   Time 0.141 (0.139)   Data 0.109 (0.105)   Loss 0.4304 (0.3306)   Prec@1 75.000 (86.064)   Prec@5 75.000 (86.064)   [2018-04-01 06:57:13]
  Epoch: [214][1000/1475]   Time 0.131 (0.139)   Data 0.100 (0.105)   Loss 0.2841 (0.3319)   Prec@1 81.250 (85.930)   Prec@5 81.250 (85.930)   [2018-04-01 06:57:41]
  Epoch: [214][1200/1475]   Time 0.141 (0.139)   Data 0.094 (0.105)   Loss 0.2360 (0.3321)   Prec@1 90.625 (85.941)   Prec@5 90.625 (85.941)   [2018-04-01 06:58:09]
  Epoch: [214][1400/1475]   Time 0.129 (0.139)   Data 0.097 (0.105)   Loss 0.4389 (0.3316)   Prec@1 81.250 (85.932)   Prec@5 81.250 (85.932)   [2018-04-01 06:58:37]
  **Train** Prec@1 85.930 Prec@5 85.930 Error@1 14.070
  **VAL** Prec@1 88.086 Prec@5 88.086 Error@1 11.914

==>>[2018-04-01 06:59:05] [Epoch=215/250] [Need: 02:10:17] [learning_rate=0.0002] [Best : Accuracy=88.45, Error=11.55]

==>>Epoch=[215/250]], [2018-04-01 06:59:05], LR=[0.002], Batch=[32] [Model=SimpleNet]
  Epoch: [215][000/1475]   Time 0.143 (0.143)   Data 0.110 (0.110)   Loss 0.2611 (0.2611)   Prec@1 90.625 (90.625)   Prec@5 90.625 (90.625)   [2018-04-01 06:59:05]
  Epoch: [215][200/1475]   Time 0.141 (0.138)   Data 0.109 (0.106)   Loss 0.2865 (0.3396)   Prec@1 84.375 (85.432)   Prec@5 84.375 (85.432)   [2018-04-01 06:59:33]
  Epoch: [215][400/1475]   Time 0.133 (0.138)   Data 0.102 (0.106)   Loss 0.2797 (0.3401)   Prec@1 87.500 (85.388)   Prec@5 87.500 (85.388)   [2018-04-01 07:00:00]
  Epoch: [215][600/1475]   Time 0.142 (0.138)   Data 0.111 (0.106)   Loss 0.4379 (0.3362)   Prec@1 81.250 (85.732)   Prec@5 81.250 (85.732)   [2018-04-01 07:00:28]
  Epoch: [215][800/1475]   Time 0.125 (0.139)   Data 0.094 (0.106)   Loss 0.5292 (0.3357)   Prec@1 81.250 (85.799)   Prec@5 81.250 (85.799)   [2018-04-01 07:00:56]
  Epoch: [215][1000/1475]   Time 0.147 (0.139)   Data 0.109 (0.106)   Loss 0.2478 (0.3352)   Prec@1 87.500 (85.839)   Prec@5 87.500 (85.839)   [2018-04-01 07:01:24]
  Epoch: [215][1200/1475]   Time 0.147 (0.139)   Data 0.109 (0.106)   Loss 0.3230 (0.3340)   Prec@1 87.500 (85.882)   Prec@5 87.500 (85.882)   [2018-04-01 07:01:51]
  Epoch: [215][1400/1475]   Time 0.141 (0.139)   Data 0.094 (0.106)   Loss 0.3628 (0.3329)   Prec@1 87.500 (85.912)   Prec@5 87.500 (85.912)   [2018-04-01 07:02:19]
  **Train** Prec@1 85.917 Prec@5 85.917 Error@1 14.083
  **VAL** Prec@1 88.314 Prec@5 88.314 Error@1 11.686

==>>[2018-04-01 07:02:47] [Epoch=216/250] [Need: 02:06:33] [learning_rate=0.0002] [Best : Accuracy=88.45, Error=11.55]

==>>Epoch=[216/250]], [2018-04-01 07:02:47], LR=[0.002], Batch=[32] [Model=SimpleNet]
  Epoch: [216][000/1475]   Time 0.147 (0.147)   Data 0.115 (0.115)   Loss 0.3138 (0.3138)   Prec@1 81.250 (81.250)   Prec@5 81.250 (81.250)   [2018-04-01 07:02:47]
  Epoch: [216][200/1475]   Time 0.141 (0.139)   Data 0.109 (0.105)   Loss 0.2227 (0.3318)   Prec@1 90.625 (86.256)   Prec@5 90.625 (86.256)   [2018-04-01 07:03:15]
  Epoch: [216][400/1475]   Time 0.125 (0.139)   Data 0.094 (0.106)   Loss 0.5635 (0.3343)   Prec@1 78.125 (86.074)   Prec@5 78.125 (86.074)   [2018-04-01 07:03:43]
  Epoch: [216][600/1475]   Time 0.141 (0.139)   Data 0.109 (0.106)   Loss 0.2799 (0.3345)   Prec@1 87.500 (85.987)   Prec@5 87.500 (85.987)   [2018-04-01 07:04:11]
  Epoch: [216][800/1475]   Time 0.141 (0.139)   Data 0.109 (0.106)   Loss 0.2623 (0.3318)   Prec@1 84.375 (85.986)   Prec@5 84.375 (85.986)   [2018-04-01 07:04:38]
  Epoch: [216][1000/1475]   Time 0.161 (0.139)   Data 0.114 (0.106)   Loss 0.2786 (0.3344)   Prec@1 87.500 (85.883)   Prec@5 87.500 (85.883)   [2018-04-01 07:05:06]
  Epoch: [216][1200/1475]   Time 0.130 (0.139)   Data 0.099 (0.106)   Loss 0.3503 (0.3327)   Prec@1 84.375 (85.944)   Prec@5 84.375 (85.944)   [2018-04-01 07:05:34]
  Epoch: [216][1400/1475]   Time 0.133 (0.139)   Data 0.102 (0.106)   Loss 0.4600 (0.3333)   Prec@1 78.125 (85.894)   Prec@5 78.125 (85.894)   [2018-04-01 07:06:02]
  **Train** Prec@1 85.898 Prec@5 85.898 Error@1 14.102
  **VAL** Prec@1 88.278 Prec@5 88.278 Error@1 11.722

==>>[2018-04-01 07:06:30] [Epoch=217/250] [Need: 02:02:50] [learning_rate=0.0002] [Best : Accuracy=88.45, Error=11.55]

==>>Epoch=[217/250]], [2018-04-01 07:06:30], LR=[0.002], Batch=[32] [Model=SimpleNet]
  Epoch: [217][000/1475]   Time 0.143 (0.143)   Data 0.112 (0.112)   Loss 0.3160 (0.3160)   Prec@1 90.625 (90.625)   Prec@5 90.625 (90.625)   [2018-04-01 07:06:30]
  Epoch: [217][200/1475]   Time 0.141 (0.139)   Data 0.094 (0.106)   Loss 0.2225 (0.3392)   Prec@1 84.375 (85.603)   Prec@5 84.375 (85.603)   [2018-04-01 07:06:58]
  Epoch: [217][400/1475]   Time 0.147 (0.139)   Data 0.109 (0.105)   Loss 0.2330 (0.3319)   Prec@1 96.875 (86.019)   Prec@5 96.875 (86.019)   [2018-04-01 07:07:25]
  Epoch: [217][600/1475]   Time 0.143 (0.139)   Data 0.110 (0.105)   Loss 0.1853 (0.3330)   Prec@1 93.750 (86.044)   Prec@5 93.750 (86.044)   [2018-04-01 07:07:53]
  Epoch: [217][800/1475]   Time 0.141 (0.139)   Data 0.109 (0.105)   Loss 0.3596 (0.3344)   Prec@1 78.125 (85.990)   Prec@5 78.125 (85.990)   [2018-04-01 07:08:21]
  Epoch: [217][1000/1475]   Time 0.125 (0.139)   Data 0.094 (0.105)   Loss 0.1396 (0.3318)   Prec@1 93.750 (86.058)   Prec@5 93.750 (86.058)   [2018-04-01 07:08:49]
  Epoch: [217][1200/1475]   Time 0.135 (0.139)   Data 0.112 (0.105)   Loss 0.3859 (0.3341)   Prec@1 84.375 (85.931)   Prec@5 84.375 (85.931)   [2018-04-01 07:09:16]
  Epoch: [217][1400/1475]   Time 0.154 (0.139)   Data 0.121 (0.105)   Loss 0.2467 (0.3342)   Prec@1 90.625 (85.961)   Prec@5 90.625 (85.961)   [2018-04-01 07:09:44]
  **Train** Prec@1 85.987 Prec@5 85.987 Error@1 14.013
  **VAL** Prec@1 88.386 Prec@5 88.386 Error@1 11.614

==>>[2018-04-01 07:10:12] [Epoch=218/250] [Need: 01:59:06] [learning_rate=0.0002] [Best : Accuracy=88.45, Error=11.55]

==>>Epoch=[218/250]], [2018-04-01 07:10:12], LR=[0.002], Batch=[32] [Model=SimpleNet]
  Epoch: [218][000/1475]   Time 0.136 (0.136)   Data 0.105 (0.105)   Loss 0.2557 (0.2557)   Prec@1 90.625 (90.625)   Prec@5 90.625 (90.625)   [2018-04-01 07:10:12]
  Epoch: [218][200/1475]   Time 0.130 (0.139)   Data 0.099 (0.106)   Loss 0.3653 (0.3301)   Prec@1 84.375 (86.303)   Prec@5 84.375 (86.303)   [2018-04-01 07:10:40]
  Epoch: [218][400/1475]   Time 0.142 (0.139)   Data 0.109 (0.106)   Loss 0.4467 (0.3375)   Prec@1 78.125 (85.895)   Prec@5 78.125 (85.895)   [2018-04-01 07:11:08]
  Epoch: [218][600/1475]   Time 0.141 (0.139)   Data 0.094 (0.106)   Loss 0.4270 (0.3321)   Prec@1 84.375 (86.174)   Prec@5 84.375 (86.174)   [2018-04-01 07:11:35]
  Epoch: [218][800/1475]   Time 0.141 (0.139)   Data 0.109 (0.106)   Loss 0.4899 (0.3290)   Prec@1 75.000 (86.228)   Prec@5 75.000 (86.228)   [2018-04-01 07:12:03]
  Epoch: [218][1000/1475]   Time 0.131 (0.139)   Data 0.100 (0.106)   Loss 0.2745 (0.3300)   Prec@1 87.500 (86.117)   Prec@5 87.500 (86.117)   [2018-04-01 07:12:31]
  Epoch: [218][1200/1475]   Time 0.125 (0.139)   Data 0.094 (0.106)   Loss 0.5027 (0.3324)   Prec@1 84.375 (85.988)   Prec@5 84.375 (85.988)   [2018-04-01 07:12:59]
  Epoch: [218][1400/1475]   Time 0.147 (0.139)   Data 0.109 (0.106)   Loss 0.5082 (0.3323)   Prec@1 78.125 (85.952)   Prec@5 78.125 (85.952)   [2018-04-01 07:13:26]
  **Train** Prec@1 85.922 Prec@5 85.922 Error@1 14.078
  **VAL** Prec@1 88.338 Prec@5 88.338 Error@1 11.662

==>>[2018-04-01 07:13:54] [Epoch=219/250] [Need: 01:55:23] [learning_rate=0.0002] [Best : Accuracy=88.45, Error=11.55]

==>>Epoch=[219/250]], [2018-04-01 07:13:54], LR=[0.002], Batch=[32] [Model=SimpleNet]
  Epoch: [219][000/1475]   Time 0.145 (0.145)   Data 0.114 (0.114)   Loss 0.4056 (0.4056)   Prec@1 75.000 (75.000)   Prec@5 75.000 (75.000)   [2018-04-01 07:13:55]
  Epoch: [219][200/1475]   Time 0.145 (0.139)   Data 0.114 (0.105)   Loss 0.2270 (0.3277)   Prec@1 90.625 (86.023)   Prec@5 90.625 (86.023)   [2018-04-01 07:14:22]
  Epoch: [219][400/1475]   Time 0.141 (0.139)   Data 0.110 (0.105)   Loss 0.2079 (0.3315)   Prec@1 96.875 (85.871)   Prec@5 96.875 (85.871)   [2018-04-01 07:14:50]
  Epoch: [219][600/1475]   Time 0.143 (0.139)   Data 0.097 (0.105)   Loss 0.3283 (0.3314)   Prec@1 87.500 (86.039)   Prec@5 87.500 (86.039)   [2018-04-01 07:15:18]
  Epoch: [219][800/1475]   Time 0.136 (0.139)   Data 0.090 (0.105)   Loss 0.2464 (0.3331)   Prec@1 96.875 (85.982)   Prec@5 96.875 (85.982)   [2018-04-01 07:15:45]
  Epoch: [219][1000/1475]   Time 0.135 (0.139)   Data 0.104 (0.105)   Loss 0.2884 (0.3316)   Prec@1 87.500 (86.017)   Prec@5 87.500 (86.017)   [2018-04-01 07:16:13]
  Epoch: [219][1200/1475]   Time 0.153 (0.139)   Data 0.109 (0.105)   Loss 0.3461 (0.3310)   Prec@1 84.375 (86.027)   Prec@5 84.375 (86.027)   [2018-04-01 07:16:41]
  Epoch: [219][1400/1475]   Time 0.141 (0.139)   Data 0.109 (0.105)   Loss 0.3751 (0.3320)   Prec@1 81.250 (85.936)   Prec@5 81.250 (85.936)   [2018-04-01 07:17:09]
  **Train** Prec@1 85.926 Prec@5 85.926 Error@1 14.074
  **VAL** Prec@1 88.230 Prec@5 88.230 Error@1 11.770

==>>[2018-04-01 07:17:37] [Epoch=220/250] [Need: 01:51:39] [learning_rate=0.0002] [Best : Accuracy=88.45, Error=11.55]

==>>Epoch=[220/250]], [2018-04-01 07:17:37], LR=[0.002], Batch=[32] [Model=SimpleNet]
  Epoch: [220][000/1475]   Time 0.136 (0.136)   Data 0.089 (0.089)   Loss 0.1509 (0.1509)   Prec@1 96.875 (96.875)   Prec@5 96.875 (96.875)   [2018-04-01 07:17:37]
  Epoch: [220][200/1475]   Time 0.142 (0.139)   Data 0.108 (0.105)   Loss 0.2133 (0.3256)   Prec@1 93.750 (86.272)   Prec@5 93.750 (86.272)   [2018-04-01 07:18:05]
  Epoch: [220][400/1475]   Time 0.141 (0.139)   Data 0.109 (0.105)   Loss 0.2522 (0.3264)   Prec@1 90.625 (86.238)   Prec@5 90.625 (86.238)   [2018-04-01 07:18:32]
  Epoch: [220][600/1475]   Time 0.136 (0.139)   Data 0.105 (0.105)   Loss 0.4898 (0.3271)   Prec@1 78.125 (86.231)   Prec@5 78.125 (86.231)   [2018-04-01 07:19:00]
  Epoch: [220][800/1475]   Time 0.129 (0.139)   Data 0.098 (0.105)   Loss 0.3348 (0.3277)   Prec@1 84.375 (86.291)   Prec@5 84.375 (86.291)   [2018-04-01 07:19:28]
  Epoch: [220][1000/1475]   Time 0.125 (0.139)   Data 0.094 (0.105)   Loss 0.2404 (0.3304)   Prec@1 93.750 (86.095)   Prec@5 93.750 (86.095)   [2018-04-01 07:19:56]
  Epoch: [220][1200/1475]   Time 0.141 (0.139)   Data 0.109 (0.105)   Loss 0.3491 (0.3314)   Prec@1 87.500 (86.137)   Prec@5 87.500 (86.137)   [2018-04-01 07:20:24]
  Epoch: [220][1400/1475]   Time 0.141 (0.139)   Data 0.109 (0.105)   Loss 0.6143 (0.3310)   Prec@1 81.250 (86.130)   Prec@5 81.250 (86.130)   [2018-04-01 07:20:51]
  **Train** Prec@1 86.115 Prec@5 86.115 Error@1 13.885
  **VAL** Prec@1 88.122 Prec@5 88.122 Error@1 11.878

==>>[2018-04-01 07:21:19] [Epoch=221/250] [Need: 01:47:56] [learning_rate=0.0002] [Best : Accuracy=88.45, Error=11.55]

==>>Epoch=[221/250]], [2018-04-01 07:21:19], LR=[0.002], Batch=[32] [Model=SimpleNet]
  Epoch: [221][000/1475]   Time 0.140 (0.140)   Data 0.106 (0.106)   Loss 0.1661 (0.1661)   Prec@1 96.875 (96.875)   Prec@5 96.875 (96.875)   [2018-04-01 07:21:20]
  Epoch: [221][200/1475]   Time 0.148 (0.139)   Data 0.100 (0.106)   Loss 0.4659 (0.3339)   Prec@1 84.375 (85.805)   Prec@5 84.375 (85.805)   [2018-04-01 07:21:47]
  Epoch: [221][400/1475]   Time 0.125 (0.139)   Data 0.094 (0.106)   Loss 0.2656 (0.3345)   Prec@1 90.625 (85.684)   Prec@5 90.625 (85.684)   [2018-04-01 07:22:15]
  Epoch: [221][600/1475]   Time 0.141 (0.139)   Data 0.109 (0.106)   Loss 0.3554 (0.3288)   Prec@1 84.375 (86.096)   Prec@5 84.375 (86.096)   [2018-04-01 07:22:43]
  Epoch: [221][800/1475]   Time 0.141 (0.139)   Data 0.109 (0.106)   Loss 0.3899 (0.3299)   Prec@1 81.250 (86.111)   Prec@5 81.250 (86.111)   [2018-04-01 07:23:11]
  Epoch: [221][1000/1475]   Time 0.141 (0.139)   Data 0.109 (0.106)   Loss 0.3479 (0.3293)   Prec@1 87.500 (86.126)   Prec@5 87.500 (86.126)   [2018-04-01 07:23:38]
  Epoch: [221][1200/1475]   Time 0.141 (0.139)   Data 0.094 (0.106)   Loss 0.4209 (0.3301)   Prec@1 81.250 (86.098)   Prec@5 81.250 (86.098)   [2018-04-01 07:24:06]
  Epoch: [221][1400/1475]   Time 0.125 (0.139)   Data 0.094 (0.105)   Loss 0.2151 (0.3300)   Prec@1 93.750 (86.128)   Prec@5 93.750 (86.128)   [2018-04-01 07:24:34]
  **Train** Prec@1 86.096 Prec@5 86.096 Error@1 13.904
  **VAL** Prec@1 88.302 Prec@5 88.302 Error@1 11.698

==>>[2018-04-01 07:25:02] [Epoch=222/250] [Need: 01:44:12] [learning_rate=0.0002] [Best : Accuracy=88.45, Error=11.55]

==>>Epoch=[222/250]], [2018-04-01 07:25:02], LR=[0.002], Batch=[32] [Model=SimpleNet]
  Epoch: [222][000/1475]   Time 0.136 (0.136)   Data 0.089 (0.089)   Loss 0.4059 (0.4059)   Prec@1 81.250 (81.250)   Prec@5 81.250 (81.250)   [2018-04-01 07:25:02]
  Epoch: [222][200/1475]   Time 0.141 (0.139)   Data 0.094 (0.105)   Loss 0.3406 (0.3317)   Prec@1 87.500 (86.039)   Prec@5 87.500 (86.039)   [2018-04-01 07:25:30]
  Epoch: [222][400/1475]   Time 0.141 (0.139)   Data 0.120 (0.105)   Loss 0.3114 (0.3337)   Prec@1 93.750 (85.988)   Prec@5 93.750 (85.988)   [2018-04-01 07:25:57]
  Epoch: [222][600/1475]   Time 0.144 (0.139)   Data 0.113 (0.105)   Loss 0.4621 (0.3332)   Prec@1 75.000 (86.054)   Prec@5 75.000 (86.054)   [2018-04-01 07:26:25]
  Epoch: [222][800/1475]   Time 0.129 (0.139)   Data 0.098 (0.105)   Loss 0.2686 (0.3339)   Prec@1 93.750 (85.990)   Prec@5 93.750 (85.990)   [2018-04-01 07:26:53]
  Epoch: [222][1000/1475]   Time 0.135 (0.139)   Data 0.104 (0.105)   Loss 0.2727 (0.3318)   Prec@1 84.375 (86.129)   Prec@5 84.375 (86.129)   [2018-04-01 07:27:21]
  Epoch: [222][1200/1475]   Time 0.141 (0.139)   Data 0.109 (0.105)   Loss 0.4730 (0.3310)   Prec@1 81.250 (86.069)   Prec@5 81.250 (86.069)   [2018-04-01 07:27:48]
  Epoch: [222][1400/1475]   Time 0.149 (0.139)   Data 0.117 (0.105)   Loss 0.3646 (0.3312)   Prec@1 81.250 (86.014)   Prec@5 81.250 (86.014)   [2018-04-01 07:28:16]
  **Train** Prec@1 85.949 Prec@5 85.949 Error@1 14.051
  **VAL** Prec@1 88.278 Prec@5 88.278 Error@1 11.722

==>>[2018-04-01 07:28:44] [Epoch=223/250] [Need: 01:40:29] [learning_rate=0.0002] [Best : Accuracy=88.45, Error=11.55]

==>>Epoch=[223/250]], [2018-04-01 07:28:44], LR=[0.002], Batch=[32] [Model=SimpleNet]
  Epoch: [223][000/1475]   Time 0.134 (0.134)   Data 0.103 (0.103)   Loss 0.3069 (0.3069)   Prec@1 90.625 (90.625)   Prec@5 90.625 (90.625)   [2018-04-01 07:28:44]
  Epoch: [223][200/1475]   Time 0.139 (0.138)   Data 0.108 (0.106)   Loss 0.2306 (0.3276)   Prec@1 87.500 (86.365)   Prec@5 87.500 (86.365)   [2018-04-01 07:29:12]
  Epoch: [223][400/1475]   Time 0.141 (0.139)   Data 0.109 (0.106)   Loss 0.2774 (0.3320)   Prec@1 87.500 (86.074)   Prec@5 87.500 (86.074)   [2018-04-01 07:29:40]
  Epoch: [223][600/1475]   Time 0.141 (0.139)   Data 0.109 (0.106)   Loss 0.3395 (0.3322)   Prec@1 84.375 (86.054)   Prec@5 84.375 (86.054)   [2018-04-01 07:30:08]
  Epoch: [223][800/1475]   Time 0.145 (0.139)   Data 0.114 (0.106)   Loss 0.3278 (0.3285)   Prec@1 87.500 (86.158)   Prec@5 87.500 (86.158)   [2018-04-01 07:30:35]
  Epoch: [223][1000/1475]   Time 0.150 (0.139)   Data 0.114 (0.106)   Loss 0.2861 (0.3313)   Prec@1 90.625 (85.942)   Prec@5 90.625 (85.942)   [2018-04-01 07:31:03]
  Epoch: [223][1200/1475]   Time 0.125 (0.139)   Data 0.094 (0.105)   Loss 0.3324 (0.3309)   Prec@1 78.125 (85.934)   Prec@5 78.125 (85.934)   [2018-04-01 07:31:31]
  Epoch: [223][1400/1475]   Time 0.141 (0.139)   Data 0.109 (0.105)   Loss 0.4263 (0.3316)   Prec@1 84.375 (85.874)   Prec@5 84.375 (85.874)   [2018-04-01 07:31:59]
  **Train** Prec@1 85.873 Prec@5 85.873 Error@1 14.127
  **VAL** Prec@1 88.230 Prec@5 88.230 Error@1 11.770

==>>[2018-04-01 07:32:27] [Epoch=224/250] [Need: 01:36:46] [learning_rate=0.0002] [Best : Accuracy=88.45, Error=11.55]

==>>Epoch=[224/250]], [2018-04-01 07:32:27], LR=[0.002], Batch=[32] [Model=SimpleNet]
  Epoch: [224][000/1475]   Time 0.145 (0.145)   Data 0.109 (0.109)   Loss 0.1331 (0.1331)   Prec@1 93.750 (93.750)   Prec@5 93.750 (93.750)   [2018-04-01 07:32:27]
  Epoch: [224][200/1475]   Time 0.127 (0.139)   Data 0.095 (0.106)   Loss 0.4939 (0.3354)   Prec@1 84.375 (85.961)   Prec@5 84.375 (85.961)   [2018-04-01 07:32:55]
  Epoch: [224][400/1475]   Time 0.143 (0.139)   Data 0.112 (0.106)   Loss 0.3293 (0.3342)   Prec@1 87.500 (85.887)   Prec@5 87.500 (85.887)   [2018-04-01 07:33:22]
  Epoch: [224][600/1475]   Time 0.144 (0.139)   Data 0.111 (0.106)   Loss 0.3078 (0.3364)   Prec@1 87.500 (85.659)   Prec@5 87.500 (85.659)   [2018-04-01 07:33:50]
  Epoch: [224][800/1475]   Time 0.141 (0.139)   Data 0.094 (0.106)   Loss 0.3270 (0.3347)   Prec@1 87.500 (85.932)   Prec@5 87.500 (85.932)   [2018-04-01 07:34:18]
  Epoch: [224][1000/1475]   Time 0.141 (0.139)   Data 0.109 (0.106)   Loss 0.2314 (0.3341)   Prec@1 87.500 (85.983)   Prec@5 87.500 (85.983)   [2018-04-01 07:34:46]
  Epoch: [224][1200/1475]   Time 0.144 (0.139)   Data 0.109 (0.106)   Loss 0.3984 (0.3345)   Prec@1 84.375 (85.928)   Prec@5 84.375 (85.928)   [2018-04-01 07:35:13]
  Epoch: [224][1400/1475]   Time 0.140 (0.139)   Data 0.107 (0.106)   Loss 0.2530 (0.3341)   Prec@1 90.625 (85.930)   Prec@5 90.625 (85.930)   [2018-04-01 07:35:41]
  **Train** Prec@1 85.939 Prec@5 85.939 Error@1 14.061
  **VAL** Prec@1 88.158 Prec@5 88.158 Error@1 11.842

==>>[2018-04-01 07:36:10] [Epoch=225/250] [Need: 01:33:02] [learning_rate=0.0000] [Best : Accuracy=88.45, Error=11.55]

==>>Epoch=[225/250]], [2018-04-01 07:36:10], LR=[0.002], Batch=[32] [Model=SimpleNet]
  Epoch: [225][000/1475]   Time 0.141 (0.141)   Data 0.109 (0.109)   Loss 0.3288 (0.3288)   Prec@1 84.375 (84.375)   Prec@5 84.375 (84.375)   [2018-04-01 07:36:10]
  Epoch: [225][200/1475]   Time 0.128 (0.139)   Data 0.097 (0.105)   Loss 0.4404 (0.3369)   Prec@1 81.250 (85.712)   Prec@5 81.250 (85.712)   [2018-04-01 07:36:37]
  Epoch: [225][400/1475]   Time 0.149 (0.139)   Data 0.118 (0.105)   Loss 0.4318 (0.3371)   Prec@1 75.000 (85.676)   Prec@5 75.000 (85.676)   [2018-04-01 07:37:05]
  Epoch: [225][600/1475]   Time 0.141 (0.139)   Data 0.109 (0.106)   Loss 0.4974 (0.3356)   Prec@1 75.000 (85.691)   Prec@5 75.000 (85.691)   [2018-04-01 07:37:33]
  Epoch: [225][800/1475]   Time 0.141 (0.139)   Data 0.109 (0.106)   Loss 0.3831 (0.3365)   Prec@1 78.125 (85.709)   Prec@5 78.125 (85.709)   [2018-04-01 07:38:01]
  Epoch: [225][1000/1475]   Time 0.139 (0.139)   Data 0.105 (0.105)   Loss 0.2733 (0.3347)   Prec@1 87.500 (85.805)   Prec@5 87.500 (85.805)   [2018-04-01 07:38:28]
  Epoch: [225][1200/1475]   Time 0.124 (0.139)   Data 0.093 (0.105)   Loss 0.3370 (0.3354)   Prec@1 81.250 (85.790)   Prec@5 81.250 (85.790)   [2018-04-01 07:38:56]
  Epoch: [225][1400/1475]   Time 0.125 (0.139)   Data 0.094 (0.105)   Loss 0.2454 (0.3321)   Prec@1 87.500 (86.010)   Prec@5 87.500 (86.010)   [2018-04-01 07:39:24]
  **Train** Prec@1 86.053 Prec@5 86.053 Error@1 13.947
  **VAL** Prec@1 88.182 Prec@5 88.182 Error@1 11.818

==>>[2018-04-01 07:39:52] [Epoch=226/250] [Need: 01:29:19] [learning_rate=0.0000] [Best : Accuracy=88.45, Error=11.55]

==>>Epoch=[226/250]], [2018-04-01 07:39:52], LR=[0.002], Batch=[32] [Model=SimpleNet]
  Epoch: [226][000/1475]   Time 0.141 (0.141)   Data 0.109 (0.109)   Loss 0.2277 (0.2277)   Prec@1 87.500 (87.500)   Prec@5 87.500 (87.500)   [2018-04-01 07:39:52]
  Epoch: [226][200/1475]   Time 0.141 (0.139)   Data 0.114 (0.106)   Loss 0.2366 (0.3333)   Prec@1 93.750 (85.619)   Prec@5 93.750 (85.619)   [2018-04-01 07:40:20]
  Epoch: [226][400/1475]   Time 0.128 (0.139)   Data 0.105 (0.106)   Loss 0.4171 (0.3327)   Prec@1 78.125 (85.700)   Prec@5 78.125 (85.700)   [2018-04-01 07:40:48]
  Epoch: [226][600/1475]   Time 0.141 (0.139)   Data 0.094 (0.105)   Loss 0.2013 (0.3328)   Prec@1 96.875 (85.826)   Prec@5 96.875 (85.826)   [2018-04-01 07:41:16]
  Epoch: [226][800/1475]   Time 0.141 (0.139)   Data 0.094 (0.106)   Loss 0.3606 (0.3328)   Prec@1 87.500 (85.897)   Prec@5 87.500 (85.897)   [2018-04-01 07:41:43]
  Epoch: [226][1000/1475]   Time 0.141 (0.139)   Data 0.109 (0.106)   Loss 0.2778 (0.3307)   Prec@1 87.500 (85.955)   Prec@5 87.500 (85.955)   [2018-04-01 07:42:11]
  Epoch: [226][1200/1475]   Time 0.141 (0.139)   Data 0.109 (0.106)   Loss 0.4041 (0.3325)   Prec@1 90.625 (85.879)   Prec@5 90.625 (85.879)   [2018-04-01 07:42:39]
  Epoch: [226][1400/1475]   Time 0.125 (0.139)   Data 0.094 (0.106)   Loss 0.4556 (0.3329)   Prec@1 78.125 (85.927)   Prec@5 78.125 (85.927)   [2018-04-01 07:43:06]
  **Train** Prec@1 85.975 Prec@5 85.975 Error@1 14.025
  **VAL** Prec@1 88.338 Prec@5 88.338 Error@1 11.662

==>>[2018-04-01 07:43:35] [Epoch=227/250] [Need: 01:25:35] [learning_rate=0.0000] [Best : Accuracy=88.45, Error=11.55]

==>>Epoch=[227/250]], [2018-04-01 07:43:35], LR=[0.002], Batch=[32] [Model=SimpleNet]
  Epoch: [227][000/1475]   Time 0.141 (0.141)   Data 0.109 (0.109)   Loss 0.1461 (0.1461)   Prec@1 100.000 (100.000)   Prec@5 100.000 (100.000)   [2018-04-01 07:43:35]
  Epoch: [227][200/1475]   Time 0.125 (0.139)   Data 0.105 (0.106)   Loss 0.2478 (0.3242)   Prec@1 84.375 (86.178)   Prec@5 84.375 (86.178)   [2018-04-01 07:44:03]
  Epoch: [227][400/1475]   Time 0.141 (0.139)   Data 0.109 (0.106)   Loss 0.2843 (0.3344)   Prec@1 93.750 (85.708)   Prec@5 93.750 (85.708)   [2018-04-01 07:44:30]
  Epoch: [227][600/1475]   Time 0.136 (0.139)   Data 0.103 (0.106)   Loss 0.2718 (0.3326)   Prec@1 93.750 (86.034)   Prec@5 93.750 (86.034)   [2018-04-01 07:44:58]
  Epoch: [227][800/1475]   Time 0.144 (0.139)   Data 0.118 (0.106)   Loss 0.3760 (0.3336)   Prec@1 87.500 (86.021)   Prec@5 87.500 (86.021)   [2018-04-01 07:45:26]
  Epoch: [227][1000/1475]   Time 0.137 (0.139)   Data 0.106 (0.105)   Loss 0.4289 (0.3348)   Prec@1 81.250 (85.917)   Prec@5 81.250 (85.917)   [2018-04-01 07:45:54]
  Epoch: [227][1200/1475]   Time 0.134 (0.139)   Data 0.103 (0.106)   Loss 0.2439 (0.3345)   Prec@1 87.500 (85.954)   Prec@5 87.500 (85.954)   [2018-04-01 07:46:22]
  Epoch: [227][1400/1475]   Time 0.141 (0.139)   Data 0.094 (0.106)   Loss 0.3499 (0.3325)   Prec@1 84.375 (85.985)   Prec@5 84.375 (85.985)   [2018-04-01 07:46:49]
  **Train** Prec@1 86.040 Prec@5 86.040 Error@1 13.960
  **VAL** Prec@1 88.266 Prec@5 88.266 Error@1 11.734

==>>[2018-04-01 07:47:18] [Epoch=228/250] [Need: 01:21:52] [learning_rate=0.0000] [Best : Accuracy=88.45, Error=11.55]

==>>Epoch=[228/250]], [2018-04-01 07:47:18], LR=[0.002], Batch=[32] [Model=SimpleNet]
  Epoch: [228][000/1475]   Time 0.134 (0.134)   Data 0.098 (0.098)   Loss 0.2552 (0.2552)   Prec@1 87.500 (87.500)   Prec@5 87.500 (87.500)   [2018-04-01 07:47:18]
  Epoch: [228][200/1475]   Time 0.141 (0.139)   Data 0.109 (0.106)   Loss 0.2590 (0.3288)   Prec@1 87.500 (85.992)   Prec@5 87.500 (85.992)   [2018-04-01 07:47:45]
  Epoch: [228][400/1475]   Time 0.140 (0.139)   Data 0.098 (0.105)   Loss 0.2568 (0.3266)   Prec@1 90.625 (86.386)   Prec@5 90.625 (86.386)   [2018-04-01 07:48:13]
  Epoch: [228][600/1475]   Time 0.144 (0.139)   Data 0.113 (0.105)   Loss 0.2775 (0.3286)   Prec@1 84.375 (86.309)   Prec@5 84.375 (86.309)   [2018-04-01 07:48:41]
  Epoch: [228][800/1475]   Time 0.139 (0.139)   Data 0.108 (0.106)   Loss 0.1878 (0.3313)   Prec@1 90.625 (86.103)   Prec@5 90.625 (86.103)   [2018-04-01 07:49:09]
  Epoch: [228][1000/1475]   Time 0.141 (0.139)   Data 0.109 (0.105)   Loss 0.2059 (0.3309)   Prec@1 93.750 (86.126)   Prec@5 93.750 (86.126)   [2018-04-01 07:49:36]
  Epoch: [228][1200/1475]   Time 0.144 (0.139)   Data 0.097 (0.105)   Loss 0.3615 (0.3334)   Prec@1 84.375 (86.012)   Prec@5 84.375 (86.012)   [2018-04-01 07:50:04]
  Epoch: [228][1400/1475]   Time 0.136 (0.139)   Data 0.105 (0.105)   Loss 0.2725 (0.3327)   Prec@1 93.750 (86.023)   Prec@5 93.750 (86.023)   [2018-04-01 07:50:32]
  **Train** Prec@1 86.021 Prec@5 86.021 Error@1 13.979
  **VAL** Prec@1 88.338 Prec@5 88.338 Error@1 11.662

==>>[2018-04-01 07:51:00] [Epoch=229/250] [Need: 01:18:09] [learning_rate=0.0000] [Best : Accuracy=88.45, Error=11.55]

==>>Epoch=[229/250]], [2018-04-01 07:51:00], LR=[0.002], Batch=[32] [Model=SimpleNet]
  Epoch: [229][000/1475]   Time 0.127 (0.127)   Data 0.096 (0.096)   Loss 0.2646 (0.2646)   Prec@1 90.625 (90.625)   Prec@5 90.625 (90.625)   [2018-04-01 07:51:00]
  Epoch: [229][200/1475]   Time 0.141 (0.139)   Data 0.109 (0.106)   Loss 0.3194 (0.3263)   Prec@1 84.375 (86.101)   Prec@5 84.375 (86.101)   [2018-04-01 07:51:28]
  Epoch: [229][400/1475]   Time 0.142 (0.139)   Data 0.111 (0.106)   Loss 0.4738 (0.3344)   Prec@1 78.125 (85.762)   Prec@5 78.125 (85.762)   [2018-04-01 07:51:56]
  Epoch: [229][600/1475]   Time 0.149 (0.139)   Data 0.102 (0.106)   Loss 0.2541 (0.3404)   Prec@1 90.625 (85.488)   Prec@5 90.625 (85.488)   [2018-04-01 07:52:23]
  Epoch: [229][800/1475]   Time 0.144 (0.139)   Data 0.113 (0.106)   Loss 0.4580 (0.3368)   Prec@1 78.125 (85.748)   Prec@5 78.125 (85.748)   [2018-04-01 07:52:51]
  Epoch: [229][1000/1475]   Time 0.125 (0.139)   Data 0.094 (0.106)   Loss 0.3270 (0.3347)   Prec@1 90.625 (85.839)   Prec@5 90.625 (85.839)   [2018-04-01 07:53:19]
  Epoch: [229][1200/1475]   Time 0.141 (0.139)   Data 0.109 (0.106)   Loss 0.2080 (0.3342)   Prec@1 93.750 (85.856)   Prec@5 93.750 (85.856)   [2018-04-01 07:53:47]
  Epoch: [229][1400/1475]   Time 0.139 (0.139)   Data 0.120 (0.106)   Loss 0.3052 (0.3335)   Prec@1 84.375 (85.856)   Prec@5 84.375 (85.856)   [2018-04-01 07:54:14]
  **Train** Prec@1 85.888 Prec@5 85.888 Error@1 14.112
  **VAL** Prec@1 88.422 Prec@5 88.422 Error@1 11.578

==>>[2018-04-01 07:54:43] [Epoch=230/250] [Need: 01:14:25] [learning_rate=0.0000] [Best : Accuracy=88.45, Error=11.55]

==>>Epoch=[230/250]], [2018-04-01 07:54:43], LR=[0.002], Batch=[32] [Model=SimpleNet]
  Epoch: [230][000/1475]   Time 0.125 (0.125)   Data 0.094 (0.094)   Loss 0.3871 (0.3871)   Prec@1 78.125 (78.125)   Prec@5 78.125 (78.125)   [2018-04-01 07:54:43]
  Epoch: [230][200/1475]   Time 0.141 (0.139)   Data 0.109 (0.106)   Loss 0.2421 (0.3355)   Prec@1 90.625 (85.821)   Prec@5 90.625 (85.821)   [2018-04-01 07:55:11]
  Epoch: [230][400/1475]   Time 0.141 (0.139)   Data 0.116 (0.105)   Loss 0.1561 (0.3355)   Prec@1 96.875 (85.778)   Prec@5 96.875 (85.778)   [2018-04-01 07:55:38]
  Epoch: [230][600/1475]   Time 0.130 (0.139)   Data 0.098 (0.105)   Loss 0.2049 (0.3317)   Prec@1 93.750 (85.883)   Prec@5 93.750 (85.883)   [2018-04-01 07:56:06]
  Epoch: [230][800/1475]   Time 0.128 (0.139)   Data 0.109 (0.105)   Loss 0.2222 (0.3303)   Prec@1 93.750 (85.908)   Prec@5 93.750 (85.908)   [2018-04-01 07:56:34]
  Epoch: [230][1000/1475]   Time 0.140 (0.139)   Data 0.094 (0.105)   Loss 0.3115 (0.3298)   Prec@1 90.625 (85.992)   Prec@5 90.625 (85.992)   [2018-04-01 07:57:01]
  Epoch: [230][1200/1475]   Time 0.144 (0.139)   Data 0.113 (0.105)   Loss 0.3258 (0.3304)   Prec@1 78.125 (86.014)   Prec@5 78.125 (86.014)   [2018-04-01 07:57:29]
  Epoch: [230][1400/1475]   Time 0.145 (0.139)   Data 0.114 (0.105)   Loss 0.3867 (0.3302)   Prec@1 81.250 (86.003)   Prec@5 81.250 (86.003)   [2018-04-01 07:57:57]
  **Train** Prec@1 85.987 Prec@5 85.987 Error@1 14.013
  **VAL** Prec@1 88.302 Prec@5 88.302 Error@1 11.698

==>>[2018-04-01 07:58:25] [Epoch=231/250] [Need: 01:10:42] [learning_rate=0.0000] [Best : Accuracy=88.45, Error=11.55]

==>>Epoch=[231/250]], [2018-04-01 07:58:25], LR=[0.002], Batch=[32] [Model=SimpleNet]
  Epoch: [231][000/1475]   Time 0.141 (0.141)   Data 0.109 (0.109)   Loss 0.5417 (0.5417)   Prec@1 81.250 (81.250)   Prec@5 81.250 (81.250)   [2018-04-01 07:58:25]
  Epoch: [231][200/1475]   Time 0.133 (0.139)   Data 0.101 (0.106)   Loss 0.2776 (0.3292)   Prec@1 84.375 (85.868)   Prec@5 84.375 (85.868)   [2018-04-01 07:58:53]
  Epoch: [231][400/1475]   Time 0.141 (0.139)   Data 0.109 (0.106)   Loss 0.3359 (0.3297)   Prec@1 90.625 (86.019)   Prec@5 90.625 (86.019)   [2018-04-01 07:59:21]
  Epoch: [231][600/1475]   Time 0.148 (0.139)   Data 0.109 (0.106)   Loss 0.2043 (0.3306)   Prec@1 90.625 (85.997)   Prec@5 90.625 (85.997)   [2018-04-01 07:59:49]
  Epoch: [231][800/1475]   Time 0.126 (0.139)   Data 0.095 (0.105)   Loss 0.1540 (0.3330)   Prec@1 96.875 (85.975)   Prec@5 96.875 (85.975)   [2018-04-01 08:00:16]
  Epoch: [231][1000/1475]   Time 0.133 (0.139)   Data 0.100 (0.105)   Loss 0.2353 (0.3305)   Prec@1 87.500 (86.176)   Prec@5 87.500 (86.176)   [2018-04-01 08:00:44]
  Epoch: [231][1200/1475]   Time 0.141 (0.139)   Data 0.109 (0.105)   Loss 0.2996 (0.3309)   Prec@1 87.500 (86.176)   Prec@5 87.500 (86.176)   [2018-04-01 08:01:12]
  Epoch: [231][1400/1475]   Time 0.149 (0.139)   Data 0.116 (0.105)   Loss 0.2404 (0.3318)   Prec@1 93.750 (86.026)   Prec@5 93.750 (86.026)   [2018-04-01 08:01:40]
  **Train** Prec@1 86.021 Prec@5 86.021 Error@1 13.979
  **VAL** Prec@1 88.362 Prec@5 88.362 Error@1 11.638

==>>[2018-04-01 08:02:08] [Epoch=232/250] [Need: 01:06:59] [learning_rate=0.0000] [Best : Accuracy=88.45, Error=11.55]

==>>Epoch=[232/250]], [2018-04-01 08:02:08], LR=[0.002], Batch=[32] [Model=SimpleNet]
  Epoch: [232][000/1475]   Time 0.143 (0.143)   Data 0.094 (0.094)   Loss 0.2174 (0.2174)   Prec@1 90.625 (90.625)   Prec@5 90.625 (90.625)   [2018-04-01 08:02:08]
  Epoch: [232][200/1475]   Time 0.135 (0.139)   Data 0.105 (0.105)   Loss 0.4273 (0.3311)   Prec@1 78.125 (85.619)   Prec@5 78.125 (85.619)   [2018-04-01 08:02:36]
  Epoch: [232][400/1475]   Time 0.141 (0.139)   Data 0.094 (0.106)   Loss 0.3463 (0.3293)   Prec@1 81.250 (86.019)   Prec@5 81.250 (86.019)   [2018-04-01 08:03:04]
  Epoch: [232][600/1475]   Time 0.141 (0.139)   Data 0.109 (0.105)   Loss 0.3397 (0.3320)   Prec@1 87.500 (85.961)   Prec@5 87.500 (85.961)   [2018-04-01 08:03:31]
  Epoch: [232][800/1475]   Time 0.141 (0.139)   Data 0.109 (0.105)   Loss 0.5425 (0.3320)   Prec@1 81.250 (85.982)   Prec@5 81.250 (85.982)   [2018-04-01 08:03:59]
  Epoch: [232][1000/1475]   Time 0.138 (0.139)   Data 0.094 (0.105)   Loss 0.3134 (0.3320)   Prec@1 87.500 (86.036)   Prec@5 87.500 (86.036)   [2018-04-01 08:04:27]
  Epoch: [232][1200/1475]   Time 0.148 (0.139)   Data 0.117 (0.105)   Loss 0.4914 (0.3329)   Prec@1 71.875 (86.017)   Prec@5 71.875 (86.017)   [2018-04-01 08:04:55]
  Epoch: [232][1400/1475]   Time 0.128 (0.139)   Data 0.097 (0.105)   Loss 0.4951 (0.3322)   Prec@1 81.250 (86.021)   Prec@5 81.250 (86.021)   [2018-04-01 08:05:22]
  **Train** Prec@1 86.055 Prec@5 86.055 Error@1 13.945
  **VAL** Prec@1 88.254 Prec@5 88.254 Error@1 11.746

==>>[2018-04-01 08:05:50] [Epoch=233/250] [Need: 01:03:15] [learning_rate=0.0000] [Best : Accuracy=88.45, Error=11.55]

==>>Epoch=[233/250]], [2018-04-01 08:05:50], LR=[0.002], Batch=[32] [Model=SimpleNet]
  Epoch: [233][000/1475]   Time 0.125 (0.125)   Data 0.094 (0.094)   Loss 0.5221 (0.5221)   Prec@1 78.125 (78.125)   Prec@5 78.125 (78.125)   [2018-04-01 08:05:51]
  Epoch: [233][200/1475]   Time 0.141 (0.139)   Data 0.109 (0.106)   Loss 0.3294 (0.3237)   Prec@1 90.625 (85.992)   Prec@5 90.625 (85.992)   [2018-04-01 08:06:18]
  Epoch: [233][400/1475]   Time 0.143 (0.139)   Data 0.097 (0.105)   Loss 0.2942 (0.3277)   Prec@1 84.375 (86.121)   Prec@5 84.375 (86.121)   [2018-04-01 08:06:46]
  Epoch: [233][600/1475]   Time 0.133 (0.139)   Data 0.114 (0.105)   Loss 0.3132 (0.3243)   Prec@1 87.500 (86.242)   Prec@5 87.500 (86.242)   [2018-04-01 08:07:14]
  Epoch: [233][800/1475]   Time 0.141 (0.139)   Data 0.109 (0.105)   Loss 0.4335 (0.3282)   Prec@1 78.125 (86.103)   Prec@5 78.125 (86.103)   [2018-04-01 08:07:42]
  Epoch: [233][1000/1475]   Time 0.141 (0.139)   Data 0.109 (0.105)   Loss 0.3197 (0.3291)   Prec@1 87.500 (86.108)   Prec@5 87.500 (86.108)   [2018-04-01 08:08:09]
  Epoch: [233][1200/1475]   Time 0.145 (0.139)   Data 0.113 (0.105)   Loss 0.3302 (0.3295)   Prec@1 93.750 (86.061)   Prec@5 93.750 (86.061)   [2018-04-01 08:08:37]
  Epoch: [233][1400/1475]   Time 0.141 (0.139)   Data 0.109 (0.105)   Loss 0.2538 (0.3298)   Prec@1 87.500 (86.081)   Prec@5 87.500 (86.081)   [2018-04-01 08:09:05]
  **Train** Prec@1 86.125 Prec@5 86.125 Error@1 13.875
  **VAL** Prec@1 88.230 Prec@5 88.230 Error@1 11.770

==>>[2018-04-01 08:09:33] [Epoch=234/250] [Need: 00:59:32] [learning_rate=0.0000] [Best : Accuracy=88.45, Error=11.55]

==>>Epoch=[234/250]], [2018-04-01 08:09:33], LR=[0.002], Batch=[32] [Model=SimpleNet]
  Epoch: [234][000/1475]   Time 0.133 (0.133)   Data 0.102 (0.102)   Loss 0.2365 (0.2365)   Prec@1 90.625 (90.625)   Prec@5 90.625 (90.625)   [2018-04-01 08:09:33]
  Epoch: [234][200/1475]   Time 0.146 (0.139)   Data 0.115 (0.106)   Loss 0.1917 (0.3353)   Prec@1 96.875 (85.308)   Prec@5 96.875 (85.308)   [2018-04-01 08:10:01]
  Epoch: [234][400/1475]   Time 0.145 (0.139)   Data 0.109 (0.106)   Loss 0.3143 (0.3318)   Prec@1 84.375 (85.840)   Prec@5 84.375 (85.840)   [2018-04-01 08:10:29]
  Epoch: [234][600/1475]   Time 0.128 (0.139)   Data 0.106 (0.106)   Loss 0.2773 (0.3351)   Prec@1 90.625 (85.717)   Prec@5 90.625 (85.717)   [2018-04-01 08:10:57]
  Epoch: [234][800/1475]   Time 0.141 (0.139)   Data 0.094 (0.106)   Loss 0.2687 (0.3327)   Prec@1 90.625 (85.838)   Prec@5 90.625 (85.838)   [2018-04-01 08:11:24]
  Epoch: [234][1000/1475]   Time 0.138 (0.139)   Data 0.104 (0.106)   Loss 0.2128 (0.3320)   Prec@1 93.750 (85.852)   Prec@5 93.750 (85.852)   [2018-04-01 08:11:52]
  Epoch: [234][1200/1475]   Time 0.144 (0.139)   Data 0.115 (0.106)   Loss 0.2507 (0.3316)   Prec@1 84.375 (85.884)   Prec@5 84.375 (85.884)   [2018-04-01 08:12:20]
  Epoch: [234][1400/1475]   Time 0.130 (0.139)   Data 0.099 (0.106)   Loss 0.1964 (0.3309)   Prec@1 90.625 (85.972)   Prec@5 90.625 (85.972)   [2018-04-01 08:12:48]
  **Train** Prec@1 85.998 Prec@5 85.998 Error@1 14.002
  **VAL** Prec@1 88.398 Prec@5 88.398 Error@1 11.602

==>>[2018-04-01 08:13:16] [Epoch=235/250] [Need: 00:55:49] [learning_rate=0.0000] [Best : Accuracy=88.45, Error=11.55]

==>>Epoch=[235/250]], [2018-04-01 08:13:16], LR=[0.002], Batch=[32] [Model=SimpleNet]
  Epoch: [235][000/1475]   Time 0.136 (0.136)   Data 0.089 (0.089)   Loss 0.4322 (0.4322)   Prec@1 84.375 (84.375)   Prec@5 84.375 (84.375)   [2018-04-01 08:13:16]
  Epoch: [235][200/1475]   Time 0.141 (0.139)   Data 0.094 (0.106)   Loss 0.2211 (0.3384)   Prec@1 93.750 (85.370)   Prec@5 93.750 (85.370)   [2018-04-01 08:13:44]
  Epoch: [235][400/1475]   Time 0.152 (0.139)   Data 0.109 (0.106)   Loss 0.4192 (0.3416)   Prec@1 81.250 (85.224)   Prec@5 81.250 (85.224)   [2018-04-01 08:14:12]
  Epoch: [235][600/1475]   Time 0.138 (0.139)   Data 0.105 (0.105)   Loss 0.5345 (0.3324)   Prec@1 75.000 (85.717)   Prec@5 75.000 (85.717)   [2018-04-01 08:14:39]
  Epoch: [235][800/1475]   Time 0.141 (0.139)   Data 0.108 (0.106)   Loss 0.2213 (0.3329)   Prec@1 90.625 (85.721)   Prec@5 90.625 (85.721)   [2018-04-01 08:15:07]
  Epoch: [235][1000/1475]   Time 0.141 (0.139)   Data 0.109 (0.106)   Loss 0.3242 (0.3321)   Prec@1 84.375 (85.817)   Prec@5 84.375 (85.817)   [2018-04-01 08:15:35]
  Epoch: [235][1200/1475]   Time 0.141 (0.139)   Data 0.109 (0.106)   Loss 0.1834 (0.3300)   Prec@1 90.625 (85.944)   Prec@5 90.625 (85.944)   [2018-04-01 08:16:03]
  Epoch: [235][1400/1475]   Time 0.141 (0.139)   Data 0.109 (0.106)   Loss 0.3231 (0.3294)   Prec@1 84.375 (85.952)   Prec@5 84.375 (85.952)   [2018-04-01 08:16:30]
  **Train** Prec@1 85.947 Prec@5 85.947 Error@1 14.053
  **VAL** Prec@1 88.398 Prec@5 88.398 Error@1 11.602

==>>[2018-04-01 08:16:59] [Epoch=236/250] [Need: 00:52:05] [learning_rate=0.0000] [Best : Accuracy=88.45, Error=11.55]

==>>Epoch=[236/250]], [2018-04-01 08:16:59], LR=[0.002], Batch=[32] [Model=SimpleNet]
  Epoch: [236][000/1475]   Time 0.141 (0.141)   Data 0.094 (0.094)   Loss 0.2833 (0.2833)   Prec@1 84.375 (84.375)   Prec@5 84.375 (84.375)   [2018-04-01 08:16:59]
  Epoch: [236][200/1475]   Time 0.125 (0.139)   Data 0.094 (0.105)   Loss 0.3554 (0.3463)   Prec@1 87.500 (85.432)   Prec@5 87.500 (85.432)   [2018-04-01 08:17:27]
  Epoch: [236][400/1475]   Time 0.141 (0.139)   Data 0.094 (0.105)   Loss 0.3425 (0.3402)   Prec@1 84.375 (85.591)   Prec@5 84.375 (85.591)   [2018-04-01 08:17:54]
  Epoch: [236][600/1475]   Time 0.141 (0.139)   Data 0.109 (0.105)   Loss 0.4111 (0.3319)   Prec@1 78.125 (86.039)   Prec@5 78.125 (86.039)   [2018-04-01 08:18:22]
  Epoch: [236][800/1475]   Time 0.144 (0.139)   Data 0.113 (0.105)   Loss 0.2551 (0.3282)   Prec@1 90.625 (86.197)   Prec@5 90.625 (86.197)   [2018-04-01 08:18:50]
  Epoch: [236][1000/1475]   Time 0.138 (0.139)   Data 0.091 (0.105)   Loss 0.2463 (0.3303)   Prec@1 90.625 (86.117)   Prec@5 90.625 (86.117)   [2018-04-01 08:19:18]
  Epoch: [236][1200/1475]   Time 0.132 (0.139)   Data 0.101 (0.105)   Loss 0.2723 (0.3311)   Prec@1 81.250 (86.056)   Prec@5 81.250 (86.056)   [2018-04-01 08:19:45]
  Epoch: [236][1400/1475]   Time 0.141 (0.139)   Data 0.109 (0.105)   Loss 0.2339 (0.3311)   Prec@1 93.750 (85.950)   Prec@5 93.750 (85.950)   [2018-04-01 08:20:13]
  **Train** Prec@1 85.990 Prec@5 85.990 Error@1 14.010
  **VAL** Prec@1 88.422 Prec@5 88.422 Error@1 11.578

==>>[2018-04-01 08:20:42] [Epoch=237/250] [Need: 00:48:22] [learning_rate=0.0000] [Best : Accuracy=88.45, Error=11.55]

==>>Epoch=[237/250]], [2018-04-01 08:20:42], LR=[0.002], Batch=[32] [Model=SimpleNet]
  Epoch: [237][000/1475]   Time 0.141 (0.141)   Data 0.109 (0.109)   Loss 0.3073 (0.3073)   Prec@1 90.625 (90.625)   Prec@5 90.625 (90.625)   [2018-04-01 08:20:42]
  Epoch: [237][200/1475]   Time 0.128 (0.139)   Data 0.105 (0.106)   Loss 0.2432 (0.3381)   Prec@1 90.625 (85.836)   Prec@5 90.625 (85.836)   [2018-04-01 08:21:09]
  Epoch: [237][400/1475]   Time 0.141 (0.139)   Data 0.107 (0.106)   Loss 0.4184 (0.3373)   Prec@1 81.250 (85.910)   Prec@5 81.250 (85.910)   [2018-04-01 08:21:37]
  Epoch: [237][600/1475]   Time 0.141 (0.139)   Data 0.109 (0.106)   Loss 0.2130 (0.3370)   Prec@1 93.750 (85.857)   Prec@5 93.750 (85.857)   [2018-04-01 08:22:05]
  Epoch: [237][800/1475]   Time 0.125 (0.139)   Data 0.094 (0.106)   Loss 0.4821 (0.3349)   Prec@1 81.250 (85.908)   Prec@5 81.250 (85.908)   [2018-04-01 08:22:33]
  Epoch: [237][1000/1475]   Time 0.141 (0.139)   Data 0.109 (0.106)   Loss 0.3670 (0.3333)   Prec@1 84.375 (85.936)   Prec@5 84.375 (85.936)   [2018-04-01 08:23:00]
  Epoch: [237][1200/1475]   Time 0.144 (0.139)   Data 0.113 (0.106)   Loss 0.4994 (0.3314)   Prec@1 71.875 (85.986)   Prec@5 71.875 (85.986)   [2018-04-01 08:23:28]
  Epoch: [237][1400/1475]   Time 0.141 (0.139)   Data 0.094 (0.106)   Loss 0.2154 (0.3312)   Prec@1 90.625 (86.023)   Prec@5 90.625 (86.023)   [2018-04-01 08:23:56]
  **Train** Prec@1 86.045 Prec@5 86.045 Error@1 13.955
  **VAL** Prec@1 88.374 Prec@5 88.374 Error@1 11.626

==>>[2018-04-01 08:24:24] [Epoch=238/250] [Need: 00:44:39] [learning_rate=0.0000] [Best : Accuracy=88.45, Error=11.55]

==>>Epoch=[238/250]], [2018-04-01 08:24:24], LR=[0.002], Batch=[32] [Model=SimpleNet]
  Epoch: [238][000/1475]   Time 0.125 (0.125)   Data 0.094 (0.094)   Loss 0.5302 (0.5302)   Prec@1 81.250 (81.250)   Prec@5 81.250 (81.250)   [2018-04-01 08:24:24]
  Epoch: [238][200/1475]   Time 0.145 (0.139)   Data 0.114 (0.105)   Loss 0.4934 (0.3278)   Prec@1 78.125 (86.023)   Prec@5 78.125 (86.023)   [2018-04-01 08:24:52]
  Epoch: [238][400/1475]   Time 0.139 (0.139)   Data 0.106 (0.106)   Loss 0.4124 (0.3368)   Prec@1 84.375 (85.895)   Prec@5 84.375 (85.895)   [2018-04-01 08:25:20]
  Epoch: [238][600/1475]   Time 0.141 (0.139)   Data 0.109 (0.106)   Loss 0.3431 (0.3355)   Prec@1 87.500 (86.158)   Prec@5 87.500 (86.158)   [2018-04-01 08:25:47]
  Epoch: [238][800/1475]   Time 0.126 (0.139)   Data 0.095 (0.106)   Loss 0.3887 (0.3361)   Prec@1 84.375 (86.076)   Prec@5 84.375 (86.076)   [2018-04-01 08:26:15]
  Epoch: [238][1000/1475]   Time 0.141 (0.139)   Data 0.109 (0.106)   Loss 0.3626 (0.3344)   Prec@1 81.250 (86.061)   Prec@5 81.250 (86.061)   [2018-04-01 08:26:43]
  Epoch: [238][1200/1475]   Time 0.152 (0.139)   Data 0.113 (0.106)   Loss 0.4265 (0.3353)   Prec@1 84.375 (85.986)   Prec@5 84.375 (85.986)   [2018-04-01 08:27:11]
  Epoch: [238][1400/1475]   Time 0.136 (0.139)   Data 0.105 (0.106)   Loss 0.2218 (0.3332)   Prec@1 93.750 (86.012)   Prec@5 93.750 (86.012)   [2018-04-01 08:27:38]
  **Train** Prec@1 85.962 Prec@5 85.962 Error@1 14.038
  **VAL** Prec@1 88.386 Prec@5 88.386 Error@1 11.614

==>>[2018-04-01 08:28:07] [Epoch=239/250] [Need: 00:40:55] [learning_rate=0.0000] [Best : Accuracy=88.45, Error=11.55]

==>>Epoch=[239/250]], [2018-04-01 08:28:07], LR=[0.002], Batch=[32] [Model=SimpleNet]
  Epoch: [239][000/1475]   Time 0.145 (0.145)   Data 0.098 (0.098)   Loss 0.1604 (0.1604)   Prec@1 90.625 (90.625)   Prec@5 90.625 (90.625)   [2018-04-01 08:28:07]
  Epoch: [239][200/1475]   Time 0.128 (0.139)   Data 0.105 (0.106)   Loss 0.5083 (0.3337)   Prec@1 81.250 (85.868)   Prec@5 81.250 (85.868)   [2018-04-01 08:28:35]
  Epoch: [239][400/1475]   Time 0.141 (0.139)   Data 0.094 (0.106)   Loss 0.2439 (0.3370)   Prec@1 87.500 (85.902)   Prec@5 87.500 (85.902)   [2018-04-01 08:29:02]
  Epoch: [239][600/1475]   Time 0.145 (0.139)   Data 0.109 (0.105)   Loss 0.4184 (0.3314)   Prec@1 87.500 (86.148)   Prec@5 87.500 (86.148)   [2018-04-01 08:29:30]
  Epoch: [239][800/1475]   Time 0.141 (0.139)   Data 0.109 (0.105)   Loss 0.5303 (0.3324)   Prec@1 78.125 (86.131)   Prec@5 78.125 (86.131)   [2018-04-01 08:29:58]
  Epoch: [239][1000/1475]   Time 0.125 (0.139)   Data 0.094 (0.106)   Loss 0.4366 (0.3321)   Prec@1 84.375 (86.070)   Prec@5 84.375 (86.070)   [2018-04-01 08:30:26]
  Epoch: [239][1200/1475]   Time 0.141 (0.139)   Data 0.109 (0.106)   Loss 0.1888 (0.3302)   Prec@1 90.625 (86.155)   Prec@5 90.625 (86.155)   [2018-04-01 08:30:53]
  Epoch: [239][1400/1475]   Time 0.140 (0.139)   Data 0.098 (0.106)   Loss 0.3885 (0.3292)   Prec@1 81.250 (86.188)   Prec@5 81.250 (86.188)   [2018-04-01 08:31:21]
  **Train** Prec@1 86.178 Prec@5 86.178 Error@1 13.822
  **VAL** Prec@1 88.446 Prec@5 88.446 Error@1 11.554

==>>[2018-04-01 08:31:49] [Epoch=240/250] [Need: 00:37:12] [learning_rate=0.0000] [Best : Accuracy=88.45, Error=11.55]

==>>Epoch=[240/250]], [2018-04-01 08:31:49], LR=[0.002], Batch=[32] [Model=SimpleNet]
  Epoch: [240][000/1475]   Time 0.134 (0.134)   Data 0.103 (0.103)   Loss 0.2728 (0.2728)   Prec@1 90.625 (90.625)   Prec@5 90.625 (90.625)   [2018-04-01 08:31:49]
  Epoch: [240][200/1475]   Time 0.141 (0.139)   Data 0.109 (0.106)   Loss 0.2110 (0.3257)   Prec@1 93.750 (86.163)   Prec@5 93.750 (86.163)   [2018-04-01 08:32:17]
  Epoch: [240][400/1475]   Time 0.148 (0.139)   Data 0.113 (0.106)   Loss 0.3062 (0.3300)   Prec@1 87.500 (86.066)   Prec@5 87.500 (86.066)   [2018-04-01 08:32:45]
  Epoch: [240][600/1475]   Time 0.141 (0.139)   Data 0.109 (0.106)   Loss 0.4040 (0.3302)   Prec@1 84.375 (86.018)   Prec@5 84.375 (86.018)   [2018-04-01 08:33:13]
  Epoch: [240][800/1475]   Time 0.127 (0.139)   Data 0.095 (0.106)   Loss 0.2584 (0.3305)   Prec@1 93.750 (86.037)   Prec@5 93.750 (86.037)   [2018-04-01 08:33:41]
  Epoch: [240][1000/1475]   Time 0.138 (0.139)   Data 0.091 (0.106)   Loss 0.2203 (0.3280)   Prec@1 90.625 (86.142)   Prec@5 90.625 (86.142)   [2018-04-01 08:34:08]
  Epoch: [240][1200/1475]   Time 0.141 (0.139)   Data 0.109 (0.106)   Loss 0.6028 (0.3306)   Prec@1 71.875 (86.038)   Prec@5 71.875 (86.038)   [2018-04-01 08:34:36]
  Epoch: [240][1400/1475]   Time 0.141 (0.139)   Data 0.109 (0.106)   Loss 0.4123 (0.3301)   Prec@1 84.375 (86.039)   Prec@5 84.375 (86.039)   [2018-04-01 08:35:04]
  **Train** Prec@1 86.085 Prec@5 86.085 Error@1 13.915
  **VAL** Prec@1 88.410 Prec@5 88.410 Error@1 11.590

==>>[2018-04-01 08:35:32] [Epoch=241/250] [Need: 00:33:29] [learning_rate=0.0000] [Best : Accuracy=88.45, Error=11.55]

==>>Epoch=[241/250]], [2018-04-01 08:35:32], LR=[0.002], Batch=[32] [Model=SimpleNet]
  Epoch: [241][000/1475]   Time 0.139 (0.139)   Data 0.104 (0.104)   Loss 0.3167 (0.3167)   Prec@1 87.500 (87.500)   Prec@5 87.500 (87.500)   [2018-04-01 08:35:32]
  Epoch: [241][200/1475]   Time 0.125 (0.139)   Data 0.094 (0.106)   Loss 0.2792 (0.3346)   Prec@1 90.625 (85.386)   Prec@5 90.625 (85.386)   [2018-04-01 08:36:00]
  Epoch: [241][400/1475]   Time 0.141 (0.139)   Data 0.109 (0.105)   Loss 0.3308 (0.3307)   Prec@1 78.125 (85.747)   Prec@5 78.125 (85.747)   [2018-04-01 08:36:28]
  Epoch: [241][600/1475]   Time 0.150 (0.139)   Data 0.130 (0.105)   Loss 0.3928 (0.3278)   Prec@1 84.375 (86.080)   Prec@5 84.375 (86.080)   [2018-04-01 08:36:55]
  Epoch: [241][800/1475]   Time 0.150 (0.139)   Data 0.109 (0.106)   Loss 0.3718 (0.3289)   Prec@1 87.500 (86.002)   Prec@5 87.500 (86.002)   [2018-04-01 08:37:23]
  Epoch: [241][1000/1475]   Time 0.141 (0.139)   Data 0.109 (0.106)   Loss 0.4991 (0.3284)   Prec@1 78.125 (86.105)   Prec@5 78.125 (86.105)   [2018-04-01 08:37:51]
  Epoch: [241][1200/1475]   Time 0.125 (0.139)   Data 0.094 (0.106)   Loss 0.3556 (0.3290)   Prec@1 81.250 (86.108)   Prec@5 81.250 (86.108)   [2018-04-01 08:38:19]
  Epoch: [241][1400/1475]   Time 0.141 (0.139)   Data 0.109 (0.106)   Loss 0.2033 (0.3294)   Prec@1 93.750 (86.066)   Prec@5 93.750 (86.066)   [2018-04-01 08:38:46]
  **Train** Prec@1 86.040 Prec@5 86.040 Error@1 13.960
  **VAL** Prec@1 88.506 Prec@5 88.506 Error@1 11.494

==>>[2018-04-01 08:39:15] [Epoch=242/250] [Need: 00:29:46] [learning_rate=0.0000] [Best : Accuracy=88.51, Error=11.49]

==>>Epoch=[242/250]], [2018-04-01 08:39:15], LR=[0.002], Batch=[32] [Model=SimpleNet]
  Epoch: [242][000/1475]   Time 0.141 (0.141)   Data 0.094 (0.094)   Loss 0.4659 (0.4659)   Prec@1 75.000 (75.000)   Prec@5 75.000 (75.000)   [2018-04-01 08:39:15]
  Epoch: [242][200/1475]   Time 0.144 (0.139)   Data 0.113 (0.105)   Loss 0.2767 (0.3326)   Prec@1 93.750 (86.210)   Prec@5 93.750 (86.210)   [2018-04-01 08:39:43]
  Epoch: [242][400/1475]   Time 0.129 (0.139)   Data 0.097 (0.106)   Loss 0.3985 (0.3318)   Prec@1 84.375 (86.012)   Prec@5 84.375 (86.012)   [2018-04-01 08:40:10]
  Epoch: [242][600/1475]   Time 0.131 (0.139)   Data 0.094 (0.105)   Loss 0.4277 (0.3299)   Prec@1 81.250 (86.044)   Prec@5 81.250 (86.044)   [2018-04-01 08:40:38]
  Epoch: [242][800/1475]   Time 0.147 (0.139)   Data 0.113 (0.105)   Loss 0.3541 (0.3308)   Prec@1 81.250 (85.932)   Prec@5 81.250 (85.932)   [2018-04-01 08:41:06]
  Epoch: [242][1000/1475]   Time 0.145 (0.139)   Data 0.114 (0.105)   Loss 0.4262 (0.3308)   Prec@1 84.375 (85.927)   Prec@5 84.375 (85.927)   [2018-04-01 08:41:34]
  Epoch: [242][1200/1475]   Time 0.141 (0.139)   Data 0.109 (0.105)   Loss 0.1880 (0.3311)   Prec@1 93.750 (85.856)   Prec@5 93.750 (85.856)   [2018-04-01 08:42:01]
  Epoch: [242][1400/1475]   Time 0.130 (0.139)   Data 0.099 (0.105)   Loss 0.4264 (0.3319)   Prec@1 78.125 (85.838)   Prec@5 78.125 (85.838)   [2018-04-01 08:42:29]
  **Train** Prec@1 85.862 Prec@5 85.862 Error@1 14.138
  **VAL** Prec@1 88.530 Prec@5 88.530 Error@1 11.470

==>>[2018-04-01 08:42:58] [Epoch=243/250] [Need: 00:26:02] [learning_rate=0.0000] [Best : Accuracy=88.53, Error=11.47]

==>>Epoch=[243/250]], [2018-04-01 08:42:58], LR=[0.002], Batch=[32] [Model=SimpleNet]
  Epoch: [243][000/1475]   Time 0.125 (0.125)   Data 0.094 (0.094)   Loss 0.3120 (0.3120)   Prec@1 84.375 (84.375)   Prec@5 84.375 (84.375)   [2018-04-01 08:42:58]
  Epoch: [243][200/1475]   Time 0.141 (0.139)   Data 0.109 (0.105)   Loss 0.2143 (0.3448)   Prec@1 87.500 (85.712)   Prec@5 87.500 (85.712)   [2018-04-01 08:43:25]
  Epoch: [243][400/1475]   Time 0.130 (0.139)   Data 0.099 (0.105)   Loss 0.3860 (0.3371)   Prec@1 71.875 (86.097)   Prec@5 71.875 (86.097)   [2018-04-01 08:43:53]
  Epoch: [243][600/1475]   Time 0.129 (0.139)   Data 0.098 (0.105)   Loss 0.2294 (0.3325)   Prec@1 90.625 (86.174)   Prec@5 90.625 (86.174)   [2018-04-01 08:44:21]
  Epoch: [243][800/1475]   Time 0.149 (0.139)   Data 0.109 (0.105)   Loss 0.2870 (0.3340)   Prec@1 84.375 (86.115)   Prec@5 84.375 (86.115)   [2018-04-01 08:44:49]
  Epoch: [243][1000/1475]   Time 0.141 (0.139)   Data 0.094 (0.105)   Loss 0.3359 (0.3336)   Prec@1 84.375 (86.061)   Prec@5 84.375 (86.061)   [2018-04-01 08:45:16]
  Epoch: [243][1200/1475]   Time 0.146 (0.139)   Data 0.109 (0.105)   Loss 0.2422 (0.3303)   Prec@1 93.750 (86.217)   Prec@5 93.750 (86.217)   [2018-04-01 08:45:44]
  Epoch: [243][1400/1475]   Time 0.130 (0.139)   Data 0.111 (0.105)   Loss 0.2650 (0.3311)   Prec@1 87.500 (86.164)   Prec@5 87.500 (86.164)   [2018-04-01 08:46:12]
  **Train** Prec@1 86.161 Prec@5 86.161 Error@1 13.839
  **VAL** Prec@1 88.326 Prec@5 88.326 Error@1 11.674

==>>[2018-04-01 08:46:40] [Epoch=244/250] [Need: 00:22:19] [learning_rate=0.0000] [Best : Accuracy=88.53, Error=11.47]

==>>Epoch=[244/250]], [2018-04-01 08:46:40], LR=[0.002], Batch=[32] [Model=SimpleNet]
  Epoch: [244][000/1475]   Time 0.130 (0.130)   Data 0.094 (0.094)   Loss 0.3479 (0.3479)   Prec@1 84.375 (84.375)   Prec@5 84.375 (84.375)   [2018-04-01 08:46:41]
  Epoch: [244][200/1475]   Time 0.125 (0.139)   Data 0.094 (0.106)   Loss 0.4081 (0.3366)   Prec@1 84.375 (85.728)   Prec@5 84.375 (85.728)   [2018-04-01 08:47:08]
  Epoch: [244][400/1475]   Time 0.127 (0.139)   Data 0.096 (0.106)   Loss 0.3101 (0.3318)   Prec@1 96.875 (86.136)   Prec@5 96.875 (86.136)   [2018-04-01 08:47:36]
  Epoch: [244][600/1475]   Time 0.141 (0.139)   Data 0.094 (0.106)   Loss 0.3043 (0.3330)   Prec@1 87.500 (86.164)   Prec@5 87.500 (86.164)   [2018-04-01 08:48:04]
  Epoch: [244][800/1475]   Time 0.129 (0.139)   Data 0.098 (0.106)   Loss 0.2353 (0.3315)   Prec@1 93.750 (86.162)   Prec@5 93.750 (86.162)   [2018-04-01 08:48:32]
  Epoch: [244][1000/1475]   Time 0.141 (0.139)   Data 0.109 (0.106)   Loss 0.3046 (0.3347)   Prec@1 90.625 (85.920)   Prec@5 90.625 (85.920)   [2018-04-01 08:48:59]
  Epoch: [244][1200/1475]   Time 0.127 (0.139)   Data 0.096 (0.106)   Loss 0.2909 (0.3324)   Prec@1 90.625 (85.993)   Prec@5 90.625 (85.993)   [2018-04-01 08:49:27]
  Epoch: [244][1400/1475]   Time 0.125 (0.139)   Data 0.094 (0.106)   Loss 0.2884 (0.3324)   Prec@1 87.500 (85.994)   Prec@5 87.500 (85.994)   [2018-04-01 08:49:55]
  **Train** Prec@1 86.021 Prec@5 86.021 Error@1 13.979
  **VAL** Prec@1 88.530 Prec@5 88.530 Error@1 11.470

==>>[2018-04-01 08:50:23] [Epoch=245/250] [Need: 00:18:36] [learning_rate=0.0000] [Best : Accuracy=88.53, Error=11.47]

==>>Epoch=[245/250]], [2018-04-01 08:50:23], LR=[0.002], Batch=[32] [Model=SimpleNet]
  Epoch: [245][000/1475]   Time 0.141 (0.141)   Data 0.109 (0.109)   Loss 0.4638 (0.4638)   Prec@1 78.125 (78.125)   Prec@5 78.125 (78.125)   [2018-04-01 08:50:23]
  Epoch: [245][200/1475]   Time 0.148 (0.139)   Data 0.116 (0.106)   Loss 0.2968 (0.3316)   Prec@1 87.500 (85.759)   Prec@5 87.500 (85.759)   [2018-04-01 08:50:51]
  Epoch: [245][400/1475]   Time 0.141 (0.139)   Data 0.109 (0.105)   Loss 0.3239 (0.3319)   Prec@1 87.500 (85.676)   Prec@5 87.500 (85.676)   [2018-04-01 08:51:19]
  Epoch: [245][600/1475]   Time 0.141 (0.139)   Data 0.109 (0.105)   Loss 0.5342 (0.3316)   Prec@1 75.000 (85.784)   Prec@5 75.000 (85.784)   [2018-04-01 08:51:47]
  Epoch: [245][800/1475]   Time 0.133 (0.139)   Data 0.094 (0.105)   Loss 0.3295 (0.3323)   Prec@1 87.500 (85.877)   Prec@5 87.500 (85.877)   [2018-04-01 08:52:14]
  Epoch: [245][1000/1475]   Time 0.138 (0.139)   Data 0.107 (0.106)   Loss 0.3684 (0.3301)   Prec@1 84.375 (86.126)   Prec@5 84.375 (86.126)   [2018-04-01 08:52:42]
  Epoch: [245][1200/1475]   Time 0.144 (0.139)   Data 0.113 (0.105)   Loss 0.2577 (0.3289)   Prec@1 87.500 (86.196)   Prec@5 87.500 (86.196)   [2018-04-01 08:53:10]
  Epoch: [245][1400/1475]   Time 0.141 (0.139)   Data 0.109 (0.105)   Loss 0.3793 (0.3294)   Prec@1 84.375 (86.164)   Prec@5 84.375 (86.164)   [2018-04-01 08:53:38]
  **Train** Prec@1 86.180 Prec@5 86.180 Error@1 13.820
  **VAL** Prec@1 88.290 Prec@5 88.290 Error@1 11.710

==>>[2018-04-01 08:54:06] [Epoch=246/250] [Need: 00:14:53] [learning_rate=0.0000] [Best : Accuracy=88.53, Error=11.47]

==>>Epoch=[246/250]], [2018-04-01 08:54:06], LR=[0.002], Batch=[32] [Model=SimpleNet]
  Epoch: [246][000/1475]   Time 0.144 (0.144)   Data 0.113 (0.113)   Loss 0.3983 (0.3983)   Prec@1 87.500 (87.500)   Prec@5 87.500 (87.500)   [2018-04-01 08:54:06]
  Epoch: [246][200/1475]   Time 0.145 (0.139)   Data 0.114 (0.106)   Loss 0.4583 (0.3451)   Prec@1 71.875 (85.106)   Prec@5 71.875 (85.106)   [2018-04-01 08:54:34]
  Epoch: [246][400/1475]   Time 0.138 (0.139)   Data 0.107 (0.106)   Loss 0.2649 (0.3369)   Prec@1 90.625 (85.754)   Prec@5 90.625 (85.754)   [2018-04-01 08:55:02]
  Epoch: [246][600/1475]   Time 0.126 (0.139)   Data 0.095 (0.106)   Loss 0.4533 (0.3345)   Prec@1 75.000 (85.930)   Prec@5 75.000 (85.930)   [2018-04-01 08:55:29]
  Epoch: [246][800/1475]   Time 0.145 (0.139)   Data 0.114 (0.106)   Loss 0.1547 (0.3317)   Prec@1 96.875 (86.088)   Prec@5 96.875 (86.088)   [2018-04-01 08:55:57]
  Epoch: [246][1000/1475]   Time 0.141 (0.139)   Data 0.109 (0.106)   Loss 0.3511 (0.3305)   Prec@1 84.375 (86.151)   Prec@5 84.375 (86.151)   [2018-04-01 08:56:25]
  Epoch: [246][1200/1475]   Time 0.140 (0.139)   Data 0.109 (0.106)   Loss 0.2001 (0.3300)   Prec@1 93.750 (86.183)   Prec@5 93.750 (86.183)   [2018-04-01 08:56:53]
  Epoch: [246][1400/1475]   Time 0.139 (0.139)   Data 0.106 (0.106)   Loss 0.5853 (0.3308)   Prec@1 75.000 (86.077)   Prec@5 75.000 (86.077)   [2018-04-01 08:57:20]
  **Train** Prec@1 86.053 Prec@5 86.053 Error@1 13.947
  **VAL** Prec@1 88.374 Prec@5 88.374 Error@1 11.626

==>>[2018-04-01 08:57:49] [Epoch=247/250] [Need: 00:11:09] [learning_rate=0.0000] [Best : Accuracy=88.53, Error=11.47]

==>>Epoch=[247/250]], [2018-04-01 08:57:49], LR=[0.002], Batch=[32] [Model=SimpleNet]
  Epoch: [247][000/1475]   Time 0.151 (0.151)   Data 0.109 (0.109)   Loss 0.3131 (0.3131)   Prec@1 84.375 (84.375)   Prec@5 84.375 (84.375)   [2018-04-01 08:57:49]
  Epoch: [247][200/1475]   Time 0.141 (0.139)   Data 0.109 (0.106)   Loss 0.4527 (0.3187)   Prec@1 81.250 (86.598)   Prec@5 81.250 (86.598)   [2018-04-01 08:58:17]
  Epoch: [247][400/1475]   Time 0.141 (0.139)   Data 0.109 (0.106)   Loss 0.3434 (0.3231)   Prec@1 90.625 (86.409)   Prec@5 90.625 (86.409)   [2018-04-01 08:58:44]
  Epoch: [247][600/1475]   Time 0.160 (0.139)   Data 0.126 (0.106)   Loss 0.3781 (0.3268)   Prec@1 81.250 (86.179)   Prec@5 81.250 (86.179)   [2018-04-01 08:59:12]
  Epoch: [247][800/1475]   Time 0.141 (0.139)   Data 0.094 (0.106)   Loss 0.3156 (0.3304)   Prec@1 81.250 (85.986)   Prec@5 81.250 (85.986)   [2018-04-01 08:59:40]
  Epoch: [247][1000/1475]   Time 0.129 (0.139)   Data 0.098 (0.106)   Loss 0.3651 (0.3313)   Prec@1 87.500 (85.905)   Prec@5 87.500 (85.905)   [2018-04-01 09:00:08]
  Epoch: [247][1200/1475]   Time 0.135 (0.139)   Data 0.104 (0.106)   Loss 0.3087 (0.3321)   Prec@1 87.500 (85.874)   Prec@5 87.500 (85.874)   [2018-04-01 09:00:35]
  Epoch: [247][1400/1475]   Time 0.144 (0.139)   Data 0.118 (0.106)   Loss 0.4507 (0.3312)   Prec@1 81.250 (85.883)   Prec@5 81.250 (85.883)   [2018-04-01 09:01:03]
  **Train** Prec@1 85.873 Prec@5 85.873 Error@1 14.127
  **VAL** Prec@1 88.386 Prec@5 88.386 Error@1 11.614

==>>[2018-04-01 09:01:31] [Epoch=248/250] [Need: 00:07:26] [learning_rate=0.0000] [Best : Accuracy=88.53, Error=11.47]

==>>Epoch=[248/250]], [2018-04-01 09:01:31], LR=[0.002], Batch=[32] [Model=SimpleNet]
  Epoch: [248][000/1475]   Time 0.141 (0.141)   Data 0.109 (0.109)   Loss 0.6560 (0.6560)   Prec@1 71.875 (71.875)   Prec@5 71.875 (71.875)   [2018-04-01 09:01:31]
  Epoch: [248][200/1475]   Time 0.148 (0.139)   Data 0.117 (0.105)   Loss 0.3357 (0.3278)   Prec@1 90.625 (86.210)   Prec@5 90.625 (86.210)   [2018-04-01 09:01:59]
  Epoch: [248][400/1475]   Time 0.135 (0.139)   Data 0.104 (0.105)   Loss 0.3558 (0.3302)   Prec@1 84.375 (86.144)   Prec@5 84.375 (86.144)   [2018-04-01 09:02:27]
  Epoch: [248][600/1475]   Time 0.131 (0.139)   Data 0.100 (0.105)   Loss 0.2819 (0.3311)   Prec@1 87.500 (86.075)   Prec@5 87.500 (86.075)   [2018-04-01 09:02:55]
  Epoch: [248][800/1475]   Time 0.141 (0.139)   Data 0.094 (0.105)   Loss 0.2739 (0.3308)   Prec@1 93.750 (86.107)   Prec@5 93.750 (86.107)   [2018-04-01 09:03:23]
  Epoch: [248][1000/1475]   Time 0.138 (0.139)   Data 0.105 (0.105)   Loss 0.2042 (0.3293)   Prec@1 90.625 (86.129)   Prec@5 90.625 (86.129)   [2018-04-01 09:03:50]
  Epoch: [248][1200/1475]   Time 0.141 (0.139)   Data 0.109 (0.105)   Loss 0.2567 (0.3297)   Prec@1 90.625 (86.137)   Prec@5 90.625 (86.137)   [2018-04-01 09:04:18]
  Epoch: [248][1400/1475]   Time 0.141 (0.139)   Data 0.109 (0.105)   Loss 0.3441 (0.3298)   Prec@1 84.375 (86.144)   Prec@5 84.375 (86.144)   [2018-04-01 09:04:46]
  **Train** Prec@1 86.108 Prec@5 86.108 Error@1 13.892
  **VAL** Prec@1 88.386 Prec@5 88.386 Error@1 11.614

==>>[2018-04-01 09:05:14] [Epoch=249/250] [Need: 00:03:43] [learning_rate=0.0000] [Best : Accuracy=88.53, Error=11.47]

==>>Epoch=[249/250]], [2018-04-01 09:05:14], LR=[0.002], Batch=[32] [Model=SimpleNet]
  Epoch: [249][000/1475]   Time 0.147 (0.147)   Data 0.100 (0.100)   Loss 0.2965 (0.2965)   Prec@1 87.500 (87.500)   Prec@5 87.500 (87.500)   [2018-04-01 09:05:14]
  Epoch: [249][200/1475]   Time 0.141 (0.139)   Data 0.109 (0.105)   Loss 0.3259 (0.3179)   Prec@1 84.375 (86.800)   Prec@5 84.375 (86.800)   [2018-04-01 09:05:42]
  Epoch: [249][400/1475]   Time 0.141 (0.139)   Data 0.109 (0.105)   Loss 0.3287 (0.3212)   Prec@1 90.625 (86.549)   Prec@5 90.625 (86.549)   [2018-04-01 09:06:10]
  Epoch: [249][600/1475]   Time 0.141 (0.139)   Data 0.109 (0.105)   Loss 0.2691 (0.3217)   Prec@1 90.625 (86.554)   Prec@5 90.625 (86.554)   [2018-04-01 09:06:37]
  Epoch: [249][800/1475]   Time 0.139 (0.139)   Data 0.108 (0.106)   Loss 0.2492 (0.3259)   Prec@1 84.375 (86.275)   Prec@5 84.375 (86.275)   [2018-04-01 09:07:05]
  Epoch: [249][1000/1475]   Time 0.125 (0.139)   Data 0.094 (0.106)   Loss 0.2999 (0.3282)   Prec@1 90.625 (86.242)   Prec@5 90.625 (86.242)   [2018-04-01 09:07:33]
  Epoch: [249][1200/1475]   Time 0.141 (0.139)   Data 0.109 (0.106)   Loss 0.2647 (0.3292)   Prec@1 96.875 (86.124)   Prec@5 96.875 (86.124)   [2018-04-01 09:08:01]
  Epoch: [249][1400/1475]   Time 0.156 (0.139)   Data 0.109 (0.106)   Loss 0.4400 (0.3298)   Prec@1 78.125 (86.097)   Prec@5 78.125 (86.097)   [2018-04-01 09:08:28]
  **Train** Prec@1 86.110 Prec@5 86.110 Error@1 13.890
  **VAL** Prec@1 88.458 Prec@5 88.458 Error@1 11.542
